<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>苏</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-03T15:33:09.642Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>sulimin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LSTM_Elmo</title>
    <link href="http://yoursite.com/article/3b406ffc.html"/>
    <id>http://yoursite.com/article/3b406ffc.html</id>
    <published>2020-07-03T15:31:48.000Z</published>
    <updated>2020-07-03T15:33:09.642Z</updated>
    
    <content type="html"><![CDATA[<p>一、词的表示<br>one-hot编码：稀疏，向量之间均是正交关系，不能表示语义。<br>word class：将相同属性的词归为一类，但分类标准过意单一、片面。如cat、dag、bird都属于动物，但是dog、cat属于爬行动物，而bird属于飞行动物，难以区分。<br>word embedding：使用词向量表示词特征，相似的词、向量接近，在空间较近。但无法解决一词多义问题，即每个word只有一个词向量。<br>contextualized word embedding：同一个词在不同的上下文中有不同的表示。</p><p>二、LSTM概述<br>  长短记忆神经网络——通常称作LSTM，是一种特殊的RNN，能够学习长的依赖关系。是为了避免长依赖问题而精心设计的。记住较长的历史信息实际上是他们的默认行为，而不是他们努力学习的东西。<br>1<br>2.1 标准RNN结构如下(单个tanh层)</p><p>2.2 lstm 结构图如下</p><p>2.3 lstm的4层交互结构说明:<br>LSTM 的4大交互模块    结构片段    说明<br>遗忘门<br>遗忘门：由sigmoid决定丢弃那些信息，输出01序列，序列作用在单元格Ct-1上，1表示全保留，0全丢弃.<br>输入门<br>分为2步：①输入门：sigmoid决定更新哪些值<br>② tanh：创建候选向量Ct<br>旧值更新操作<br>将Ct-1更新成Ct, 对应实际删除旧主题信息，并添加新信息<br>输出门<br>决定输出什么，sigmoid决定输出哪些部分。然后tanh+输出哪些部分 得到输出.<br>三、ELMO模型概述<br>基于RNN的语言模型，直接从句子学习，无需标注。<br>1<br>3.1 问题引入<br>给定句子：“潮水 退了 就 知道 谁 没穿 裤子”</p><p>问题①：红框ELMO内部究竟是啥玩意?<br>问题②：是直接将词输入到ELMO模型吗？<br>问题③：ELMO输出的h1、h2、蓝柱是什么?<br>3.2 ELMO的内部构造图<br>elmo模型是一个多层(2层)双向的LSTM组成的LM(底层lstm捕获到句法、语义方面信息，如POS，高层捕获到词义的上下文信息，如语义消歧)<br>1</p><p>3.3 如何得到输入词的词向量<br>词向量的构建是动态的过程，会随着模型参数更新而更新。对于中文，原始输入的是词语；而对于类似英文，每个单词又是由多个字母构成，中文的词语等同于英文中的单词，词语中的词等同于单词中的字母(同一粒度)，本文中是针对英文而言的，中文类似</p><p>即input→词向量表示：对每个词进行字符卷积操作，从而得到词向量。(w2c系列是通过look_up_table得到词向量)</p><p>格式(batch, n_token, max_char, char_dim)，batch表示同时有多少个句子(样本)并行卷积，n_token表示句子在未去重条件下的单词数，char_dim表示每个字符的特征长度，max_char表示句子中词的最大字符数量。<br>使用大小”高为1，宽为n_width, 通道数为char_dim”(即维度[1, n_width, char_dim])的卷积核进行卷积, 得到[n_token, max_char-n_width+1]的特征图(备注：词字符少的感觉有个填充对齐操作以达到max_char大小)<br>最终再对图的每行进行一个max_pooling操作，得到batch个[n_token]。如输入(8, 16, 12, 64)，卷积核(1, 5, 64)， 即并行计算8个句子样本，样本最多有16单词，词中最大的字符数量是12个，每个字符用64个特征表示，最终获取8个(16, 12-5+1)特征图。<br>即为词向量<br>词向量卷积过程示例<br>示例：假设batch为1，句子 “I like China very much” , 如何卷积的?<br>即初始维度：(1, n_token, max_char, char_dim)，卷积核为(1, 2, char_dim), 卷积过程如下：</p><p>3.4 loss函数以及最终输出<br>3.4.1 loss函数<br>    训练是使用负对数似然作为损失函数。<br>1</p><p>3.4.2 输出向量表示<br>elmo在train过程中，每个词都由 正反向LSTM中间向量、正反向结果向量和静态向量(init时向量，也会同步更细)表示，故每个词都会有2L+1个表示向量：</p><p>3.4.3 最终结果计算<br>    ELMO考虑既考虑最后一层lstm的输出，同时兼顾静态词向量、中间词向量, 最终的结果与w2c类似，是预测位置上的可能概率分布。故输出结果表示如下：<br>1</p><p>3.5 ELMO事项说明<br>在实际任务中使用ELMO向量：<br> 结合监督任务训练时，可冻结(如设置权重标为0不更新该参数)EMLO模型中LSTM层的参数(静态词向量、正反向lstm中的参数[指的是权重，不是api的参数项])，只训练最后一层E函数中的权重值。此时可加入dropout(随机丢掉某些层的影响)或L2正则化(使E结果更接近lstm层输出的平均值)。<br> 结合方式有：</p><ol><li>直接在输入层的词向量拼上elmo表示</li><li>将elmo直接作用在RNN的输出上<br>改进：</li><li>ELMO对输出结果采用拼接方式融合特征，‘可能’ 弱于Bert一体化的融合方式</li><li>ELMO使用了LSTM作为特征抽取器，而非Transformer(研究表明Transformer特征提取能力远强于LSTM)</li><li>由RNN导致训练时间</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、词的表示&lt;br&gt;one-hot编码：稀疏，向量之间均是正交关系，不能表示语义。&lt;br&gt;word class：将相同属性的词归为一类，但分类标准过意单一、片面。如cat、dag、bird都属于动物，但是dog、cat属于爬行动物，而bird属于飞行动物，难以区分。&lt;br&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Task03-Haar特征描述算子-人脸监测</title>
    <link href="http://yoursite.com/article/ad28cab2.html"/>
    <id>http://yoursite.com/article/ad28cab2.html</id>
    <published>2020-07-02T08:42:19.000Z</published>
    <updated>2020-07-05T04:19:13.739Z</updated>
    
    <content type="html"><![CDATA[<h1 id="3-1-算法由来"><a href="#3-1-算法由来" class="headerlink" title="3.1 算法由来"></a>3.1 算法由来</h1><p>&emsp;&emsp;Haar-like特征最早是由Papageorgiou等应用于人脸表示，2001年，Viola和Jones两位大牛发表了今典的《Rapid Object Detection using a Boosted Cascade of Simple Features》和《Robust Real-Time Face Detection》，在AdaBoost算法基础上，使用Haar-like小波特征和积分图方法进行人脸监测，并对AdaBoost训练出的强分类器进行级联。<br>        这两个大咖不是最早提出使用小波特征的，单他们设计了针对人脸监测更有效的特征，可以说是人脸检测史上里程碑式的一笔了，当时这个算法被称为Viola-Jones检测器。又过了一段时间，Rainer Liehart和Jochen Maydt两位大咖把这个检测器进行扩展，最终形成了OpenCV现在的Haar分类器。</p><p>​        AdaBoost是Freund和Schapire在1995年提出的算法，是对传统Boosting算法的一大提升。Boosting算法的核心思想，是将弱学习方法提升成强学习算法，也就是“三个臭皮匠顶一个诸葛亮”。<a id="more"></a></p><font color="blue">Haar分类器 = Haar-like特征 + 积分图方法 + AdaBoost + 级联</font><p>Haar分类器算法的要点如下：</p><ol><li>使用Haar-like特征特征做监测</li><li>使用积分图(Integral Image)对Haar-like特征求值进行加速。</li><li>使用AdaBoost算法训练区分人脸和非人脸的强分类器。</li><li>使用筛选式级联把强分类器级联到一起，提高准确率。</li></ol><h1 id="3-2-算法理解"><a href="#3-2-算法理解" class="headerlink" title="3.2 算法理解"></a>3.2 算法理解</h1><h2 id="一、Haar-like特征"><a href="#一、Haar-like特征" class="headerlink" title="一、Haar-like特征"></a>一、Haar-like特征</h2><p>​        Haar特征分为四类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形。<font color="blue">并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和</font>。Haar特征值反映了图形的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，单矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向(水平、垂直、对角)的结构。</p><p><img src="/article/ad28cab2/1328274-20180802215641326-115308739.png" srcset="/img/loading.gif" alt="img"></p><p>​        图中的A, B和D这类特征，特征数值计算公式为：v=Σ白-Σ黑。图C中，计算公式：v=Σ白-2*Σ黑。将黑色区域像素和乘以2，是为了使两种矩形区域中像素数目一致。我们希望当把矩形放到人脸区域计算出来的特征值和放到非人脸区域计算出来的特征值差别越大越好，这样就可以用来区分人脸和非人脸。<br>​        通过改变特征模板的大小和位置，可在图像子窗口中穷举出大量的特征。上图的特征模板称为“特征原型”；特征原型在图像子窗口中扩展（平移伸缩）得到的特征称为“矩形特征”；矩形特征的值称为“特征值”。</p><p><img src="/article/ad28cab2/1328274-20180815213149483-1209887019.png" srcset="/img/loading.gif" alt="img"></p><p>​        上图中两个矩形特征，表示出人脸的某些特征。比如中间一幅表示眼睛区域的颜色比脸颊区域的颜色深，右边一幅表示鼻梁两侧比鼻梁的颜色要深。同样，其他目标，如眼睛等，也可以用一些矩形特征来表示。<font color="blue">使用矩形特征比单纯地使用像素点具有很大的优越性，并且速度更快。</font><br>​        矩形特征可位于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征，如：在24*24像素大小的检测窗口内矩形特征数量可以达到16万个。这样就有两个问题需要解决了：</p><p>（1）如何快速计算那么多的特征？—-积分图大显神通；<br>（2）哪些矩形特征才是对分类器分类最有效的？—-如通过AdaBoost算法来训练。</p><h2 id="二、Haar-like特征计算-积分图"><a href="#二、Haar-like特征计算-积分图" class="headerlink" title="二、Haar-like特征计算-积分图"></a>二、Haar-like特征计算-积分图</h2><p>​        <font color="blue">积分图是只遍历一次图像就可求出图像中所有区域像素和的快速算法</font>，大大提高了图像特征值计算的效率。<br>​        积分图的主要思想是将图像从起点开始到各个点形成的矩形区域像素之和作为一个数组的元素提前保存在数组中。当要计算某个区域的像素和时，直接索引数组的元素进行线性计算即可，从而加快了计算速度。<br>​        积分图是一种能够描述全局信息的举证表示方法。积分图的构造方式是位置位置（𝑖,𝑗）处的值𝑖𝑖(𝑖,𝑗)是原图像(𝑖,𝑗)左上角方向所有像素𝑓(𝑘,𝑙)的和：</p><script type="math/tex; mode=display">𝑖𝑖(𝑖,𝑗)=\sum_{𝑘≤𝑖,𝑙≤𝑗}{𝑓(𝑘,𝑙)}</script>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;3-1-算法由来&quot;&gt;&lt;a href=&quot;#3-1-算法由来&quot; class=&quot;headerlink&quot; title=&quot;3.1 算法由来&quot;&gt;&lt;/a&gt;3.1 算法由来&lt;/h1&gt;&lt;p&gt;&amp;emsp;&amp;emsp;Haar-like特征最早是由Papageorgiou等应用于人脸表示，2001年，Viola和Jones两位大牛发表了今典的《Rapid Object Detection using a Boosted Cascade of Simple Features》和《Robust Real-Time Face Detection》，在AdaBoost算法基础上，使用Haar-like小波特征和积分图方法进行人脸监测，并对AdaBoost训练出的强分类器进行级联。&lt;br&gt;        这两个大咖不是最早提出使用小波特征的，单他们设计了针对人脸监测更有效的特征，可以说是人脸检测史上里程碑式的一笔了，当时这个算法被称为Viola-Jones检测器。又过了一段时间，Rainer Liehart和Jochen Maydt两位大咖把这个检测器进行扩展，最终形成了OpenCV现在的Haar分类器。&lt;/p&gt;
&lt;p&gt;​        AdaBoost是Freund和Schapire在1995年提出的算法，是对传统Boosting算法的一大提升。Boosting算法的核心思想，是将弱学习方法提升成强学习算法，也就是“三个臭皮匠顶一个诸葛亮”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="图像处理下" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8B/"/>
    
    
      <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>pandas下综合练习</title>
    <link href="http://yoursite.com/article/67aeaf8.html"/>
    <id>http://yoursite.com/article/67aeaf8.html</id>
    <published>2020-07-01T07:36:38.000Z</published>
    <updated>2020-07-02T15:13:53.092Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、端午节的淘宝粽子交易"><a href="#一、端午节的淘宝粽子交易" class="headerlink" title="一、端午节的淘宝粽子交易"></a>一、端午节的淘宝粽子交易</h1><h2 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h2><p>（1）请删除最后一列为缺失值的行，并求所有在杭州发货的商品单价均值。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'端午粽子数据.csv'</span>)df.head()</code></pre><p><img src="/article/67aeaf8/image-20200701220417701.png" srcset="/img/loading.gif" alt="image-20200701220417701">列名中有些包含了空格，先去除。</p><pre><code class="hljs python">df.columns = df.columns.str.strip()df.columns</code></pre><p><img src="/article/67aeaf8/image-20200701220535004.png" srcset="/img/loading.gif" alt="image-20200701220535004"><br>然后将最后一列为空的行去除，使用dropna方法。再查看最后一列为空的行。</p><pre><code class="hljs python">df_clean = df.dropna(axis=<span class="hljs-number">0</span>,subset=[df.columns[<span class="hljs-number">-1</span>]])df_clean[df_clean[df_clean.columns[<span class="hljs-number">-1</span>]].isna()]</code></pre><p><img src="/article/67aeaf8/image-20200701220754043.png" srcset="/img/loading.gif" alt="image-20200701220754043"><br>确认已删除完毕。<br>将所有在杭州交易的记录取出。</p><pre><code class="hljs python">df_h = df_clean[df_clean[<span class="hljs-string">'发货地址'</span>].str.contains(<span class="hljs-string">'杭州'</span>)]</code></pre><p>查看其中价格列不能转换为float的行。</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">is_number</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">try</span>:        float(x)        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>    <span class="hljs-keyword">except</span> (SyntaxError, ValueError) <span class="hljs-keyword">as</span> e:        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>df_h[~df_h[<span class="hljs-string">'价格'</span>].map(is_number)]</code></pre><p><img src="/article/67aeaf8/image-20200701221103969.png" srcset="/img/loading.gif" alt="image-20200701221103969"></p><p>将其价格改为45。计算价格转换为float类型并计算均值。</p><pre><code class="hljs python">df_h.loc[<span class="hljs-number">4376</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45</span>df_h[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>).mean()</code></pre><p><img src="/article/67aeaf8/image-20200701221222671.png" srcset="/img/loading.gif" alt="image-20200701221222671"></p><p>（2）商品标题带有”嘉兴”但发货地却不在嘉兴的商品有多少条记录？<br>直接使用contain方法求取即可。</p><pre><code class="hljs python">df_bj = df_clean[df_clean[<span class="hljs-string">'标题'</span>].str.contains(<span class="hljs-string">'嘉兴'</span>)]df_bj_nj =df_bj[~(df_bj[<span class="hljs-string">'发货地址'</span>].str.contains(<span class="hljs-string">'嘉兴'</span>))]df_bj_nj.shape</code></pre><p>（3）请按照分位数将价格分为“高、较高、中、较低、低” 5个类别，再将类别结果插入到标题一列之后，最后对类别列进行降序排序。<br>查看价格列中异常值。</p><pre><code class="hljs python">df_clean[~df_clean[<span class="hljs-string">'价格'</span>].map(is_number)]</code></pre><p><img src="/article/67aeaf8/image-20200701221526292.png" srcset="/img/loading.gif" alt="image-20200701221526292"><br>修改为对应的正常值。</p><pre><code class="hljs python">df_clean.loc[<span class="hljs-number">538</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45.9</span>df_clean.loc[<span class="hljs-number">4376</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45</span></code></pre><p>使用qcut方法将价格按数量等分为5类。并重命名为“高、较高、中、较低、低” 5个类别。</p><pre><code class="hljs python">q_cut = pd.qcut(df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>),<span class="hljs-number">5</span>)df_clean.loc[:,<span class="hljs-string">'类别'</span>] = q_cut.cat.rename_categories([<span class="hljs-string">'低'</span>,<span class="hljs-string">'较低'</span>,<span class="hljs-string">'中'</span>,<span class="hljs-string">'较高'</span>,<span class="hljs-string">'高'</span>])</code></pre><p>将列顺序按要求排列。并将表格按照类别进行降序排列。</p><pre><code class="hljs python">df_clean = df_clean[[<span class="hljs-string">'标题'</span>,<span class="hljs-string">'类别'</span>,<span class="hljs-string">'价格'</span>,<span class="hljs-string">'付款人数'</span>,<span class="hljs-string">'店铺'</span>,<span class="hljs-string">'发货地址'</span>]]df_clean = df_clean.sort_values(<span class="hljs-string">'类别'</span>, ascending=<span class="hljs-literal">False</span>)</code></pre><p><img src="/article/67aeaf8/image-20200701221901144.png" srcset="/img/loading.gif" alt="image-20200701221901144"><br>（4）付款人数一栏有缺失值吗？若有则请利用上一问的分类结果对这些缺失值进行合理估计并填充。<br>查看付款人数一栏有缺失值的行。</p><pre><code class="hljs python">df_clean[df_clean[<span class="hljs-string">'付款人数'</span>].isna()]</code></pre><p><img src="/article/67aeaf8/image-20200701221950329.png" srcset="/img/loading.gif" alt="image-20200701221950329"></p><pre><code class="hljs python">df_clean[<span class="hljs-string">'价格'</span>] = df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>)df_clean.sort_values(<span class="hljs-string">'价格'</span>, ascending=<span class="hljs-literal">False</span>)df_clean[<span class="hljs-string">'发货地址'</span>].value_counts()</code></pre><pre><code class="hljs gml">def replace(<span class="hljs-symbol">x</span>):    try:        <span class="hljs-symbol">x</span> = str(<span class="hljs-symbol">x</span>)        <span class="hljs-keyword">if</span> <span class="hljs-string">'万'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'万'</span>)            <span class="hljs-keyword">return</span> float(<span class="hljs-symbol">x</span>[:i]) * <span class="hljs-number">10000</span>        <span class="hljs-keyword">if</span> <span class="hljs-string">'+'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'+'</span>)        elif <span class="hljs-string">'人'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'人'</span>)        <span class="hljs-keyword">else</span>:            # print(<span class="hljs-symbol">x</span>)            <span class="hljs-keyword">return</span> None        <span class="hljs-keyword">return</span> int(<span class="hljs-symbol">x</span>[:i])    except:        print(<span class="hljs-symbol">x</span>)        <span class="hljs-keyword">return</span> None</code></pre><pre><code class="hljs python">df_clean[<span class="hljs-string">'付款人数'</span>] = df_clean[<span class="hljs-string">'付款人数'</span>].map(replace)df_clean = df_clean.set_index(<span class="hljs-string">'价格'</span>)df_clean[<span class="hljs-string">'付款人数'</span>] = df_clean[<span class="hljs-string">'付款人数'</span>].interpolate(method=<span class="hljs-string">'index'</span>)df_clean = df_clean.reset_index()df_clean = df_clean[[<span class="hljs-string">'标题'</span>,<span class="hljs-string">'类别'</span>,<span class="hljs-string">'价格'</span>,<span class="hljs-string">'付款人数'</span>,<span class="hljs-string">'店铺'</span>,<span class="hljs-string">'发货地址'</span>]]</code></pre><p>（5）请将数据后四列合并为如下格式的Series:商品发货地为xx，店铺为xx，共计xx人付款，单价为xx。</p><pre><code class="hljs prolog">result = <span class="hljs-string">'商品发货地为'</span>+df_clean[<span class="hljs-string">'发货地址'</span>]+<span class="hljs-string">',店铺为'</span>+df_clean[<span class="hljs-string">'店铺'</span>] \                    +<span class="hljs-string">',共计'</span>+df_clean[<span class="hljs-string">'付款人数'</span>].astype(<span class="hljs-string">'str'</span>)+<span class="hljs-string">',单价为'</span>\                   + df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'str'</span>)+<span class="hljs-string">'元'</span></code></pre><pre><code class="hljs awk">result.str.extract(<span class="hljs-string">r'商品发货地为(?P&lt;发货地址&gt;[\w]+),店铺为(?P&lt;店铺&gt;[\w]+) \</span><span class="hljs-string">                    ,共计(?P&lt;付款人数&gt;[\w]+),单价为(?P&lt;价格&gt;[?\d.\dh]+)元'</span>)</code></pre><p>提取的时候失败了。。</p><h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><pre><code class="hljs routeros">df3 = pd.read_csv(<span class="hljs-string">'摩拜单车数据.csv'</span>)df3[<span class="hljs-string">'start_time'</span>] = pd.to_datetime(df3[<span class="hljs-string">'start_time'</span>].values)df3[<span class="hljs-string">'end_time'</span>] = pd.to_datetime(df3[<span class="hljs-string">'end_time'</span>].values)df3[<span class="hljs-string">'run'</span>] = df3[<span class="hljs-string">'end_time'</span>]-df3[<span class="hljs-string">'start_time'</span>]result1 = df3[df3[<span class="hljs-string">'start_time'</span>].dt.dayofweek.isin([5,6])]result2 = df3[df3[<span class="hljs-string">'start_time'</span>].dt.dayofweek.isin([0,1,2,3,4])]result1.index = result1[<span class="hljs-string">'start_time'</span>].dt.strftime(<span class="hljs-string">'%Y-%m-%d'</span>)result2.index = result2[<span class="hljs-string">'start_time'</span>].dt.strftime(<span class="hljs-string">'%Y-%m-%d'</span>)result1.index.name = <span class="hljs-string">'day'</span>result2.index.name = <span class="hljs-string">'day'</span>result2.head()group1 = result1.groupby([<span class="hljs-string">'day'</span>])num1 = 0num2 = 0group1.size()<span class="hljs-comment"># for name, group in group1:</span><span class="hljs-keyword">for</span> name,<span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> group1:    <span class="hljs-builtin-name">print</span>(name)    num1 += 1    <span class="hljs-keyword">if</span> num1 ==1:        result_1 = group[<span class="hljs-string">'run'</span>].sum()    <span class="hljs-keyword">else</span>:        result_1 += group[<span class="hljs-string">'run'</span>].sum()result_1 = result_1 / num1<span class="hljs-builtin-name">print</span>(result_1)group2 = result2.groupby(<span class="hljs-string">'day'</span>)<span class="hljs-keyword">for</span> name,<span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> group2:    num2 += 1    <span class="hljs-keyword">if</span> num2 ==1:        result_2 = group[<span class="hljs-string">'run'</span>].sum()    <span class="hljs-keyword">else</span>:        result_2 += group[<span class="hljs-string">'run'</span>].sum()result_2 = result_2 / num2<span class="hljs-builtin-name">print</span>(result_2)</code></pre><p><img src="/article/67aeaf8/image-20200701222344893.png" srcset="/img/loading.gif" alt="image-20200701222344893"></p><p><img src="/article/67aeaf8/image-20200701222354535.png" srcset="/img/loading.gif" alt="image-20200701222354535"></p><pre><code class="hljs nginx"><span class="hljs-attribute">result_1</span> &gt; result_2</code></pre><p><img src="/article/67aeaf8/image-20200701222414991.png" srcset="/img/loading.gif" alt="image-20200701222414991"></p><p>周末的工作量是比工作日当天用车量更大</p><p>（2）</p><pre><code class="hljs python">result2[<span class="hljs-string">"start_hour"</span>] = result2[<span class="hljs-string">'start_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">0</span>]))result2[<span class="hljs-string">"end_hour"</span>] = result2[<span class="hljs-string">'end_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">0</span>]))result2[<span class="hljs-string">"start_min"</span>] = result2[<span class="hljs-string">'start_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">1</span>]))result2[<span class="hljs-string">'end_min'</span>] = result2[<span class="hljs-string">'end_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                                            (<span class="hljs-string">":"</span>)[<span class="hljs-number">1</span>]))</code></pre><pre><code class="hljs python">result2_e = result2[(result2[<span class="hljs-string">'start_hour'</span>]&lt;<span class="hljs-number">9</span>) | ((result2[<span class="hljs-string">'start_hour'</span>]==<span class="hljs-number">9</span>) &amp; (result2[<span class="hljs-string">'start_min'</span>] &lt; <span class="hljs-number">30</span>))]result2_e = result2_e[(result2_e[<span class="hljs-string">'end_hour'</span>]&gt;<span class="hljs-number">7</span>) | ((result2_e[<span class="hljs-string">'end_hour'</span>]==<span class="hljs-number">7</span>) &amp; (result2_e[<span class="hljs-string">'end_min'</span>] &gt; <span class="hljs-number">30</span>))]result2_a = result2[(result2[<span class="hljs-string">'start_hour'</span>]&lt;<span class="hljs-number">19</span>) &amp; (result2[<span class="hljs-string">'end_hour'</span>]&gt;<span class="hljs-number">17</span>)]</code></pre><pre><code class="hljs python">result2_e.shape[<span class="hljs-number">0</span>]group2_e = result2_e.groupby(<span class="hljs-string">'day'</span>)list_e = []list_n = []<span class="hljs-keyword">for</span> name, group <span class="hljs-keyword">in</span> group2_e:    list_n.append(name)    list_e.append(group[<span class="hljs-string">'run'</span>].sum())group2_a = result2_a.groupby(<span class="hljs-string">'day'</span>)list_a = []list_n2 = []<span class="hljs-keyword">for</span> name, group <span class="hljs-keyword">in</span> group2_a:    list_n2.append(name)    list_a.append(group[<span class="hljs-string">'run'</span>].sum())result_day = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(list_a)):    <span class="hljs-keyword">if</span> list_e[i] &gt; list_a[i]:        result_day.append(list_n[i])    <span class="hljs-keyword">else</span>:        <span class="hljs-keyword">pass</span></code></pre><pre><code class="hljs python">result_day</code></pre><p><img src="/article/67aeaf8/image-20200701223448673.png" srcset="/img/loading.gif" alt="image-20200701223448673"></p>]]></content>
    
    <summary type="html">
    
      pandas下综合练习三道大题。
    
    </summary>
    
    
      <category term="pandas下" scheme="http://yoursite.com/categories/pandas%E4%B8%8B/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>4方差分析</title>
    <link href="http://yoursite.com/article/2ad44fb9.html"/>
    <id>http://yoursite.com/article/2ad44fb9.html</id>
    <published>2020-06-30T15:17:02.000Z</published>
    <updated>2020-07-01T06:23:42.271Z</updated>
    
    <content type="html"><![CDATA[<h1 id="方差分析"><a href="#方差分析" class="headerlink" title="方差分析"></a>方差分析</h1><h2 id="1-概要"><a href="#1-概要" class="headerlink" title="1 概要"></a>1 概要</h2><p><strong>方差分析</strong>(<strong>Analysis of variance, ANOVA</strong>) 主要研究分类变量作为自变量时，对因变量的影响是否是显著的。</p><p>方差分析的方法是由20世纪的统计学家Ronald Aylmer Fisher在1918年到1925年之间提出并陆续完善起来的，该方法刚开始是用于解决田间实验的数据分析问题，因此，方差分析的学习是和实验设计、实验数据的分析密不可分的。</p><p>实验设计和方差分析都有自己相应的语言。因此，在这里我们通过一个焦虑症治疗的实例，先了解一些术语，并且思考一下，方差分析主要用于解决什么样的问题。<a id="more"></a></p><p>以焦虑症治疗为例，现有两种治疗方案：认知行为疗法（CBT）和眼动脱敏再加工法（EMDR）。我们招募10位焦虑症患者作为志愿者，随机分配一半的人接受为期五周的CBT，另外一半接受为期五周的EMDR，设计方案如表1-1所示。在治疗结束时，要求每位患者都填写状态特质焦虑问卷（STAI），也就是一份焦虑度测量的自我评测报告。</p><p><strong>表1-1 单因素组间方差分析</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">CBT</th><th style="text-align:center">EMDR</th></tr></thead><tbody><tr><td style="text-align:center">s1</td><td style="text-align:center">s6</td></tr><tr><td style="text-align:center">s2</td><td style="text-align:center">s7</td></tr><tr><td style="text-align:center">s3</td><td style="text-align:center">s8</td></tr><tr><td style="text-align:center">s4</td><td style="text-align:center">s9</td></tr><tr><td style="text-align:center">s5</td><td style="text-align:center">s10</td></tr></tbody></table></div><p>在这个实验设计中，治疗方案是两水平（CBT、EMDR）的<strong>组间因子</strong>。之所以称其为<strong>组间因子</strong>，是因为每位患者都仅被分配到一个组别中，没有患者同时接受CBT和EMDR。表中字母s代表受试者（患者）。STAI是<strong>因变量</strong>，治疗方案是<strong>自变量</strong>。由于在每种治疗方案下观测数相等，因此这种设计也称为<strong>均衡设计</strong>（balanced design）；若观测数不同，则称作<strong>非均衡设计</strong>（unbalanced design）。</p><p>因为仅有一个类别型变量，表1的统计设计又称为<strong>单因素方差分析（one-way ANOVA）</strong>，或进一步称为<strong>单因素组间方差分析</strong>。方差分析主要通过F检验来进行效果评测，若治疗方案的F检验显著，则说明五周后两种疗法的STAI得分均值不同。</p><p>假设你只对CBT的效果感兴趣，则需将10个患者都放在CBT组中，然后在治疗五周和六个月后分别评价疗效，设计方案如表1-2所示。</p><p><strong>表1-2 单因素组内方差分析</strong></p><div class="table-container"><table><thead><tr><th></th><th>时间</th><th></th></tr></thead><tbody><tr><td><strong>患者</strong></td><td>5周</td><td>6个月</td></tr><tr><td>s1</td><td></td><td></td></tr><tr><td>s2</td><td></td><td></td></tr><tr><td>s3</td><td></td><td></td></tr><tr><td>s4</td><td></td><td></td></tr><tr><td>s5</td><td></td><td></td></tr><tr><td>s6</td><td></td><td></td></tr><tr><td>s7</td><td></td><td></td></tr><tr><td>s8</td><td></td><td></td></tr><tr><td>s9</td><td></td><td></td></tr><tr><td>s10</td><td></td></tr></tbody></table></div><p>此时，时间（time）是两水平（五周、六个月）的<strong>组内因子</strong>。因为每位患者在所有水平下都进行了测量，所以这种统计设计称<strong>单因素组内方差分析</strong>；又由于每个受试者都不止一次被测量，也称作<strong>重复测量方差分析</strong>。当时间的F检验显著时，说明患者的STAI得分均值在五周和六个月间发生了改变。</p><p>现假设你对治疗方案差异和它随时间的改变都感兴趣，则将两个设计结合起来即可：随机分配五位患者到CBT，另外五位到EMDR，在五周和六个月后分别评价他们的STAI结果（见表1-3）。</p><p><strong>表1-3 含组间和组内因子的双因素方差分析</strong></p><div class="table-container"><table><thead><tr><th></th><th></th><th>时间</th><th></th></tr></thead><tbody><tr><td><strong>疗法</strong></td><td><strong>患者</strong></td><td>5周</td><td>6个月</td></tr><tr><td>CBT</td><td>s1</td><td></td><td></td></tr><tr><td></td><td>s2</td><td></td><td></td></tr><tr><td></td><td>s3</td><td></td><td></td></tr><tr><td></td><td>s4</td><td></td><td></td></tr><tr><td></td><td>s5</td><td></td><td></td></tr><tr><td>EMDR</td><td>s6</td><td></td><td></td></tr><tr><td></td><td>s7</td><td></td><td></td></tr><tr><td></td><td>s8</td><td></td><td></td></tr><tr><td></td><td>s9</td><td></td><td></td></tr><tr><td></td><td>s10</td><td></td></tr></tbody></table></div><p>疗法（therapy）和时间（time）都作为因子时，我们既可分析疗法的影响（时间跨度上的平均）和时间的影响（疗法类型跨度上的平均），又可分析疗法和时间的交互影响。前两个称作<strong>主效应</strong>，交互部分称作<strong>交互效应</strong>。</p><p>当设计包含两个甚至更多的因子时，便是<strong>因素方差分析设计</strong>，比如两因子时称作<strong>双因素方差分析</strong>，三因子时称作三因素方差分析，以此类推。若因子设计包括组内和组间因子，又称作<strong>混合模型方差分析</strong>，当前的例子就是典型的双因素混合模型方差分析。</p><p>本例中，你将做三次F检验：疗法因素一次，时间因素一次，两者交。</p><p>15214互因素一次。若疗法结果显著，说明CBT和EMDR对焦虑症的治疗效果不同；若时间结果显著，说明焦虑度从五周到六个月发生了变化；若两者交互效应显著，说明两种疗法随着时间变化对焦虑症治疗影响不同（也就是说，焦虑度从五周到六个月的改变程度在两种疗法间是不同的）。</p><p>现在，我们对上面的实验设计稍微做些扩展。众所周知，抑郁症对病症治疗有影响，而且抑郁症和焦虑症常常同时出现。即使受试者被随机分配到不同的治疗方案中，在研究开始时，两组疗法中的患者抑郁水平就可能不同，任何治疗后的差异都有可能是最初的抑郁水平不同导致的，而不是由于实验的操作问题。抑郁症也可以解释因变量的组间差异，因此它常称为<strong>混淆因素</strong>（confounding factor）。由于你对抑郁症不感兴趣，它也被称作<strong>干扰变数</strong>（nuisance variable）。</p><p>假设招募患者时使用抑郁症的自我评测报告，比如白氏抑郁症量表（BDI），记录了他们的抑郁水平，那么你可以在评测疗法类型的影响前，对任何抑郁水平的组间差异进行统计性调整。本案例中，BDI为<strong>协变量</strong>，该设计为<strong>协方差分析</strong>（ANCOVA）。</p><p>以上设计只记录了单个因变量情况（STAI），为增强研究的有效性，可以对焦虑症进行其他的测量（比如家庭评分、医师评分，以及焦虑症对日常行为的影响评价）。当因变量不止一个时，设计被称作<strong>多元方差分析</strong>（MANOVA）， 若协变量也存在， 那么就叫<strong>多元协方差分析</strong>（MANCOVA）。</p><p>下面我们主要介绍单因素方差分析与双因素方差分析的原理与实现。</p><h2 id="2-单因素方差分析"><a href="#2-单因素方差分析" class="headerlink" title="2 单因素方差分析"></a>2 单因素方差分析</h2><h3 id="2-1-推导过程"><a href="#2-1-推导过程" class="headerlink" title="2.1 推导过程"></a>2.1 推导过程</h3><p>接下来我们使用种小麦的例子，去帮助理解方差分析里涉及的一些变量。</p><p>假设我们现在有若干品种的小麦，要在某一地区播种，我们想知道这些品种的产量有没有显著区别，为此我们先设计了一个田间实验，取一大块地将其分成形状大小都相同的$n$小块．设供选择的品种有$k$个，我们打算其中的$n_1$小块种植品种1, $n_2$小块种植品种2，等等，$n_1 + n_2 + … n_k = n$.</p><p>接下来，我们使用方差分析的方法去看不同小麦品种的产量是否有显著差异。</p><p>设问题中涉及一个因素$A$，有$k$个水平，如上例的$k$个种子品种，以$Y_{ij}$记第$i$个水平的第$j$个观察值，如上例$Y_{ij}$是种植品种$i$的第$j$小块地上的亩产量。模型为</p><script type="math/tex; mode=display">Y_{ij} = a_i + e_{ij},  j = 1,...,n_i, i = 1,...,k\qquad(2.1)</script><p>$a_i$表示水平$i$的理论平均值，称为水平$i$的效应。在小麦例子中，$a_i$就是品种$i$的平均亩产量，$e_{ij}$就是随机误差。并且我们假定：</p><script type="math/tex; mode=display">E(e_{ij})=0, 0<Var(e_{ij})={\sigma}^2<\infty,一切e_{ij}独立同分布\qquad(2.2)</script><p>因素$A$的各水平的高低优劣，取决于其理论平均$a_{i}$的大小。故对模型(2.1)，我们头一个关心的事情，就是诸$a_{i}$是否全相同。 如果是，则表示因素$A$对所考察的指标$Y$其实无影响．这时我们就说因素A的效应不显著，否则就说它显著。当然，在实际应用中，所谓“显著”，是指诸$a_{i}$之间的差异要大到一定的程度．这个 “一定的程度”，是从其实用上的意义着眼，而“统计显著性”，则是与随机误差相比而言．这点在下文的讨论中会有所体现．我们把所要检验的假设写为：</p><script type="math/tex; mode=display">H_0:a_1=a_2=\cdots=a_k \qquad (2.3)</script><p>为检验该假设，我们需要分析，为什么各个$Y_{ij}$会有差异？从模型(2.1)来看，无非两个原因：一是各$a_{i}$可能有差异．例如，若 $a_1&gt;a_2$, 这就使$Y_{1j}$倾向于大于$Y_{2j}$；二是随机误差的存在。这一分析启发了如下的想法：找一个衡量全部$y_{ij}$的变异的量：</p><script type="math/tex; mode=display">SS= \sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y} \right )^2, \qquad \bar{Y}=\sum_{i=1}^{k}\sum_{j=1}^{n_i}Y_{ij}/n \qquad (2.4)</script><p>$SS$愈大，表示$Y_{ij}$之间的差异越大。</p><p>接下来，把$SS$分为两部分，一部分表示随机误差的影响，记为$SS_e$；另一部分表示因素$A$的各水平理论平均值$a_i$不同带来的影响，记为$SS_A$。</p><p>关于$SS_e$，先固定一个$i$，此时对应的所有观测值$Y_{i1},Y_{i2},\cdots,Y_{in}$，他们之间的差异与每个水平的理论平均值不等无关，而是取决于随机误差，反映这些观察值差异程度的量是$\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2$，其中</p><script type="math/tex; mode=display">\bar{Y_i}=(Y_{i1}+Y_{i2}+\cdots+Y_{in})/n_i,\quad i=1, 2,\cdots,n \qquad (2.5)</script><p>$\bar{Y_i}$可以视为对$a_i$的估计。把上述平方和做累加得：</p><script type="math/tex; mode=display">SS_e=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \qquad (2.6)</script><p>可求得$SS_A$:</p><script type="math/tex; mode=display">\begin{align}SS_A &= SS-SS_e \\ &=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y} \right )^2-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})-(\bar{Y}-\bar{Y_i}) \right )^2-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})^2-2(Y_{ij}-\bar{Y_i})(\bar{Y}-\bar{Y_i})+(\bar{Y}-\bar{Y_i})^2 \right )-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2 - 2\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})(\bar{Y}-\bar{Y_i}) \right )+ \sum_{i=1}^{k}\sum_{j=1}^{n_i}(\bar{Y}-\bar{Y_i})^2 - \sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&= \sum_{i=1}^{k}\sum_{j=1}^{n_i}(\bar{Y_i}-\bar{Y})^2 - 2\sum_{i=1}^{k}\left ( (\bar{Y}-\bar{Y_i})\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i}) \right ) \quad (ps:\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})=0) \\&= \sum_{i=1}^{k}n_i(\bar{Y_i}-\bar{Y})^2 \qquad (2.7)\end{align}</script><p>因为$\bar{Y_i}$可以视为对$a_i$的估计，$a_i$的差异越大，$\bar{Y_i}$之间的差异也越大，所以$SS_A$可以用来衡量不同水平之间的差异程度。</p><p>在统计学上，通常称$SS$为<strong>总平方和</strong>，$SS_A$为<strong>因素$A$的平方和</strong>，$SS_e$为<strong>误差平方和</strong>，分解式$SS=SS_A+SS_e$为该模型的<strong>方差分析</strong>。</p><p>基于上面的分析，我们可以得到假设（5.3）的一个检验方法：当比值$SS_A/SS_e$大于某一给定界限时，否定$H_0$，不然就接受$H_0$。为了构造$F$分布的检验统计量，我们假定随机误差$e_{ij}$满足正态分布$N(0, \sigma^2)$，同时我们也假定观察值$Y_{ij}$符合正态分布，此时，记</p><script type="math/tex; mode=display">MS_A = SS_A/(k-1), \quad MS_e = SS_e/(n-k) \qquad (2.8)</script><p>当$H_0$成立时，有：</p><script type="math/tex; mode=display">MS_A / MS_e \sim F_{k-1, n-k} \qquad (2.9)</script><p>据（5.9），在给定显著性水平$\alpha$时，即得（5.3）的假设$H_0$的检验如下：</p><script type="math/tex; mode=display">当MS_A / MS_e \leqslant  F_{k-1, n-k}(\alpha)时，接受H_0，不然就拒绝H_0 \qquad (2.10)</script><p>$MS_A$和$MS_e$分别被称为<strong>因素$A$和随机误差的平均平方和</strong>。被除数$k-1$和$n-k$，分别称为这两个平方和的<strong>自由度</strong>。$MS_e$的自由度为什么是$n-k$呢？因为平方和$\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2$的自由度为$n_i-1$，故对$i$求和，$SS_e$的自由度就是$n-k$。那么，$MS_A$的自由度为什么是$k-1$呢？因为一共有$k$个平均值$a_1,\cdots,a_k$等$k-1$个，故自由度为$k-1$，两者自由度之和为$n-1$，恰好是总平方和的自由度。</p><p>到这里，我们可以做出方差分析表如表2-1</p><p><strong>2-1 单因素方差分析的方差分析表</strong></p><div class="table-container"><table><thead><tr><th>项目</th><th>$SS$</th><th>自由度</th><th>$MS$</th><th>$F$比</th><th>显著性</th></tr></thead><tbody><tr><td>$A$</td><td>$SS_A$</td><td>$k-1$</td><td>$MS_A$</td><td>$MS_A / MS_e$</td><td><em>, *</em>, 或无</td></tr><tr><td>误差</td><td>$SS_e$</td><td>$n-k$</td><td>$MS_e$</td><td></td><td></td></tr><tr><td>总和</td><td>$SS$</td><td>$n-1$</td><td></td><td></td></tr></tbody></table></div><p>在上表中，对于显著性一栏，一般来说，我们把算出的$F$比，即$MS_A / MS_e$，与$F_{k-1, n-k}(0.05)=c_1$和$F_{k-1, n-k}(0.01)=c_2$比较。若$MS_A / MS_e&gt;c_2$，用**表示，表明A因素的效应是高度显著的，即在$\alpha=0.01$的显著性水平下，拒绝原假设（5.3）。同理，$c_2<MS_A ms_e<c_1$用$\ast $表示，$ms_a ms_e>c_1$时不显著。</MS_A></p><h3 id="2-2-代码实例"><a href="#2-2-代码实例" class="headerlink" title="2.2 代码实例"></a>2.2 代码实例</h3><h4 id="单因素方差分析的R语言实现"><a href="#单因素方差分析的R语言实现" class="headerlink" title="单因素方差分析的R语言实现"></a>单因素方差分析的R语言实现</h4><p>单因素方差分析中，你感兴趣的是比较分类因子定义的两个或多个组别中的因变量均值。以multcomp包中的cholesterol数据集为例，50个患者均接受降低胆固醇药物治疗（trt）五种疗法中的一种疗法。其中三种治疗条件使用药物相同，分别是20mg一天一次（1time）、10mg一天两次（2times）和5mg一天四次（4times）。剩下的两种方式（drugD和drugE）代表候选药物。</p><pre><code class="hljs angelscript">&gt; library(multcomp)&gt; attach(cholesterol)&gt; &gt; # 统计各组样本大小&gt; table(trt) trt <span class="hljs-number">1</span>time <span class="hljs-number">2</span>times <span class="hljs-number">4</span>times  drugD  drugE     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span> &gt; &gt; # 各组均值&gt; aggregate(response, by=list(trt), FUN=mean)  Group<span class="hljs-number">.1</span>        x<span class="hljs-number">1</span>   <span class="hljs-number">1</span>time  <span class="hljs-number">5.78197</span><span class="hljs-number">2</span>  <span class="hljs-number">2</span>times  <span class="hljs-number">9.22497</span><span class="hljs-number">3</span>  <span class="hljs-number">4</span>times <span class="hljs-number">12.37478</span><span class="hljs-number">4</span>   drugD <span class="hljs-number">15.36117</span><span class="hljs-number">5</span>   drugE <span class="hljs-number">20.94752</span>&gt; &gt; # 各组标准差&gt; aggregate(response, by=list(trt), FUN=sd)  Group<span class="hljs-number">.1</span>        x<span class="hljs-number">1</span>   <span class="hljs-number">1</span>time <span class="hljs-number">2.878113</span><span class="hljs-number">2</span>  <span class="hljs-number">2</span>times <span class="hljs-number">3.483054</span><span class="hljs-number">3</span>  <span class="hljs-number">4</span>times <span class="hljs-number">2.923119</span><span class="hljs-number">4</span>   drugD <span class="hljs-number">3.454636</span><span class="hljs-number">5</span>   drugE <span class="hljs-number">3.345003</span>&gt; &gt; # 进行方差分析&gt; fit &lt;- aov(response ~ trt)&gt; summary(fit)            Df Sum Sq Mean Sq F value   Pr(&gt;F)    trt          <span class="hljs-number">4</span> <span class="hljs-number">1351.4</span>   <span class="hljs-number">337.8</span>   <span class="hljs-number">32.43</span> <span class="hljs-number">9.82e-13</span> ***Residuals   <span class="hljs-number">45</span>  <span class="hljs-number">468.8</span>    <span class="hljs-number">10.4</span>                     ---Signif. codes:  <span class="hljs-number">0</span> ‘***’ <span class="hljs-number">0.001</span> ‘**’ <span class="hljs-number">0.01</span> ‘*’ <span class="hljs-number">0.05</span> ‘.’ <span class="hljs-number">0.1</span> ‘ ’ <span class="hljs-number">1</span></code></pre><p>方差分析的结果中，各项数字的含义可以参照表2-1。</p><h4 id="查看各水平对应的组均值的差异"><a href="#查看各水平对应的组均值的差异" class="headerlink" title="查看各水平对应的组均值的差异"></a>查看各水平对应的组均值的差异</h4><p>gplots包中的plotmeans()可以用来绘制带有置信区间的组均值图形。如图9-1所示，图形展示了带有95%的置信区间的各疗法均值，可以清楚看到它们之间的差异。</p><pre><code class="hljs lisp">library(<span class="hljs-name">gplots</span>)plotmeans(<span class="hljs-name">response</span> ~ trt, xlab=<span class="hljs-string">"Treatment"</span>, ylab=<span class="hljs-string">"Response"</span>,          main=<span class="hljs-string">"Mean Plot\nwith 95% CI"</span>)detach(<span class="hljs-name">cholesterol</span>)</code></pre><p><img src="/article/2ad44fb9/2-1.png" srcset="/img/loading.gif" alt="2-1"></p><p><strong>2-1 五种降低胆固醇药物疗法的均值，含95%的置信区间</strong></p><h4 id="多重比较"><a href="#多重比较" class="headerlink" title="多重比较"></a>多重比较</h4><p>虽然ANOVA对各疗法的F检验表明五种药物疗法效果不同，但是并没有告诉你哪种疗法与其他疗法不同。多重比较可以解决这个问题。例如，TukeyHSD()函数提供了对各组均值差异的成对检验。</p><pre><code class="hljs angelscript">&gt; TukeyHSD(fit)  Tukey multiple comparisons of means    <span class="hljs-number">95</span>% family-wise confidence levelFit: aov(formula = response ~ trt)$trt                  diff        lwr       upr     p adj<span class="hljs-number">2</span>times<span class="hljs-number">-1</span>time   <span class="hljs-number">3.44300</span> <span class="hljs-number">-0.6582817</span>  <span class="hljs-number">7.544282</span> <span class="hljs-number">0.1380949</span><span class="hljs-number">4</span>times<span class="hljs-number">-1</span>time   <span class="hljs-number">6.59281</span>  <span class="hljs-number">2.4915283</span> <span class="hljs-number">10.694092</span> <span class="hljs-number">0.0003542</span>drugD<span class="hljs-number">-1</span>time    <span class="hljs-number">9.57920</span>  <span class="hljs-number">5.4779183</span> <span class="hljs-number">13.680482</span> <span class="hljs-number">0.0000003</span>drugE<span class="hljs-number">-1</span>time   <span class="hljs-number">15.16555</span> <span class="hljs-number">11.0642683</span> <span class="hljs-number">19.266832</span> <span class="hljs-number">0.0000000</span><span class="hljs-number">4</span>times<span class="hljs-number">-2</span>times  <span class="hljs-number">3.14981</span> <span class="hljs-number">-0.9514717</span>  <span class="hljs-number">7.251092</span> <span class="hljs-number">0.2050382</span>drugD<span class="hljs-number">-2</span>times   <span class="hljs-number">6.13620</span>  <span class="hljs-number">2.0349183</span> <span class="hljs-number">10.237482</span> <span class="hljs-number">0.0009611</span>drugE<span class="hljs-number">-2</span>times  <span class="hljs-number">11.72255</span>  <span class="hljs-number">7.6212683</span> <span class="hljs-number">15.823832</span> <span class="hljs-number">0.0000000</span>drugD<span class="hljs-number">-4</span>times   <span class="hljs-number">2.98639</span> <span class="hljs-number">-1.1148917</span>  <span class="hljs-number">7.087672</span> <span class="hljs-number">0.2512446</span>drugE<span class="hljs-number">-4</span>times   <span class="hljs-number">8.57274</span>  <span class="hljs-number">4.4714583</span> <span class="hljs-number">12.674022</span> <span class="hljs-number">0.0000037</span>drugE-drugD    <span class="hljs-number">5.58635</span>  <span class="hljs-number">1.4850683</span>  <span class="hljs-number">9.687632</span> <span class="hljs-number">0.0030633</span>&gt; par(las=<span class="hljs-number">2</span>)&gt; par(mar=c(<span class="hljs-number">5</span>,<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>))&gt; plot(TukeyHSD(fit))</code></pre><p>成对比较图形如图2-2所示。图形中置信区间包含0的疗法说明差异不显著（p&gt;0.05）。</p><p><img src="/article/2ad44fb9/2-2.png" srcset="/img/loading.gif" alt="2-2"></p><p><strong>图2-2 Tukey HSD均值成对比较图</strong></p><h4 id="评估检验的假设条件"><a href="#评估检验的假设条件" class="headerlink" title="评估检验的假设条件"></a>评估检验的假设条件</h4><p>根据2.1中我们讲的关于方差分析的推导中，我们知道，方差分析结果的有效性是建立在一系列假设条件之上的，因此，在我们使用方差分析模型时，需要评估进行方差分析的数据，是否符合模型使用的假设条件。</p><h5 id="正态性检验"><a href="#正态性检验" class="headerlink" title="正态性检验"></a>正态性检验</h5><p>第一，在建立模型时，我们假设因变量是服从正态分布的，需要进行正态性检验。</p><p>正态性检验的方法有两种，一是通过QQ图进行检验。</p><pre><code class="hljs routeros"><span class="hljs-comment"># QQ plot</span>library(car)qqPlot(lm(response ~ trt, <span class="hljs-attribute">data</span>=cholesterol),       <span class="hljs-attribute">simulate</span>=<span class="hljs-literal">TRUE</span>, <span class="hljs-attribute">main</span>=<span class="hljs-string">"Q-Q Plot"</span>, <span class="hljs-attribute">labels</span>=<span class="hljs-literal">FALSE</span>)</code></pre><p><img src="/article/2ad44fb9/2-3.png" srcset="/img/loading.gif" alt="2-3"></p><p>除此之外，R里面也提供了一些package来进行正态性检验。</p><h6 id="K-S-test"><a href="#K-S-test" class="headerlink" title="K-S test"></a>K-S test</h6><p>统计学里, Kolmogorov–Smirnov 检验(亦称：K–S 检验)是用来检验数据是否符合某种分布的一种非参数检验。其原假设$H_0$:两个数据分布一致或者数据符合理论分布。在R语言里，我们可以使用<code>ks.test(x, pnorm)</code>进行正态性检验，若结果中的p值大于0.05，则数据符合正态分布。</p><h6 id="Anderson–Darling-test"><a href="#Anderson–Darling-test" class="headerlink" title="Anderson–Darling test"></a><strong>Anderson–Darling test</strong></h6><p>Anderson–Darling检验是一种用来检验给定的样本是否来自于某个确定的概率分布的统计检验方法。在R语言中，我们可以从<strong>nortest包</strong>中的<code>ad.test()</code>进行检验。若结果中的p值大于0.05，则数据符合正态分布。</p><h6 id="Shapiro-Wilk-test"><a href="#Shapiro-Wilk-test" class="headerlink" title="Shapiro-Wilk test"></a><strong>Shapiro-Wilk test</strong></h6><p>Shapiro-Wilk检验在小样本情况下，是很普通的正态性检验方法，<code>Shapiro.test()</code>在默认安装的<strong>stats</strong>包中。原假设$H_0$: 数据符合正态分布。</p><h6 id="Lilliefor-test"><a href="#Lilliefor-test" class="headerlink" title="Lilliefor test"></a><strong>Lilliefor test</strong></h6><p>Lilliefor test是基于Kolmogorov–Smirnov test的一种正态性检验。原假设$H_0$: 数据符合正态分布，<code>lillie.test()</code>也在<strong>nortest包</strong>中。</p><h5 id="方差齐性检验"><a href="#方差齐性检验" class="headerlink" title="方差齐性检验"></a>方差齐性检验</h5><p>因为方差分析的实质是检验多个水平的均值是否有显著差异，如果各个水平的观察值方差差异太大，只检验均值之间的差异就没有意义了，所以要进行方差齐性检验。</p><p>Bartlett test可以用来检验数据的方差齐性。</p><pre><code class="hljs armasm">&gt; <span class="hljs-keyword">bartlett.test(response </span>~ trt, <span class="hljs-meta">data</span><span class="hljs-symbol">=cholesterol</span>)<span class="hljs-keyword">Bartlett </span>test of homogeneity of variances<span class="hljs-symbol">data</span>:  response <span class="hljs-keyword">by </span>trt<span class="hljs-keyword">Bartlett's </span>K-squared = <span class="hljs-number">0</span>.<span class="hljs-number">57975</span>, df = <span class="hljs-number">4</span>, p-value = <span class="hljs-number">0</span>.<span class="hljs-number">9653</span></code></pre><p>Bartlett检验表明五组的方差并没有显著不同（p=0.97）。其他检验如Fligner-Killeen检验<br>（<code>fligner.test()</code>函数）和Brown-Forsythe检验（<strong>HH包</strong>中的<code>hov()</code>函数）此处没有做演示，但它们获得的结果与Bartlett检验相同。</p><p>不过，方差齐性分析对离群点非常敏感。可利用<strong>car包</strong>中的<code>outlierTest()</code>函数来检测离群点：</p><pre><code class="hljs yaml"><span class="hljs-string">outlierTest(fit)</span><span class="hljs-literal">No</span> <span class="hljs-string">Studentized</span> <span class="hljs-string">residuals</span> <span class="hljs-string">with</span> <span class="hljs-string">Bonferroni</span> <span class="hljs-string">p</span> <span class="hljs-string">&lt;</span> <span class="hljs-number">0.05</span><span class="hljs-string">Largest</span> <span class="hljs-string">|rstudent|:</span>   <span class="hljs-string">rstudent</span> <span class="hljs-string">unadjusted</span> <span class="hljs-string">p-value</span> <span class="hljs-string">Bonferroni</span> <span class="hljs-string">p</span><span class="hljs-number">19</span> <span class="hljs-number">2.251149</span>           <span class="hljs-number">0.029422</span>           <span class="hljs-string">NA</span></code></pre><p>从输出结果来看，并没有证据说明胆固醇数据中含有离群点（当p&gt;1时将产生NA）。因此根据正态性检验、方差齐性检验和离群点检验，该数据似乎可以用ANOVA模型拟合得很好。<strong>这些方法反过来增强了我们对于所得结果的信心。</strong></p><h2 id="3-双因素方差分析"><a href="#3-双因素方差分析" class="headerlink" title="3 双因素方差分析"></a>3 双因素方差分析</h2><h3 id="3-1-推导过程"><a href="#3-1-推导过程" class="headerlink" title="3.1 推导过程"></a>3.1 推导过程</h3><p>在很多种情况下，只考虑一个指标对观察值的影响，显然是不够的，这时就会用到多因素方差分析。双因素方差分析和多因素方差分析在原理上是相似的，这里为了书写简便，我们只以双因素方差分析为例进行推导。</p><p>还是以田间实验的例子帮助理解推导过程，我们设有两个因素$A, B$，分别有$k, l$个水平（例如$A$为品种，有$k$个；$B$为播种量，考虑$l$种不同的数值，如20斤／亩，25斤／亩，……）．$A$的水平$i$与$B$的水平$j$的组合记为$(i,j)$，其试验结果记为 $Y_{ij}, i = 1, · · ·, k,j = 1,…, l$．统计模型定为</p><script type="math/tex; mode=display">Y_{ij} = \mu + a_i + b_j + e_{ij}，i= 1, · · ·, k,j = 1,· · ·, l\qquad (3.1)</script><p>为解释这模型，首先把右边分成两部分：$e_{ij}$为随机误差，它包含了未加控制的因素($A,B$以外的因素）及大量随机因素的影响．假定</p><script type="math/tex; mode=display">E(e_{ij})=0, 0<Var(e_{ij})={\sigma}^2<\infty,一切e_{ij}独立同分布\qquad(3.2)</script><p>另一部分$\mu + a_i + b_j$，它显示水平组合$(i,j)$的平均效应．它可以又分解为三部分：$\mu$是总平均（一切水平组合效应的平均），是一个基准．$a_i$表示由$A$的水平$i$带来的增加部分，称为因素$A$的水平$i$的效应．$b_j$有类似的解释．调整$\mu$的值，我们可以补充要求：</p><script type="math/tex; mode=display">a_1+···+a_k=0,b_1+···+b_l=0 \qquad (3.3)</script><p>如果$(3.3)$式不成立，则分别把$\mu$换为 $\mu + \bar{a}+\bar{b}$，$a_i$换为$a_i-\bar{a}$，$b_j$换为$b_j-\bar{b}$，则$(3.1)$式不变，而$(3.3)$式成立。</p><p>约束条件$(3.3)$给了$a_i，b_j$的意义一种更清晰的解释：$a_i&gt;0$ 表示A的水平$i$的效应在$A$的全部水平的平均效应之上，$a_i&lt;0$ 则相反。另外，这个约束条件也给了$\mu，a_i,b_j$的 一个适当的估计法：把$Y_{ij}$对一切$i,j$相加．注意到$(3.3)$，有</p><script type="math/tex; mode=display">\sum_{i=1}^{k}\sum_{j=1}^{l}Y_{ij}= kl\mu+\sum_{i=1}^{k}\sum_{j=1}^{l}e_{ij} \qquad (3.4)</script><p>由$(3.2)$得，</p><script type="math/tex; mode=display">\bar{Y}=\sum_{i=1}^{k}\sum_{j=1}^{l}Y_{ij}/kl \qquad (3.5)</script><p>是$\mu$的一个无偏估计。其次，有</p><script type="math/tex; mode=display">\sum_{j=1}^{l}Y_{ij}=l\mu+la+\sum_{j=1}^{l}e_{ij} \qquad (3.6)</script><p>于是，记</p><script type="math/tex; mode=display">\bar{Y_i}=\sum_{j=1}^{l}Y_{ij}/l, \quad \bar{Y_j}=\sum_{i=1}^{k}Y_{ij}/k \qquad (3.7)</script><p>由$(3.7)$知，$\bar{Y_j}$为$\mu+a_i$的一个无偏估计。于是得到$a_i$的一个无偏估计为</p><script type="math/tex; mode=display">\hat{a_i}=\bar{Y_i}-\bar{Y}, i=1,\cdots,k \qquad(3.8)</script><p>同理，</p><script type="math/tex; mode=display">\hat{b_j}=\bar{Y_j}-\bar{Y}, j=1,\cdots,l \qquad(3.9)</script><p>$\hat{a_i},\hat{b_j}$适合约束条件$(3.3)$。</p><p>下面进行方差分析，要设法把总平方和</p><script type="math/tex; mode=display">SS=\sum_{i=1}^{k}\sum_{j=1}^{l}(Y_{ij}-\bar{Y})^2</script><p>分解为三部分：$SS_A,SS_B,SS_e$，分别表示因素$A,B$和随机误差的影响。这种分解的主要目的是假设检验：</p><script type="math/tex; mode=display">H_{0A}:a_1=\cdots=a_k=0 \qquad(3.10)</script><p>和</p><script type="math/tex; mode=display">H_{0B}:b_1=\cdots=b_k=0 \qquad(3.11)</script><p>$H_0A$成立表示因素$A$对指标其实无影响。在实际问题中，绝对无影响的场合少见，但如影响甚小以致被随机误差所掩盖时，这种影响事实上等于没有。因此，拿$SS_A$和$SS_e$的比作为检验统计量正符合这一想法．</p><p>接下来讲一下方差分解的小技巧：</p><script type="math/tex; mode=display">Y_{ij}-\bar{Y}=(\bar{Y_i}-\bar{Y}) + (\bar{Y_j}-\bar{Y})+(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})</script><p>两边平方，对$i,j$求和，结合约束条件(3.3)，注意到</p><script type="math/tex; mode=display">\sum_{i=1}^{l}(\bar{Y_{i}}-\bar{Y})=0， \sum_{j=1}^{k}(\bar{Y_{j}}-\bar{Y})=0，</script><script type="math/tex; mode=display">\sum_{i=1}^{k}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})=\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})=0</script><script type="math/tex; mode=display">\sum_{i=1}^{k}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y}) = k\bar{Y_j}\\=\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y}) = l\bar{Y_i}</script><p>即知所有交叉积之和皆为0，而得到</p><script type="math/tex; mode=display">\begin{align}SS&=l\sum_{i=1}^{k}(\bar{Y_{i}}-\bar{Y})^2+k\sum_{j=1}^{l}(\bar{Y_{j}}-\bar{Y})^2+\sum_{i=1}^{k}\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})^2 \\&=SS_A + SS_B + SS_e \qquad(3.12)\end{align}</script><p>第一个平方和可以作为因素$A$的影响的衡量，从前述$\bar{Y_{i}}-\bar{Y}$作为 $a_i$的估计可以理解第二个平方和同理。至于第三个平方和可作为随机误差的影响这一点， 直接看不甚明显。可以从两个角度去理解：在$SS$中去掉$SS_A$ 和$SS_B$后，剩余下的再没有其他系统性因素的影响，故只能作为$SS_e$。另外，由模型$(3.1)$及约束条件$(3.3)$，易知</p><script type="math/tex; mode=display">\begin{align}Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y} &= (\mu + a_i + b_j + e_{ij}) - (\mu + a_i + \bar{e_{i}}) - (\mu + b_j + \bar{e_{j}} ) + (\mu + \bar{e})  \\&=e_{ij}-\bar{e_i}-\bar{e_j}+\bar{e} \qquad(3.13)\end{align}</script><p>这里面已经毫无$\mu,a_i,b_j$的影响，而只含随机误差。</p><p>得到分解式$(3.12)$后，我们就可以像单囚素情况那样，写出下面的方差分析表：<br>$SS_A , SS_B$ 自由度分别为其水平数减去1，这一点与单因素情况相同．总和自由度为全部观察值数目$kl$减去1．剩下的就是误差平方和自由度：</p><script type="math/tex; mode=display">(kl - 1) - (k - 1) - (l - 1) = (k - 1) (l - 1)</script><p><strong>表3.1 双因素方差分析表</strong></p><div class="table-container"><table><thead><tr><th>项目</th><th>$SS$</th><th>自由度</th><th>$MS$</th><th>$F$比</th><th>显著性</th></tr></thead><tbody><tr><td>$A$</td><td>$SS_A$</td><td>$k-1$</td><td>$MS_A$</td><td>$MS_A / MS_e$</td><td><em>, *</em>, 或无</td></tr><tr><td>$B$</td><td>$SS_B$</td><td>$l-1$</td><td>$MS_B$</td><td>$MS_B / MS_e$</td><td></td></tr><tr><td>误差</td><td>$SS_e$</td><td>$(k - 1) (l - 1)$</td><td>$MS_e$</td><td></td><td></td></tr><tr><td>总和</td><td>$SS$</td><td>$kl-1$</td><td></td><td></td></tr></tbody></table></div><p>还有一点要注意：在采纳模型$(3.1)$时，我们事实上引进了 一 种假定，即两因素$A,B$对指标的效应是可以叠加的．换一种方式说：因素$A$的各水平的优劣比较，与因素$B$处在哪个水平无关，反之亦然．更一般的情况是：$A,B$两因子有“交互作用 ＂ 。这时在模型(5.13)中，还要加上表示交互作用的项$c_{ij}$．这时不仅统计分析复杂化了，尤其是分析结果的解释也复杂化了．本文档暂不讨论这种情况。在一个特定的问题中，交互作用是否需要考虑，在很大程度上取决于问题的实际背景和经验．有时，通过试验数据的分析也可以看出一些问题。例如，若误差方差$\sigma^2$的估计$MS_e $反常地大，则有可能是由于交互作用所致．因为可以证明：若交互作用确实存在而未加考虑，则它的影响进入随机误差而增大了$MS_e$。</p><h3 id="3-2-代码实例"><a href="#3-2-代码实例" class="headerlink" title="3.2 代码实例"></a>3.2 代码实例</h3><p>在双因素方差分析中，受试者被分配到两因子的交叉类别组中。以基础安装中的ToothGrowth数据集为例，随机分配60只豚鼠，分别采用两种喂食方法（橙汁或维生素C），各喂食方法中抗坏血酸含量有三种水平（0.5mg、1mg或2mg），每种处理方式组合都被分配10只豚鼠，牙齿长度为因变量。</p><pre><code class="hljs angelscript">&gt; attach(ToothGrowth)&gt; table(supp, dose)    dosesupp <span class="hljs-number">0.5</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>  OJ  <span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>  VC  <span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>&gt; &gt; aggregate(len, by=list(supp, dose), FUN=mean)  Group<span class="hljs-number">.1</span> Group<span class="hljs-number">.2</span>     x<span class="hljs-number">1</span>      OJ     <span class="hljs-number">0.5</span> <span class="hljs-number">13.23</span><span class="hljs-number">2</span>      VC     <span class="hljs-number">0.5</span>  <span class="hljs-number">7.98</span><span class="hljs-number">3</span>      OJ     <span class="hljs-number">1.0</span> <span class="hljs-number">22.70</span><span class="hljs-number">4</span>      VC     <span class="hljs-number">1.0</span> <span class="hljs-number">16.77</span><span class="hljs-number">5</span>      OJ     <span class="hljs-number">2.0</span> <span class="hljs-number">26.06</span><span class="hljs-number">6</span>      VC     <span class="hljs-number">2.0</span> <span class="hljs-number">26.14</span>&gt; &gt; aggregate(len, by=list(supp, dose), FUN=sd)  Group<span class="hljs-number">.1</span> Group<span class="hljs-number">.2</span>        x<span class="hljs-number">1</span>      OJ     <span class="hljs-number">0.5</span> <span class="hljs-number">4.459709</span><span class="hljs-number">2</span>      VC     <span class="hljs-number">0.5</span> <span class="hljs-number">2.746634</span><span class="hljs-number">3</span>      OJ     <span class="hljs-number">1.0</span> <span class="hljs-number">3.910953</span><span class="hljs-number">4</span>      VC     <span class="hljs-number">1.0</span> <span class="hljs-number">2.515309</span><span class="hljs-number">5</span>      OJ     <span class="hljs-number">2.0</span> <span class="hljs-number">2.655058</span><span class="hljs-number">6</span>      VC     <span class="hljs-number">2.0</span> <span class="hljs-number">4.797731</span>&gt; &gt; dose &lt;- factor(dose)#dose变量被转换为因子变量，这样aov()函数就会将它当做一个分组变量，而不是一个数值型协变量&gt; # condider <span class="hljs-built_in">int</span>eractive factor&gt; fit &lt;- aov(len ~ supp*dose)&gt; summary(fit)            Df Sum Sq Mean Sq F value   Pr(&gt;F)    supp         <span class="hljs-number">1</span>  <span class="hljs-number">205.4</span>   <span class="hljs-number">205.4</span>  <span class="hljs-number">15.572</span> <span class="hljs-number">0.000231</span> ***dose         <span class="hljs-number">2</span> <span class="hljs-number">2426.4</span>  <span class="hljs-number">1213.2</span>  <span class="hljs-number">92.000</span>  &lt; <span class="hljs-number">2e-16</span> ***supp:dose    <span class="hljs-number">2</span>  <span class="hljs-number">108.3</span>    <span class="hljs-number">54.2</span>   <span class="hljs-number">4.107</span> <span class="hljs-number">0.021860</span> *  Residuals   <span class="hljs-number">54</span>  <span class="hljs-number">712.1</span>    <span class="hljs-number">13.2</span>                     ---Signif. codes:  <span class="hljs-number">0</span> ‘***’ <span class="hljs-number">0.001</span> ‘**’ <span class="hljs-number">0.01</span> ‘*’ <span class="hljs-number">0.05</span> ‘.’ <span class="hljs-number">0.1</span> ‘ ’ <span class="hljs-number">1</span></code></pre><p>计算结果表明，主效应和交互效应都是显著的。</p><p>有多种方式对结果进行可视化处理。此处可用interaction.plot()函数来展示双因素方<br>差分析的交互效应。</p><pre><code class="hljs routeros"><span class="hljs-comment"># interactive effect</span>interaction.plot(dose, supp, len, <span class="hljs-attribute">type</span>=<span class="hljs-string">"b"</span>,                 <span class="hljs-attribute">col</span>=c("red","blue"), <span class="hljs-attribute">pch</span>=c(16, 18),                 main = <span class="hljs-string">"Interaction between Dose and Supplement Type"</span>)</code></pre><p><img src="/article/2ad44fb9/3-1.png" srcset="/img/loading.gif" alt="3-1"></p><p><strong>图3-1 各种剂量喂食下豚鼠牙齿长度的均值（interaction.plot()函数绘制）</strong></p><p>还可以用gplots包中的plotmeans()函数来展示交互效应。<img src="/article/2ad44fb9/3-2.png" srcset="/img/loading.gif" alt="3-2"><strong>图3-2 喂食方法和剂量对牙齿生长的交互作用。用plotmeans()函数绘制的95%的置</strong><br><strong>信区间的牙齿长度均值</strong></p><p>图形展示了均值、误差棒（95%的置信区间）和样本大小。<br>最后，你还能用HH包中的interaction2wt()函数来可视化结果，图形对任意顺序的因子设计的主效应和交互效应都会进行展示（图3-3）。</p><pre><code class="hljs lisp">library(<span class="hljs-name">HH</span>)interaction2wt(<span class="hljs-name">len</span>~supp*dose)&gt; &gt; detach(<span class="hljs-name">ToothGrowth</span>)</code></pre><p><img src="/article/2ad44fb9/3-3.png" srcset="/img/loading.gif" alt="3-3"></p><p><strong>图3-3 ToothGrowth数据集的主效应和交互效应。图形由interaction2wt()函数创建</strong></p><p>以上三幅图形都表明随着橙汁和维生素C中的抗坏血酸剂量的增加，牙齿长度变长。对于0.5mg和1mg剂量，橙汁比维生素C更能促进牙齿生长；对于2mg剂量的抗坏血酸，两种喂食方法下牙齿长度增长相同。</p><h4 id="参考书目："><a href="#参考书目：" class="headerlink" title="参考书目："></a>参考书目：</h4><ol><li>陈希孺，概率论与数理统计</li><li>Robert I. Kabacoff, R in Action.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;方差分析&quot;&gt;&lt;a href=&quot;#方差分析&quot; class=&quot;headerlink&quot; title=&quot;方差分析&quot;&gt;&lt;/a&gt;方差分析&lt;/h1&gt;&lt;h2 id=&quot;1-概要&quot;&gt;&lt;a href=&quot;#1-概要&quot; class=&quot;headerlink&quot; title=&quot;1 概要&quot;&gt;&lt;/a&gt;1 概要&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;方差分析&lt;/strong&gt;(&lt;strong&gt;Analysis of variance, ANOVA&lt;/strong&gt;) 主要研究分类变量作为自变量时，对因变量的影响是否是显著的。&lt;/p&gt;
&lt;p&gt;方差分析的方法是由20世纪的统计学家Ronald Aylmer Fisher在1918年到1925年之间提出并陆续完善起来的，该方法刚开始是用于解决田间实验的数据分析问题，因此，方差分析的学习是和实验设计、实验数据的分析密不可分的。&lt;/p&gt;
&lt;p&gt;实验设计和方差分析都有自己相应的语言。因此，在这里我们通过一个焦虑症治疗的实例，先了解一些术语，并且思考一下，方差分析主要用于解决什么样的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="概率统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="概率统计" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>第九章-时序数据</title>
    <link href="http://yoursite.com/article/32af8df.html"/>
    <id>http://yoursite.com/article/32af8df.html</id>
    <published>2020-06-29T14:00:38.000Z</published>
    <updated>2020-06-30T16:05:49.690Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/article/32af8df/时序数据.png" srcset="/img/loading.gif" alt="时序数据"></p><a id="more"></a>明天要考试了--明后天再补全了~回答有些过于简陋了问题1(a) <pre><code class="hljs prolog">df = pd.read_csv(<span class="hljs-string">'data/time_series_one.csv'</span>, parse_dates=[<span class="hljs-string">'日期'</span>])df[<span class="hljs-string">'日期'</span>].dt.dayofweek[df[<span class="hljs-string">'销售额'</span>].idxmax()]</code></pre><p>输出：</p><p>6<br>（b）</p><pre><code class="hljs routeros">holiday = pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20170501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20170503'</span>).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20171001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20171007'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20180215'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20180221'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20180501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20180503'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20181001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20181007'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20190204'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20190224'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20190501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20190503'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20191001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20191007'</span>))result = df[~df[<span class="hljs-string">'日期'</span>].isin(holiday)].set_index(<span class="hljs-string">'日期'</span>).resample(<span class="hljs-string">'MS'</span>).sum()result.head()</code></pre><p>(c)</p><pre><code class="hljs sas">result = df[df[<span class="hljs-string">'日期'</span>].dt.dayofweek.<span class="hljs-meta">isin(</span>[5,6])].<span class="hljs-meta">set</span><span class="hljs-meta">_index(</span><span class="hljs-string">'日期'</span>).resample(<span class="hljs-string">'QS'</span>)<span class="hljs-meta">.sum(</span>)result.head()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/article/32af8df/时序数据.png&quot; srcset=&quot;/img/loading.gif&quot; alt=&quot;时序数据&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pandas下" scheme="http://yoursite.com/categories/pandas%E4%B8%8B/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>3常见分布与假设检验</title>
    <link href="http://yoursite.com/article/7c20a36d.html"/>
    <id>http://yoursite.com/article/7c20a36d.html</id>
    <published>2020-06-27T15:40:36.000Z</published>
    <updated>2020-07-01T06:23:48.145Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-一般随机变量"><a href="#1-一般随机变量" class="headerlink" title="1 一般随机变量"></a>1 一般随机变量</h2><h3 id="1-1-随机变量的两种类型"><a href="#1-1-随机变量的两种类型" class="headerlink" title="1.1 随机变量的两种类型"></a>1.1 随机变量的两种类型</h3><p>根据随机变量可能取值的个数分为离散型（取值有限）和连续型（取值无限）两类。</p><h3 id="1-2-离散型随机变量"><a href="#1-2-离散型随机变量" class="headerlink" title="1.2 离散型随机变量"></a>1.2 离散型随机变量</h3><p>对于离散型随机变量，使用概率质量函数（probability mass function），简称PMF，来描述其分布律。</p><p>假定离散型随机变量X，共有n个取值，$X_1$, $X_2$, ……, $X_n$, 那么</p><script type="math/tex; mode=display">P(X=X_n) \geq 0</script><script type="math/tex; mode=display">\Sigma_{1}^{n} P(X=X_n) =1</script><p>用到PMF的例子：二项分布，泊松分布<a id="more"></a></p><h3 id="1-3-连续型随机变量"><a href="#1-3-连续型随机变量" class="headerlink" title="1.3 连续型随机变量"></a>1.3 连续型随机变量</h3><p>对于连续型随机变量，使用概率密度函数（probability density function），简称PDF，来描述其分布情况。</p><p>连续型随机变量的特点在于取任何固定值的概率都为0，因此讨论其在特定值上的概率是没有意义的，应当讨论其在某一个区间范围内的概率，这就用到了概率密度函数的概念。</p><p>假定连续型随机变量X，f(x)为概率密度函数， 对于任意实数范围如[a,b]，有</p><script type="math/tex; mode=display">P \lbrace a\leq X \leq b\rbrace = \int ^b_a f(x) {\rm d}x</script><p>用到PDF的例子：均匀分布，正态分布，指数分布</p><p>对于连续型随机变量，通常还会用到累积分布函数 (cumulative distribution function)，简称CDF，来描述其性质，在数学上CDF是PDF的积分形式。</p><p>分布函数F(x)在点x处的函数值表示X落在区间(−∞,x]内的概率，所以分布函数就是定义域为R的一个普通函数，因此我们可以把概率问题转化为函数问题，从而可以利用普通的函数知识来研究概率问题，增大了概率的研究范围。</p><h2 id="2-常见分布"><a href="#2-常见分布" class="headerlink" title="2 常见分布"></a>2 常见分布</h2><p>本节通过一些实际例子来认识各种不同的分布及其应用场景</p><h3 id="2-1-离散型分布"><a href="#2-1-离散型分布" class="headerlink" title="2.1 离散型分布"></a>2.1 离散型分布</h3><h4 id="2-1-1-二项分布（Binomial-distribution）"><a href="#2-1-1-二项分布（Binomial-distribution）" class="headerlink" title="2.1.1 二项分布（Binomial distribution）"></a>2.1.1 二项分布（Binomial distribution）</h4><p>二项分布可以认为是一种只有两种结果（成功/失败)的单次试验重复多次后成功次数的分布概率。</p><p>二项分布需要满足以下条件：</p><ul><li>试验次数是固定的</li><li>每次试验都是独立的</li><li>对于每次试验成功的概率都是一样的</li></ul><p>一些二项分布的例子：</p><ul><li>销售电话成功的次数</li><li>一批产品中有缺陷的产品数量</li><li>掷硬币正面朝上的次数</li><li>在一袋糖果中取糖果吃，拿到红色包装的次数</li></ul><p>在n次试验中，单次试验成功率为p，失败率q=1-p，则出现成功次数的概率为</p><script type="math/tex; mode=display">P(X=x) = C_n^x p^x q^{n-x}</script><h4 id="2-1-2-泊松分布（Poisson-distribution）"><a href="#2-1-2-泊松分布（Poisson-distribution）" class="headerlink" title="2.1.2 泊松分布（Poisson distribution）"></a>2.1.2 泊松分布（Poisson distribution）</h4><p>泊松分布是用来描述泊松试验的一种分布，满足以下两个特征的试验可以认为是泊松试验：</p><ul><li>所考察的事件在任意两个长度相等的区间里发生一次的机会均等</li><li>所考察的事件在任何一个区间里发生与否和在其他区间里发生与否没有相互影响，即是独立的</li></ul><p>泊松分布需要满足一些条件：</p><ul><li>试验次数n趋向于无穷大</li><li>单次事件发生的概率p趋向于0</li><li>np是一个有限的数值</li></ul><p>泊松分布的一些例子：</p><ul><li>一定时间段内，某航空公司接到的订票电话数</li><li>一定时间内，到车站等候公交汽车的人数</li><li>一匹布上发现的瑕疵点的个数</li><li>一定页数的书刊上出现的错别字个数</li></ul><p>一个服从泊松分布的随机变量X，在具有比率参数（rate parameter）λ （λ=np）的一段固定时间间隔内，事件发生次数为i的概率为</p><script type="math/tex; mode=display">P\lbrace X= i \rbrace = e^{-λ} \frac{λ^i}{i!}</script><h4 id="2-1-3-二项分布，泊松分布，正态分布的关系"><a href="#2-1-3-二项分布，泊松分布，正态分布的关系" class="headerlink" title="2.1.3 二项分布，泊松分布，正态分布的关系"></a>2.1.3 二项分布，泊松分布，正态分布的关系</h4><p>这三个分布之间具有非常微妙的关联。</p><p>当n很大，p很小时，如<em>n</em> ≥ 100 and <em>np</em> ≤ 10时，二项分布可以近似为泊松分布。</p><p>当λ很大时，如λ≥1000时，泊松分布可以近似为正态分布。</p><p>当n很大时，np和n(1-p)都足够大时，如n ≥ 100 , np  ≥10，n(1-p) ≥10时，二项分布可以近似为正态分布。</p><h4 id="2-1-4-其他离散型随机分布"><a href="#2-1-4-其他离散型随机分布" class="headerlink" title="2.1.4 其他离散型随机分布"></a>2.1.4 其他离散型随机分布</h4><p>除了二项分布和泊松分布以外，还有其他一些不太常用的离散型分布。</p><h5 id="几何分布（Geometric-distribution）"><a href="#几何分布（Geometric-distribution）" class="headerlink" title="几何分布（Geometric distribution）"></a>几何分布（Geometric distribution）</h5><p>考虑独立重复试验，几何分布描述的是经过k次试验才首次获得成功的概率，假定每次成功率为p，</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = {(1-p)}^{n-1} p</script><h5 id="负二项分布（Negative-binomial-distribution）"><a href="#负二项分布（Negative-binomial-distribution）" class="headerlink" title="负二项分布（Negative binomial distribution）"></a>负二项分布（Negative binomial distribution）</h5><p>考虑独立重复试验，负二项分布描述的是试验一直进行到成功r次的概率，假定每次成功率为p，</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = C_{n-1}^{r-1} p^r {(1-p)}^{n-r}</script><h5 id="超几何分布（Hypergeometric-Distribution）"><a href="#超几何分布（Hypergeometric-Distribution）" class="headerlink" title="超几何分布（Hypergeometric Distribution）"></a>超几何分布（Hypergeometric Distribution）</h5><p>超几何分布描述的是在一个总数为N的总体中进行有放回地抽样，其中在总体中k个元素属于一组，剩余N-k个元素属于另一组，假定从总体中抽取n次，其中包含x个第一组的概率为</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = \frac {C_{k}^{x} C_{N-k}^{n-x}} {C_{N}^{n}}</script><h3 id="2-2-连续型分布"><a href="#2-2-连续型分布" class="headerlink" title="2.2 连续型分布"></a>2.2 连续型分布</h3><h4 id="2-2-1-均匀分布-（Uniform-distribution）"><a href="#2-2-1-均匀分布-（Uniform-distribution）" class="headerlink" title="2.2.1 均匀分布 （Uniform distribution）"></a>2.2.1 均匀分布 （Uniform distribution）</h4><p>均匀分布指的是一类在定义域内概率密度函数处处相等的统计分布。</p><p>若X是服从区间[a,b]上的均匀分布，则记作X~U[a,b]。</p><p>均匀分布X的概率密度函数为</p><script type="math/tex; mode=display">f(x)=\begin{cases}\frac {1} {b-a} ,  &  a \leq x  \leq b \\0, & others\end{cases}</script><p>分布函数为</p><script type="math/tex; mode=display">F(x)=\begin{cases}0 ,  &  x< a \\(x-a)(b-a), & a \leq x  \leq b \\1, & x>b\end{cases}</script><p>均匀分布的一些例子：</p><ul><li>一个理想的随机数生成器</li><li>一个理想的圆盘以一定力度旋转后静止时的角度</li></ul><h4 id="2-2-2-正态分布-（Normal-distribution）"><a href="#2-2-2-正态分布-（Normal-distribution）" class="headerlink" title="2.2.2 正态分布 （Normal distribution）"></a>2.2.2 正态分布 （Normal distribution）</h4><p>正态分布，也叫做高斯分布，是最为常见的统计分布之一，是一种对称的分布，概率密度呈现钟摆的形状，其概率密度函数为</p><script type="math/tex; mode=display">f(x)=\frac{1}{\sqrt{2π}\sigma}e^{\frac{-(x-u)^2}{2\sigma^2}}</script><p>记为X ~ N(μ, $σ^2$) , 其中μ为正态分布的均值，σ为正态分布的标准差</p><p>有了一般正态分布后，可以通过公式变换将其转变为标准正态分布 Z ~ N(0,1)，</p><script type="math/tex; mode=display">Z=\frac {X-μ} {σ}</script><p>正态分布的一些例子：</p><ul><li>成人的身高</li><li>不同方向的气体分子的运动速度</li><li>测量物体质量时的误差</li></ul><p>正态分布在现实生活有着非常多的例子，这一点可以从中心极限定理来解释，中心极限定理说的是一组独立同分布的随机样本的平均值近似为正态分布，无论随机变量的总体符合何种分布。</p><h4 id="2-2-3-指数分布-（Exponential-distribution）"><a href="#2-2-3-指数分布-（Exponential-distribution）" class="headerlink" title="2.2.3 指数分布 （Exponential distribution）"></a>2.2.3 指数分布 （Exponential distribution）</h4><p>指数分布通常被广泛用在描述一个特定事件发生所需要的时间，在指数分布随机变量的分布中，有着很少的大数值和非常多的小数值。</p><p>指数分布的概率密度函数为</p><script type="math/tex; mode=display">f(x)=\begin{cases}λe^{-λx} ,  &   x  \geq 0 \\0, & x < 0\end{cases}</script><p>记为 X~E（λ),   其中λ被称为率参数（rate parameter），表示每单位时间发生该事件的次数。</p><p>分布函数为</p><script type="math/tex; mode=display">F(a) = P\{X \leq a\} = 1-e^{-λa},  a\geq 0</script><p>指数分布的一些例子：</p><ul><li>顾客到达一家店铺的时间间隔</li><li>从现在开始到发生地震的时间间隔</li><li>在产线上收到一个问题产品的时间间隔</li></ul><p>关于指数分布还有一个有趣的性质的是指数分布是无记忆性的，假定在等候事件发生的过程中已经过了一些时间，此时距离下一次事件发生的时间间隔的分布情况和最开始是完全一样的，就好像中间等候的那一段时间完全没有发生一样，也不会对结果有任何影响，用数学语言来表述是</p><script type="math/tex; mode=display">P\{X>s+t | X> t\} =P\{X>s\}</script><h4 id="2-2-4-其他连续分布"><a href="#2-2-4-其他连续分布" class="headerlink" title="2.2.4 其他连续分布"></a>2.2.4 其他连续分布</h4><p><strong>$\Gamma$分布</strong></p><p>常用来描述某个事件总共要发生n次的等待时间的分布</p><p><strong>威布尔分布 （Weibull distribution）</strong></p><p>常用来描述在工程领域中某类具有“最弱链”对象的寿命</p><h3 id="2-3-常见分布的均值和方差汇总"><a href="#2-3-常见分布的均值和方差汇总" class="headerlink" title="2.3 常见分布的均值和方差汇总"></a>2.3 常见分布的均值和方差汇总</h3><p>离散型分布</p><p><img src="/article/7c20a36d/discreteoverall.PNG" srcset="/img/loading.gif" alt="discreteoverall"></p><p>连续型分布</p><p><img src="/article/7c20a36d/continuousoverall.PNG" srcset="/img/loading.gif" alt="continuousoverall"></p><p>图片来自于 [Statistical Inference by Casella and Berger]</p><h3 id="2-4-Python-代码实战"><a href="#2-4-Python-代码实战" class="headerlink" title="2.4 Python 代码实战"></a>2.4 Python 代码实战</h3><h4 id="2-4-1-生成一组符合特定分布的随机数"><a href="#2-4-1-生成一组符合特定分布的随机数" class="headerlink" title="2.4.1 生成一组符合特定分布的随机数"></a>2.4.1 生成一组符合特定分布的随机数</h4><p>在Numpy库中，提供了一组random类可以生成特定分布的随机数</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy<span class="hljs-comment"># 生成大小为1000的符合b(10,0.5)二项分布的样本集</span>s = numpy.random.binomial(n=<span class="hljs-number">10</span>,p=<span class="hljs-number">0.5</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合P(1)的泊松分布的样本集</span>s = numpy.random.poisson(lam=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合U(0,1)均匀分布的样本集，注意在此方法中边界值为左闭右开区间</span>s = numpy.random.uniform(low=<span class="hljs-number">0</span>,high=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合N(0,1)正态分布的样本集，可以用normal函数自定义均值，标准差，也可以直接使用standard_normal函数</span>s = numpy.random.normal(loc=<span class="hljs-number">0</span>,scale=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)s = numpy.random.standard_normal(size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合E(1/2)指数分布的样本集，注意该方法中的参数为指数分布参数λ的倒数</span>s = numpy.random.exponential(scale=<span class="hljs-number">2</span>,size=<span class="hljs-number">1000</span>)</code></pre><p>除了Numpy，Scipy也提供了一组生成特定分布随机数的方法</p><pre><code class="hljs python"><span class="hljs-comment"># 以均匀分布为例，rvs可用来生成一组随机变量的值</span><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> statsstats.uniform.rvs(size=<span class="hljs-number">10</span>)</code></pre><h4 id="2-4-2-计算统计分布的PMF和PDF"><a href="#2-4-2-计算统计分布的PMF和PDF" class="headerlink" title="2.4.2 计算统计分布的PMF和PDF"></a>2.4.2 计算统计分布的PMF和PDF</h4><p>Scipy库提供了一组用于计算离散型随机变量PMF和连续型随机变量PDF的方法。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-comment"># 计算二项分布B(10,0.5)的PMF</span>x=range(<span class="hljs-number">11</span>)p=stats.binom.pmf(x, n=<span class="hljs-number">10</span>, p=<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 计算泊松分布P(1)的PMF</span>x=range(<span class="hljs-number">11</span>)p=stats.poisson.pmf(x, mu=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算均匀分布U(0,1)的PDF</span>x = numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)p= stats.uniform.pdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算正态分布N(0,1)的PDF</span>x = numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)p= stats.norm.pdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算指数分布E(1)的PDF</span>x = numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">1000</span>)p= stats.expon.pdf(x,loc=<span class="hljs-number">0</span>,scale=<span class="hljs-number">1</span>)</code></pre><h4 id="2-4-3-计算统计分布的CDF"><a href="#2-4-3-计算统计分布的CDF" class="headerlink" title="2.4.3 计算统计分布的CDF"></a>2.4.3 计算统计分布的CDF</h4><p>类似计算概率质量/密度函数的方法，只需将上节中的pmf或pdf替换为cdf，即可得到分布函数的值</p><pre><code class="hljs python"><span class="hljs-comment"># 以正态分布为例，计算正态分布N(0,1)的CDF</span>x = numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)p = stats.norm.cdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)</code></pre><h4 id="2-4-4-统计分布可视化"><a href="#2-4-4-统计分布可视化" class="headerlink" title="2.4.4 统计分布可视化"></a>2.4.4 统计分布可视化</h4><h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a><strong>二项分布</strong></h5><p>比较n=10，p=0.5的二项分布的真实概率质量和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx = range(<span class="hljs-number">11</span>)  <span class="hljs-comment"># 二项分布成功的次数（X轴）</span>t = stats.binom.rvs(<span class="hljs-number">10</span>,<span class="hljs-number">0.5</span>,size=<span class="hljs-number">10000</span>) <span class="hljs-comment"># B(10,0.5)随机抽样10000次</span>p = stats.binom.pmf(x, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>) <span class="hljs-comment"># B(10,0.5)真实概率质量</span>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.scatterplot(x,p,color=<span class="hljs-string">'purple'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Binomial distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/binomial.png" srcset="/img/loading.gif" alt="binomial"></p><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a><strong>泊松分布</strong></h5><p>比较λ=2的泊松分布的真实概率质量和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=range(<span class="hljs-number">11</span>)t= stats.poisson.rvs(<span class="hljs-number">2</span>,size=<span class="hljs-number">10000</span>)p=stats.poisson.pmf(x, <span class="hljs-number">2</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.scatterplot(x,p,color=<span class="hljs-string">'purple'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Poisson distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/poisson.png" srcset="/img/loading.gif" alt="poisson"></p><p>比较不同参数λ对应的概率质量函数，可以验证随着参数增大，泊松分布开始逐渐变得对称，分布也越来越均匀，趋近于正态分布</p><pre><code class="hljs python">x=range(<span class="hljs-number">50</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  lam <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">20</span>] :        p=stats.poisson.pmf(x, lam)        sns.lineplot(x,p,label=<span class="hljs-string">'lamda= '</span>+ str(lam))plt.title(<span class="hljs-string">'Poisson distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/poisson2.png" srcset="/img/loading.gif" alt="poisson2"></p><h5 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a><strong>均匀分布</strong></h5><p>比较U(0,1)的均匀分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)t= stats.uniform.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.uniform.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Uniforml distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/uniform.png" srcset="/img/loading.gif" alt="uniform"></p><h5 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a><strong>正态分布</strong></h5><p>比较N(0,1)的正态分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">100</span>)t= stats.norm.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.norm.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">100</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Normal distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/normal.png" srcset="/img/loading.gif" alt="normal"></p><p>比较不同均值和标准差组合的正态分布的概率密度函数</p><pre><code class="hljs python">x=numpy.linspace(<span class="hljs-number">-6</span>,<span class="hljs-number">6</span>,<span class="hljs-number">100</span>)p=stats.norm.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  mean, std <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>),(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)]:         p=stats.norm.pdf(x, mean, std)        sns.lineplot(x,p,label=<span class="hljs-string">'Mean: '</span>+ str(mean) + <span class="hljs-string">', std: '</span>+ str(std))plt.title(<span class="hljs-string">'Normal distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/uniform2.png" srcset="/img/loading.gif" alt="uniform2"></p><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a><strong>指数分布</strong></h5><p>比较E(1)的指数分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>)t= stats.expon.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.expon.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">100</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Exponential distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/expo.png" srcset="/img/loading.gif" alt="expo"></p><p>比较不同参数的指数分布的概率密度函数</p><pre><code class="hljs python">x=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  scale <span class="hljs-keyword">in</span> [<span class="hljs-number">0.2</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>] :        p=stats.expon.pdf(x, scale=scale)        sns.lineplot(x,p,label=<span class="hljs-string">'lamda= '</span>+ str(<span class="hljs-number">1</span>/scale))plt.title(<span class="hljs-string">'Exponential distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/expo2.png" srcset="/img/loading.gif" alt="expo2"></p><h2 id="3-假设检验"><a href="#3-假设检验" class="headerlink" title="3 假设检验"></a>3 假设检验</h2><h3 id="3-1-基本概念"><a href="#3-1-基本概念" class="headerlink" title="3.1 基本概念"></a>3.1 基本概念</h3><p>假设检验问题是统计推断中的一类重要问题，在总体的分布函数完全未知或只知其形式，不知其参数的情况，为了推断总体的某些未知特性，提出某些关于总体的假设，这类问题被称为假设检验。</p><h3 id="3-2-基本步骤"><a href="#3-2-基本步骤" class="headerlink" title="3.2 基本步骤"></a>3.2 基本步骤</h3><p>一个假设检验问题可以分为5步，无论细节如果变化，都一定会遵循这5个步骤。</p><ol><li>陈述研究假设，包含原假设（null hypothesis）和备择假设（alternate hypothesis）</li><li>为验证假设收集数据</li><li>构造合适的统计测试量并测试</li><li>决定是接受还是拒绝原假设</li><li>展示结论</li></ol><p>步骤1：</p><p>通常来说，我们会把原假设的描述写成变量之间不存在某种差异，或不存在某种关联，备择假设则为存在某种差异或关联。</p><p>例如，原假设：男人和女人的平均身高没有差别， 备择假设男人和女人的平均身高存在显著差别。</p><p>步骤2:</p><p>为了统计检验的结果真实可靠，需要根据实际的假设命题从总体中抽取样本，要求抽样的数据要具有代表性，例如在上述男女平均身高的命题中，抽取的样本要能覆盖到各类社会阶级，各个国家等所有可能影响到身高的因素。</p><p>步骤3：</p><p>统计检验量有很多种类，但是所有的统计检验都是基于组内方差和组间方差的比较，如果组间方差足够大，使得不同组之间几乎没有重叠，那么统计量会反映出一个非常小的P值，意味着不同组之间的差异不可能是由偶然性导致的。</p><p>步骤4：</p><p>基于统计量的结果做出接受或拒绝原假设的判断，通常我们会以P=0.05作为临界值（单侧检验）。</p><p>步骤5：</p><p>展示结论。</p><h3 id="3-3-统计量的选择"><a href="#3-3-统计量的选择" class="headerlink" title="3.3 统计量的选择"></a>3.3 统计量的选择</h3><p>选择合适的统计量是进行假设检验的关键步骤，最常用的统计检验包括回归检验(regression test)，比较检验(comparison test)和关联检验(correlation test)三类。</p><p><strong>回归检验</strong></p><p>回归检验适用于预测变量是数值型的情况，根据预测变量的数量和结果变量的类型又分为以下几种。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">预测变量</th><th style="text-align:center">结果变量</th></tr></thead><tbody><tr><td style="text-align:center">简单线性回归</td><td style="text-align:center">单个，连续数值</td><td style="text-align:center">连续数值</td></tr><tr><td style="text-align:center">多重线性回归</td><td style="text-align:center">多个，连续数值</td><td style="text-align:center">连续数值</td></tr><tr><td style="text-align:center">Logistic回归</td><td style="text-align:center">连续数值</td><td style="text-align:center">二元类别</td></tr></tbody></table></div><p><strong>比较检验</strong></p><p>比较检验适用于预测变量是类别型，结果变量是数值型的情况，根据预测变量的分组数量和结果变量的数量又可以分为以下几种。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">预测变量</th><th style="text-align:center">结果变量</th></tr></thead><tbody><tr><td style="text-align:center">Paired t-test</td><td style="text-align:center">两组，类别</td><td style="text-align:center">组来自同一总体，数值</td></tr><tr><td style="text-align:center">Independent t-test</td><td style="text-align:center">两组，类别</td><td style="text-align:center">组来自不同总体，数值</td></tr><tr><td style="text-align:center">ANOVA</td><td style="text-align:center">两组及以上，类别</td><td style="text-align:center">单个，数值</td></tr><tr><td style="text-align:center">MANOVA</td><td style="text-align:center">两组及以上，类别</td><td style="text-align:center">两个及以上，数值</td></tr></tbody></table></div><p><strong>关联检验</strong></p><p>关联检验常用的只有卡方检验一种，适用于预测变量和结果变量均为类别型的情况。</p><p><strong>非参数检验</strong></p><p>此外，由于一般来说上述参数检验都需满足一些前提条件，样本之间独立，不同组的组内方差近似和数据满足正态性，所以当这些条件不满足的时候，我们可以尝试用非参数检验来代替参数检验。</p><div class="table-container"><table><thead><tr><th style="text-align:center">非参数检验</th><th style="text-align:center">用于替代的参数检验</th></tr></thead><tbody><tr><td style="text-align:center">Spearman</td><td style="text-align:center">回归和关联检验</td></tr><tr><td style="text-align:center">Sign test</td><td style="text-align:center">T-test</td></tr><tr><td style="text-align:center">Kruskal–Wallis</td><td style="text-align:center">ANOVA</td></tr><tr><td style="text-align:center">ANOSIM</td><td style="text-align:center">MANOVA</td></tr><tr><td style="text-align:center">Wilcoxon Rank-Sum test</td><td style="text-align:center">Independent t-test</td></tr><tr><td style="text-align:center">Wilcoxon Signed-rank test</td><td style="text-align:center">Paired t-test</td></tr><tr><td style="text-align:center"></td></tr></tbody></table></div><h3 id="3-4-两类错误"><a href="#3-4-两类错误" class="headerlink" title="3.4 两类错误"></a>3.4 两类错误</h3><p>事实上当我们进行假设检验的过程中是存在犯错误的可能的，并且理论上来说错误是无法完全避免的。根据定义，错误分为两类，一类错误（type I error）和二类错误（type II error）。</p><ul><li><p>一类错误：拒绝真的原假设</p></li><li><p>二类错误：接受错误的原假设</p></li></ul><p>一类错误可以通过α值来控制，在假设检验中选择的 α（显著性水平）对一类错误有着直接影响。α可以认为是我们犯一类错误的最大可能性。以95%的置信水平为例，a=0.05，这意味着我们拒绝一个真的原假设的可能性是5%。从长期来看，每做20次假设检验会有一次犯一类错误的事件发生。</p><p>二类错误通常是由小样本或高样本方差导致的，二类错误的概率可以用β来表示，和一类错误不同的是，此类错误是不能通过设置一个错误率来直接控制的。对于二类错误，可以从功效的角度来估计，首先进行功效分析（power analysis）计算出功效值1-β，进而得到二类错误的估计值β。</p><p>一般来说这两类错误是无法同时降低的，在降低犯一类错误的前提下会增加犯二类错误的可能性，在实际案例中如何平衡这两类错误取决于我们更能接受一类错误还是二类错误。</p><h3 id="3-5-Python代码实战"><a href="#3-5-Python代码实战" class="headerlink" title="3.5 Python代码实战"></a>3.5 Python代码实战</h3><p>本节通过一些例子来讲解如何使用python进行假设检验。</p><h4 id="3-5-1-正态检验"><a href="#3-5-1-正态检验" class="headerlink" title="3.5.1 正态检验"></a>3.5.1 正态检验</h4><p>Shapiro-Wilk Test是一种经典的正态检验方法。</p><p>H0: 样本总体服从正态分布</p><p>H1: 样本总体不服从正态分布 </p><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> shapirodata_nonnormal = np.random.exponential(size=<span class="hljs-number">100</span>)data_normal = np.random.normal(size=<span class="hljs-number">100</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normal_judge</span><span class="hljs-params">(data)</span>:</span>stat, p = shapiro(data)<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:<span class="hljs-keyword">return</span> <span class="hljs-string">'stat=&#123;:.3f&#125;, p = &#123;:.3f&#125;, probably gaussian'</span>.format(stat,p)<span class="hljs-keyword">else</span>:<span class="hljs-keyword">return</span> <span class="hljs-string">'stat=&#123;:.3f&#125;, p = &#123;:.3f&#125;, probably not gaussian'</span>.format(stat,p)<span class="hljs-comment"># output</span>normal_judge(data_nonnormal)<span class="hljs-comment"># 'stat=0.850, p = 0.000, probably not gaussian'</span>normal_judge(data_normal)<span class="hljs-comment"># 'stat=0.987, p = 0.415, probably gaussian'</span></code></pre><h4 id="3-5-2-卡方检验"><a href="#3-5-2-卡方检验" class="headerlink" title="3.5.2 卡方检验"></a>3.5.2 卡方检验</h4><p>目的：检验两组类别变量是相关的还是独立的</p><p>H0: 两个样本是独立的</p><p>H1: 两组样本不是独立的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> chi2_contingencytable = [[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],[<span class="hljs-number">6</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">17</span>]]stat, p, dof, expected = chi2_contingency(table)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably independent'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably dependent'</span>) <span class="hljs-comment"># output</span><span class="hljs-comment">#stat=0.272, p=0.873</span><span class="hljs-comment">#Probably independent</span></code></pre><h4 id="3-5-3-T-test"><a href="#3-5-3-T-test" class="headerlink" title="3.5.3 T-test"></a>3.5.3 T-test</h4><p>目的：检验两个独立样本集的均值是否具有显著差异</p><p>H0: 均值是相等的</p><p>H1: 均值是不等的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> ttest_ind<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata1 = np.random.normal(size=<span class="hljs-number">10</span>)data2 = np.random.normal(size=<span class="hljs-number">10</span>)stat, p = ttest_ind(data1, data2)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>)    <span class="hljs-comment"># output</span><span class="hljs-comment"># stat=-1.382, p=0.184</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h4 id="3-5-4-ANOVA"><a href="#3-5-4-ANOVA" class="headerlink" title="3.5.4 ANOVA"></a>3.5.4 ANOVA</h4><p>目的：与t-test类似，ANOVA可以检验两组及以上独立样本集的均值是否具有显著差异</p><p>H0: 均值是相等的</p><p>H1: 均值是不等的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> f_oneway<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata1 = np.random.normal(size=<span class="hljs-number">10</span>)data2 = np.random.normal(size=<span class="hljs-number">10</span>)data3 = np.random.normal(size=<span class="hljs-number">10</span>)stat, p = f_oneway(data1, data2, data3)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>) <span class="hljs-comment"># output</span><span class="hljs-comment"># stat=0.189, p=0.829</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h4 id="3-5-5-Mann-Whitney-U-Test"><a href="#3-5-5-Mann-Whitney-U-Test" class="headerlink" title="3.5.5 Mann-Whitney U Test"></a>3.5.5 Mann-Whitney U Test</h4><p>目的：检验两个样本集的分布是否相同</p><p>H0: 两个样本集的分布相同</p><p>H1: 两个样本集的分布不同</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> mannwhitneyudata1 = [<span class="hljs-number">0.873</span>, <span class="hljs-number">2.817</span>, <span class="hljs-number">0.121</span>, <span class="hljs-number">-0.945</span>, <span class="hljs-number">-0.055</span>, <span class="hljs-number">-1.436</span>, <span class="hljs-number">0.360</span>, <span class="hljs-number">-1.478</span>, <span class="hljs-number">-1.637</span>, <span class="hljs-number">-1.869</span>]data2 = [<span class="hljs-number">1.142</span>, <span class="hljs-number">-0.432</span>, <span class="hljs-number">-0.938</span>, <span class="hljs-number">-0.729</span>, <span class="hljs-number">-0.846</span>, <span class="hljs-number">-0.157</span>, <span class="hljs-number">0.500</span>, <span class="hljs-number">1.183</span>, <span class="hljs-number">-1.075</span>, <span class="hljs-number">-0.169</span>]stat, p = mannwhitneyu(data1, data2)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>)<span class="hljs-comment"># output</span><span class="hljs-comment"># stat=40.000, p=0.236</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Ross S . 概率论基础教程[M]. 人民邮电出版社, 2007.</li><li>盛骤, 谢式千, 潘承毅, 等. 概率论与数理统计 (第四版)[J]. 2008.</li><li><a href="https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/" target="_blank" rel="noopener">https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/</a></li><li><a href="https://www.scipy.org/" target="_blank" rel="noopener">https://www.scipy.org/</a></li><li><a href="https://www.thoughtco.com/difference-between-type-i-and-type-ii-errors-3126414" target="_blank" rel="noopener">https://www.thoughtco.com/difference-between-type-i-and-type-ii-errors-3126414</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-一般随机变量&quot;&gt;&lt;a href=&quot;#1-一般随机变量&quot; class=&quot;headerlink&quot; title=&quot;1 一般随机变量&quot;&gt;&lt;/a&gt;1 一般随机变量&lt;/h2&gt;&lt;h3 id=&quot;1-1-随机变量的两种类型&quot;&gt;&lt;a href=&quot;#1-1-随机变量的两种类型&quot; class=&quot;headerlink&quot; title=&quot;1.1 随机变量的两种类型&quot;&gt;&lt;/a&gt;1.1 随机变量的两种类型&lt;/h3&gt;&lt;p&gt;根据随机变量可能取值的个数分为离散型（取值有限）和连续型（取值无限）两类。&lt;/p&gt;
&lt;h3 id=&quot;1-2-离散型随机变量&quot;&gt;&lt;a href=&quot;#1-2-离散型随机变量&quot; class=&quot;headerlink&quot; title=&quot;1.2 离散型随机变量&quot;&gt;&lt;/a&gt;1.2 离散型随机变量&lt;/h3&gt;&lt;p&gt;对于离散型随机变量，使用概率质量函数（probability mass function），简称PMF，来描述其分布律。&lt;/p&gt;
&lt;p&gt;假定离散型随机变量X，共有n个取值，$X_1$, $X_2$, ……, $X_n$, 那么&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(X=X_n) \geq 0&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\Sigma_{1}^{n} P(X=X_n) =1&lt;/script&gt;&lt;p&gt;用到PMF的例子：二项分布，泊松分布&lt;/p&gt;
    
    </summary>
    
    
      <category term="概率统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="概率统计" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>第八章-分类数据</title>
    <link href="http://yoursite.com/article/28c12c7e.html"/>
    <id>http://yoursite.com/article/28c12c7e.html</id>
    <published>2020-06-27T10:22:15.000Z</published>
    <updated>2020-06-27T15:39:40.378Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/article/28c12c7e/分类数据.png" srcset="/img/loading.gif" alt></p><a id="more"></a><h1 id="问题与练习"><a href="#问题与练习" class="headerlink" title="问题与练习"></a>问题与练习</h1><p>【问题一】如何使用union_categoricals方法，它的作用是什么？</p><ul><li>如果要组合不一定具有相同类别的类别，union_categoricals函数将组合类似列表的类别。新类别将是合并的类别的并集。如下所示：</li></ul><pre><code class="hljs python"><span class="hljs-keyword">from</span> pandas.api.types <span class="hljs-keyword">import</span> union_categoricalsa = pd.Categorical([<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>])b = pd.Categorical([<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>])union_categoricals([a,b])</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-1.png" srcset="/img/loading.gif" alt="image-20200627224024855"></p><ul><li>默认情况下，生成的类别将按照在数据中显示的顺序排列。如果要对类别进行排序，可使用sort_categories=True参数。</li><li>union_categoricals也适用于组合相同类别和顺序信息的两个分类。</li><li>union_categoricals可以在合并分类时重新编码类别的整数代码。</li></ul><p>【问题二】利用concat方法将两个序列纵向拼接，它的结果一定是分类变量吗？什么情况下不是？</p><ul><li>pd.concat对象只能是pd.Series或pd.DataFrame，所以结果是object。</li><li>使用union_categoricals拼接两个分类变量可得到分类变量。</li></ul><p>【问题三】当使用groupby方法或者value_counts方法时，分类变量的统计结果和普通变量有什么区别？</p><ul><li>分类变量的groupby方法/value_counts方法，统计对象是类别。</li><li>普通变量groupby方法/value_counts方法，统计对象是唯一值(不包含NA)。</li></ul><p>【问题四】下面的代码说明了Series创建分类变量的什么”缺陷”？如何避免？（提示使用Series的copy参数）</p><pre><code class="hljs python">cat = pd.Categorical([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>], categories=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">10</span>])s = pd.Series(cat, name=<span class="hljs-string">"cat"</span>)cat</code></pre><p>输出：</p><pre><code class="hljs tex">[1, 2, 3, 10]Categories (5, int64): [1, 2, 3, 4, 10]</code></pre><hr><pre><code class="hljs python">s.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>] = <span class="hljs-number">10</span>cat</code></pre><p>输出：</p><pre><code class="hljs tex">[10, 10, 3, 10]Categories (5, int64): [1, 2, 3, 4, 10]</code></pre><p>分类变量在Series改动时也被改动了。<br><strong>使用copy参数避免分类变量被修改。</strong></p><pre><code class="hljs python">cat = pd.Categorical([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">10</span>],categories=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">10</span>])s = pd.Series(cat, name=<span class="hljs-string">'cat'</span>, copy=<span class="hljs-literal">True</span>)print(cat)s.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>] = <span class="hljs-number">10</span>print(cat)</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-2.png" srcset="/img/loading.gif" alt="image-20200627230102057"></p><p>【练习一】现继续使用第四章中的地震数据集，请解决以下问题：<br>（a）现在将深度分为七个等级：[0.5,10,15,20,30,50,np.inf]，请以深度Ⅰ,Ⅱ,Ⅲ,Ⅳ,Ⅴ,Ⅵ,Ⅶ为索引并按照由浅到深的顺序进行排序。</p><p>使用cut方法对列表中的深度划分，并将该列作为索引值。然后按索引排序即可。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/Earthquake.csv'</span>)df_result = df.copy()df_result[<span class="hljs-string">'深度'</span>] = pd.cut(df[<span class="hljs-string">'深度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">15</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">50</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>,<span class="hljs-string">'Ⅴ'</span>,<span class="hljs-string">'Ⅵ'</span>,<span class="hljs-string">'Ⅶ'</span>])df_result = df_result.set_index(<span class="hljs-string">'深度'</span>).sort_index()df_result.head()</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-3.png" srcset="/img/loading.gif" alt="image-20200627231333013"></p><p>（b）在(a)的基础上，将烈度分为4个等级：[0,3,4,5,np.inf]，依次对南部地区的深度和烈度等级建立多级索引排序。</p><p>跟(a)很相似，cut方法对深度，烈度进行切分，把index设为[‘深度’，‘烈度’]，然后进行索引排序即可。</p><pre><code class="hljs python">df[<span class="hljs-string">'烈度'</span>] = pd.cut(df[<span class="hljs-string">'烈度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>])df[<span class="hljs-string">'深度'</span>] = pd.cut(df[<span class="hljs-string">'深度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">15</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">50</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>,<span class="hljs-string">'Ⅴ'</span>,<span class="hljs-string">'Ⅵ'</span>,<span class="hljs-string">'Ⅶ'</span>])df_ds = df.set_index([<span class="hljs-string">'深度'</span>,<span class="hljs-string">'烈度'</span>])df_ds.sort_index()</code></pre><p>【练习二】 对于分类变量而言，调用第4章中的变形函数会出现一个BUG（目前的版本下还未修复）：例如对于crosstab函数，按照<a href="https://pandas.pydata.org/pandas-docs/version/1.0.0/user_guide/reshaping.html#cross-tabulations" target="_blank" rel="noopener">官方文档的说法</a>，即使没有出现的变量也会在变形后的汇总结果中出现，但事实上并不是这样，比如下面的例子就缺少了原本应该出现的行’c’和列’f’。基于这一问题，请尝试设计my_crosstab函数，在功能上能够返回正确的结果。<br>因为Categories中肯定包含出现的变量。所以将第一个参数作为index，第二个参数作为columns，建立一个DataFrame，然后把出现的变量组合起来，对应位置填入1即可。</p><pre><code class="hljs python">foo = pd.Categorical([<span class="hljs-string">'b'</span>,<span class="hljs-string">'a'</span>], categories=[<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>])bar = pd.Categorical([<span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>], categories=[<span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'f'</span>])<span class="hljs-keyword">import</span> numpy<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_crosstab</span><span class="hljs-params">(a, b)</span>:</span>    s1 = pd.Series(list(foo.categories), name=<span class="hljs-string">'row'</span>)    s2 = list(bar.categories)    df = pd.DataFrame(np.zeros((len(s1), len(s2)),int),index=s1, columns=s2)    index_1 = list(foo)    index_2 = list(bar)    <span class="hljs-keyword">for</span> loc <span class="hljs-keyword">in</span> zip(index_1, index_2):        df.loc[loc] = <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> dfmy_crosstab(foo, bar)</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-4.png" srcset="/img/loading.gif" alt="image-20200627232221247"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/article/28c12c7e/分类数据.png&quot; srcset=&quot;/img/loading.gif&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="pandas下" scheme="http://yoursite.com/categories/pandas%E4%B8%8B/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>第7章 文本数据</title>
    <link href="http://yoursite.com/article/88685a55.html"/>
    <id>http://yoursite.com/article/88685a55.html</id>
    <published>2020-06-26T14:20:12.000Z</published>
    <updated>2020-07-01T10:39:48.084Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/article/88685a55/文本数据.png" srcset="/img/loading.gif" alt></p><h1 id="六、问题与练习"><a href="#六、问题与练习" class="headerlink" title="六、问题与练习"></a>六、问题与练习</h1><h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1.问题"></a>1.问题</h2><p>【问题一】 str对象方法和df/Series对象方法有什么区别?</p><ul><li><p>str对象方法有很多针对字符串的方法</p></li><li><p>在replace方法上，二者有较大差异：</p><p>1）str.replace针对object类型或string类型，默认操作：正则表达式</p><p>​      replace针对任意类型的序列或数据框，正则表达式替换：regex=True。使用字典可以支持      多列替换。</p><p>2）str.replace类型赋值参数不能是pd.NA。</p><p>3）对于string类型Series，使用replace函数不能使用正则表达式替换。</p></li></ul><p>【问题二】 给出一列string类型，如何判断单元格是否是数值型数据？</p><ul><li>正则表达式进行匹配。</li><li>str.isnumeric有一定的局限性。</li></ul><p>【问题三】str.split方法的作用是什么？在什么场合下使用？</p><ul><li>使用分隔符对每个字符串进行分割并返回列表。</li><li>可以使用str方法进行元素选择。</li></ul><p>【问题四】在本章的第二到第四节分别介绍了字符串类型的5类操作，请思考这些操作的应用场景？</p><ul><li>拆分：</li><li>拼接</li><li>替换</li><li>字符匹配</li><li>字符提取</li></ul><h2 id="2-练习"><a href="#2-练习" class="headerlink" title="2.练习"></a>2.练习</h2><p>【练习一】现有一份关于字符串的数据集，请解决以下问题：</p><p>（a）先对字符串编码存储人员信息（在编号后添加ID列），使用如下格式：”xxx(名字)：x国人，性别x，生于x年x月x日“</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/String_data_one.csv'</span>,index_col=<span class="hljs-string">'人员编号'</span>, dtype=<span class="hljs-string">'string'</span>)df[<span class="hljs-string">'ID'</span>] = df[<span class="hljs-string">'姓名'</span>].str.cat([<span class="hljs-string">': '</span>+df[<span class="hljs-string">'国籍'</span>]+<span class="hljs-string">'国人,'</span>,<span class="hljs-string">'性别'</span>+df[<span class="hljs-string">'性别'</span>]+<span class="hljs-string">','</span>,<span class="hljs-string">'生于'</span>+df[<span class="hljs-string">'出生年'</span>]+<span class="hljs-string">'年'</span>,df[<span class="hljs-string">'出生月'</span>]+<span class="hljs-string">'月'</span>, df[<span class="hljs-string">'出生日'</span>] +<span class="hljs-string">'日'</span>],na_rep=<span class="hljs-string">'*'</span>)df[<span class="hljs-string">'ID'</span>].head()</code></pre><p>输出：</p><p><img src="/article/88685a55/image-1.png" srcset="/img/loading.gif" alt="image-20200627003713531"></p><p>（b）将（a）中的人员生日信息部分修改为用中文表示（如一九七四年十月二十三日），其余返回格式不变。</p><p>（c）将（b）中的ID列结果拆分为原列表相应的5列，并使用equals检验是否一致。</p><p>【练习二】现有一份半虚拟的数据集，第一列包含了新型冠状病毒的一些新闻标题，请解决以下问题：</p><p>（a）选出所有关于北京市和上海市新闻标题的所在行。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/String_data_two.csv'</span>)df[<span class="hljs-string">'col1'</span>].str.extract(<span class="hljs-string">r'(?P&lt;name_1&gt;北京|上海)'</span>).dropna().index</code></pre><p>输出：</p><p><img src="/article/88685a55/image-2.png" srcset="/img/loading.gif" alt="image-20200626233841173"></p><p>（b）求col2的均值。</p><p>（c）求col3的均值。</p>]]></content>
    
    <summary type="html">
    
      pandas中Series的str方法：对文本数据的处理
    
    </summary>
    
    
      <category term="pandas下" scheme="http://yoursite.com/categories/pandas%E4%B8%8B/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Task02-LBP特征描述算子-人脸检测</title>
    <link href="http://yoursite.com/article/50afbda.html"/>
    <id>http://yoursite.com/article/50afbda.html</id>
    <published>2020-06-25T11:44:27.000Z</published>
    <updated>2020-06-30T07:52:56.267Z</updated>
    
    <content type="html"><![CDATA[<p>学习任务：</p><p><strong>本文主要包含以下两部分</strong>：</p><ul><li><strong>理论部分</strong>：掌握LBP特征描述算子原理</li><li><strong>练习部分</strong>：使用OpenCV的LBP检测器完成人脸检测任务</li></ul><p>LBP是指局部二值模式(Local Binary Pattern)，是一种用来描述图像局部特征的算子，具有灰<br>度不变性和旋转不变性等显著优点。<a id="more"></a></p><h1 id="2-1-算法理论"><a href="#2-1-算法理论" class="headerlink" title="2.1 算法理论"></a>2.1 算法理论</h1><h2 id="LBP概念"><a href="#LBP概念" class="headerlink" title="LBP概念"></a>LBP概念</h2><p>LBP指的是局部二值模式，英文全称：Local Binary Pattern，是一种用来描述图像局部特征的算子。<br>LBP特征比较出名的应用是人脸识别和目标检测中。<br>计算机视觉开源库OpenCV：</p><ul><li>使用LBP特征实现人脸识别的接口。</li><li>使用LBP特征实现目标检测分类器的接口。</li></ul><p>本文将会对上述第一点做简单实现。</p><h2 id="基本LBP-纹理特征"><a href="#基本LBP-纹理特征" class="headerlink" title="基本LBP:纹理特征"></a>基本LBP:纹理特征</h2><p>基本LBP特征描述</p><ol><li>基本的LBP算子定义在像素3*3的领域内；</li><li>以领域中文像素为阈值，相邻8个像素的灰度值与中心像素的灰度值比较，若周围像素大于中心像素，标记为1，否则标记为0；</li><li>3*3领域内的8个点经过第二步后产生8个二进制数，依次排列为一个二进制序列。</li><li>8位二进制序列共有$2^8$即256种LBP值。中心像素的LBP值反映了该像素周围区域的纹理信息。中心像素的灰度值决定了局部区域的整体亮度。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200605193831589.png#pic_center" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.1 LBP计算示意图</div></center><p>​        中心像素坐标$(x_0,y_0)$局部领域中像素个数为$P(P&gt;1)$，$g_c$表示中心像素的灰度值，$g_p$ 表示领域像素的灰度值。</p><p>​        基本LBP表示为：$LBP(x_0,y_0)=\sum_{p=0}^{P-1} s(g_{p}-g_{c}) 2^{p}$<br>​        s是示性函数：$s(x)=\left\{\begin{array}{l}1, x \geq 0 \\<br>0, x&lt;0<br>\end{array}\right.$</p><p>&emsp;&emsp; <font color="blue">基本LBP具有灰度不变性。</font>对光照变化是鲁棒的。</p><h2 id="圆形LBP算子"><a href="#圆形LBP算子" class="headerlink" title="圆形LBP算子"></a>圆形LBP算子</h2><p>&emsp;基本LBP算子的缺陷：只覆盖了固定半径范围的小区域，不能适应不同尺度和频率纹理的变化。<br>&emsp;圆形LBP(Circular LBP or Extended LBP)：<strong>将局部领域扩展到任意领域，用圆形领域代替正方形领域。</strong><br>&emsp;改进后的LBP算子允许半径为R的圆形领域有任意多个像素点。<br>    使用可变半径的圆对近邻像素进行编码，可得到如下近邻：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/20190527221543821.png" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 五种编码对应的LBP</div></center><p>​    半径为R的圆形区域内含有P个采样点的LBP算子，表示为$LBP^{R}_P$；</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200605202407325.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 圆形LBP示意图</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/20190527221655965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 LBP扩展与多尺度表达</div></center><p>​        对于给定中心点$(x_c,y_c)$,其邻域像素位置为$(x_p,y_p)$，$p∈P$，其采样点$(x_p,y_p)$用如下公式计算：</p><script type="math/tex; mode=display">{x}_{p}=x_{c}+\operatorname{Rcos}\left(\frac{2 \pi p}{P}\right) \\y_{p}=y_{c}+\operatorname{Rsin}\left(\frac{2 \pi p}{P}\right)</script><p>​    &emsp;R是采样半径，p是第p个采样点，P是采样数目。如果近邻点不在整数位置上，就需要进行插值运算，然后使用计算出的插值点。OpenCV使用的是双线性插值。</p><h2 id="旋转不变性LBP"><a href="#旋转不变性LBP" class="headerlink" title="旋转不变性LBP"></a>旋转不变性LBP</h2><p>实现：<strong>不断旋转圆形邻域得到一系列初始定义的LPB值，取最小值作为中心像素点的LBP特征。</strong></p><script type="math/tex; mode=display">L B P_{P R}^{ri}=\min \left(R O R\left(L B P_{P, R}, i\right) | i=0,1, \ldots, P-1\right)\tag{2-7}</script><p>&emsp;其中$L B P_{P R}^{ri}$表示具有旋转不变性的LBP特征。$ROR(x, i)$为旋转函数，表示将P-bit​数右循环i位。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200606135438907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 求取旋转不变的LBP特征示意图</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/v55RgEh.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 36种不同的旋转不变性LBP特征(P=8)</div></center><h2 id="Uniform-Pattern-LBP特征"><a href="#Uniform-Pattern-LBP特征" class="headerlink" title="Uniform Pattern LBP特征"></a>Uniform Pattern LBP特征</h2><p>Uniform Pattern，也被称为等价模式或均匀模式。<br>        将LBP算子用于纹理分类或人脸识别时，常用于LBP模式的统计直方图来表达图像的信息，较多的模式种类将使得数据量过大，且直方图过于稀疏。因此，需要对原始的LBP进行降维，使得数据量减少的情况下更好地表达图像信息。<br>        为了解决二进制模式过多的问题，提高统计性，Ojala提出了采用一种“<strong>等价模式</strong>”(Uniform Pattern)来对LBP算子的模式种类<strong>进行降维</strong>。<br>        <strong>在实际图像中，绝大多数LBP模式最多只包含两次从1到0或从0到1的跳变。</strong></p><ul><li>将<strong>等价模式</strong>定义为：当某个LBP所对应的循环二进制数从0到1或从1到0最多有两次跳变时，该LBP所对应的二进制就称为一个等价模式类。<strong>如00000000(0次跳变)，00000111(只含一次从0到1的跳变)，10001111(先由1跳到0，再由0跳到1，共两次跳变)</strong>,都是等价模式类。</li><li>除等价模式类以外的模式都归为另一类，称为<strong>混合模式类</strong>。<strong>例如10010111(共四次跳变)</strong>。</li></ul><p>&emsp;&emsp;<strong>检查某种模式是否是等价模式</strong>：将当前位置和移动一位后的二进制模式按位相减。并绝对值求和。若U$\left(G_{p}\right)$ 小于等于2，则为等价模式。</p><script type="math/tex; mode=display">U\left(G_{p}\right)=\left|s\left(g_{p_{-1}}-g_{c}\right)-s\left(g_{0}-g_{c}\right)\right|+\sum_{p=1}^{P_{-1}}\left|s\left(g_{p}-g_{c}\right)-s\left(g_{P-1}-g_{c}\right)\right|</script><p>&emsp;&emsp;二进制模式数量由原来的$2^P$种减少为了$P(P-1)+2+1$种。其中等价模式类为$P(P-1)+2$种，在LBP特征图的灰度值为$1-[P(P-1)+2]$，混合模式类为1种，灰度值为0。因此等价模式LBP特征图像整体偏暗。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/image-20200628174535833.png" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 LBP等价模式类(P=8)</div></center><h2 id="LBPH特征"><a href="#LBPH特征" class="headerlink" title="LBPH特征"></a>LBPH特征</h2><p>LBPH特征：全称为Local Binary Patterns Histograms，即LBP特征的统计直方图。LBPH将LBP特征与图像的空间信息结合在一起。</p><p>一幅图具体的计算LBPH过程如下：</p><p>1.计算图像的LBP特征图像；</p><p>2.将LBP特征图像进行分块，OpenCV中将LBP特征图像分成8行8列64块区域；</p><p>3.计算每块区域特征图像的直方图cell_LBPH，并将直方图进行归一化，直方图大小为 1*numPatterns；</p><p>4.将上面计算的每块区域特征图像的直方图按分块的空间顺序依次排列成一列，形成LBP特征分量，其大小为 1<em>(numPatterns</em>64)；</p><p>5.用机器学习的方式对LBP特征向量进行训练，用来检测和识别目标。</p><h1 id="2-2-基于OpenCV的LBP人脸检测"><a href="#2-2-基于OpenCV的LBP人脸检测" class="headerlink" title="2.2 基于OpenCV的LBP人脸检测"></a>2.2 基于OpenCV的LBP人脸检测</h1><h2 id="多级级联对人脸图像进行检测："><a href="#多级级联对人脸图像进行检测：" class="headerlink" title="多级级联对人脸图像进行检测："></a>多级级联对人脸图像进行检测：</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200606145525679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.5 人脸检测流程图</div></center><h2 id="基于OpenCV的实现"><a href="#基于OpenCV的实现" class="headerlink" title="基于OpenCV的实现"></a>基于OpenCV的实现</h2><ul><li>使用OpenCV预训练模型</li><li>将haarcascade_frontalface_default.xml下载至本地以方便调用，下载链接：<a href="https://github.com/opencv/opencv/blob/master/data/lbpcascades/lbpcascade_frontalface_improved.xml" target="_blank" rel="noopener">https://github.com/opencv/opencv/blob/master/data/lbpcascades/lbpcascade_frontalface_improved.xml</a>   </li></ul><p>函数：<br>detectMultiScale(self, image, scaleFactor=None, minNeighbors=None, flags=None, minSize=None, maxSize=None)<br>参数:<br>1）scaleFactor: 图像缩放比例<br>2）minNeighbors: 每个候选矩形保留的邻居个数，值越大-&gt;精度越大<br>3）minSize：检测到的最小矩形大小<br>4）maxSize: 检测到的最大矩形大小</p><p>待测人脸图像(网上随意摘取一张)：</p><p><img src="/article/50afbda/image2.jpg" srcset="/img/loading.gif" alt="image2"></p><pre><code class="hljs python"><span class="hljs-comment"># 利用LBP进行人脸检测</span><span class="hljs-keyword">import</span> cv2img = cv2.imread(<span class="hljs-string">'image2.jpg'</span>)<span class="hljs-comment"># cv2.imshow('test', img)</span>face_detect = cv2.CascadeClassifier(<span class="hljs-string">'lbpcascade_frontalface_improved.xml'</span>)<span class="hljs-comment"># ======================检测人脸==============================</span><span class="hljs-comment"># 灰度处理</span>gray = cv2.cvtColor(img, code=cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 检测人脸 按照1.1倍放到 周围最小像素为5</span>face_zone = face_detect.detectMultiScale(gray, scaleFactor=<span class="hljs-number">1.1</span>, minNeighbors=<span class="hljs-number">2</span>)print(face_zone)<span class="hljs-comment"># 绘制矩形检测人脸</span><span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> face_zone:    <span class="hljs-comment"># 绘制矩形人脸区域</span>    print(x,y,w,h)    cv2.rectangle(img, pt1=(x, y), pt2=(x+w, y+h), color=[<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], thickness=<span class="hljs-number">2</span>)<span class="hljs-comment"># 显示图片</span>cv2.imshow(<span class="hljs-string">"output"</span>, img)<span class="hljs-comment"># 等待显示，设置任意键退出程序</span>cv2.waitKey(<span class="hljs-number">0</span>)cv2.destroyAllWindows()</code></pre><p>检测结果：</p><p><img src="/article/50afbda/output.jpg" srcset="/img/loading.gif" alt="output"></p><p>简单示意例子：<a href="https://sharky93.github.io/docs/gallery/auto_examples/plot_local_binary_pattern.html" target="_blank" rel="noopener">https://sharky93.github.io/docs/gallery/auto_examples/plot_local_binary_pattern.html</a></p><h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p><a href="https://blog.csdn.net/qq_34246778/article/details/90613779?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">LBP(Local Binary Patterns特征)</a></p><p><a href="https://blog.csdn.net/YhL_Leo/article/details/52120195" target="_blank" rel="noopener">局部二值模式（Local Binary Patterns）纹理灰度与旋转不变性</a></p><p><a href="https://github.com/datawhalechina/team-learning/blob/master/03 计算机视觉/计算机视觉基础：图像处理（下）/Task02 LBP特征描述算子.md" target="_blank" rel="noopener">Task02 LBP特征描述算子</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习任务：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本文主要包含以下两部分&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;理论部分&lt;/strong&gt;：掌握LBP特征描述算子原理&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;练习部分&lt;/strong&gt;：使用OpenCV的LBP检测器完成人脸检测任务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LBP是指局部二值模式(Local Binary Pattern)，是一种用来描述图像局部特征的算子，具有灰&lt;br&gt;度不变性和旋转不变性等显著优点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="图像处理下" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8B/"/>
    
    
      <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Task01 Harris特征点检测器-兴趣点检测</title>
    <link href="http://yoursite.com/article/8b3bd9dd.html"/>
    <id>http://yoursite.com/article/8b3bd9dd.html</id>
    <published>2020-06-24T15:53:33.000Z</published>
    <updated>2020-06-25T11:45:22.045Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Harris特征点检测器-兴趣点检测"><a href="#Harris特征点检测器-兴趣点检测" class="headerlink" title="Harris特征点检测器-兴趣点检测"></a>Harris特征点检测器-兴趣点检测</h1><p><strong>特则点</strong>又称为兴趣点或角点，通常具有旋转不变性和光照不变性和视角不变性等优点，是图像的重要特征之一，常被应用到目标匹配、目标跟踪、三维重建等应用中。</p><p><strong>点特征</strong>主要是指图像中的明显点，如突出的角点、边缘端点、权值点等等。</p><p><strong>兴趣点提取(检测)算子</strong>是用于点特征提取的算子<a id="more"></a>，常用的有</p><ul><li>Harris角点检测：用于检测角点</li><li>FAST特征检测：用于检测角点</li><li>SIFT特征检测：用于检测斑点</li><li>SURF特征检测：用于检测角点</li><li>BRIEF特征检测：用于检测斑点</li><li>ORB：该算法代表带方向的FAST算法和具有旋转不变性的BRIEF算法</li></ul><p><strong>特征匹配</strong>：</p><ul><li>暴力(Brute-Force)匹配法</li><li>基于FLANN匹配法</li></ul><h2 id="一、Harris特征点检测算法的思想和数学原理"><a href="#一、Harris特征点检测算法的思想和数学原理" class="headerlink" title="一、Harris特征点检测算法的思想和数学原理"></a>一、Harris特征点检测算法的思想和数学原理</h2><h2 id="1-1-基础知识"><a href="#1-1-基础知识" class="headerlink" title="1.1 基础知识"></a>1.1 基础知识</h2><ul><li><p><strong>角点</strong></p><p>左图表示一个平坦区域，在各个方向移动，窗口内像素值均没有太大变化；</p><p>中图表示一个边缘特征(Edges)，若沿着水平方向移动(梯度方向)，像素值会发生跳变；若沿着边缘移动(平行于边缘)，像素值不变发生变化；</p><p>右图表示一个角(Corners)，它朝哪个方向移动，像素值都会发生很大变化。即为<font color="red">角点</font>。</p></li></ul><p><img src="https://camo.githubusercontent.com/460ac04994bec25117ee2724563058da99e5bf1e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303630393230343234393231392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304d4459304e7a67784f513d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><ul><li><strong>图像梯度</strong>:图像局部内，图像梯度越大表示该局部内像素值变化越大。图形梯度在数学上可用微分或者积分表示,使用差分来近似导数：$G_x(x,y)=H(x+1,y)-H(x-1,y)$,</li><li><strong>补充</strong>：对图像求梯度通常是考虑图像的每个像素的某个领域内的灰度变化，因此通常对原始图像中像素某个领域设置梯度算子，然后采用小区域模板进行卷积计算，常用的有Prewitt算子、Sobel算子、Robinson算子、Laplace算子等。</li></ul><h2 id="1-2-Harris角点检测算法原理"><a href="#1-2-Harris角点检测算法原理" class="headerlink" title="1.2 Harris角点检测算法原理"></a>1.2 Harris角点检测算法原理</h2><h3 id="1-2-1-计算窗口内部的像素值变化量-E-x-y"><a href="#1-2-1-计算窗口内部的像素值变化量-E-x-y" class="headerlink" title="1.2.1 计算窗口内部的像素值变化量$E(x,y)$"></a>1.2.1 计算窗口内部的像素值变化量$E(x,y)$</h3><p><strong>建立数学模型，确定哪些窗口会引起较大的灰度值变化</strong></p><p>窗口$W$对应的像素坐标位置$(x,y)$，窗口的大小决定了有多少位置。</p><p>像素位置坐标$(x,y)$对应的像素灰度值为$I(x,y)$，窗口分别向$x$和$y$方向上移动$(u,v)$，到达$(x+u,y+v)$上，对应的像素灰度值$I(x+u,y+v)$。</p><p>窗口移动引起的灰度值的变化量为$I(x+u,y+v)-I(x,y)$。</p><p>$(x,y)$位置的窗口函数为$w(x,y)$。即为窗口内各像素的权重。</p><p><img src="/article/8b3bd9dd/image1-2.png" srcset="/img/loading.gif" alt="image-20200623143300133"></p><p><strong>窗口移动(u,v)引起的灰度值的加权变化量</strong>：</p><script type="math/tex; mode=display">E(u,v) = \sum_{x,y}{w(x,y){[I(x+u,y+v)-I(x,y)]}^{2}}</script><p>根据二维泰勒公式展开：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}E(u,v) &\approx \sum_{x,y \in W}{w(x,y){[I(x,y)+uI_x+vI_y-I(x,y)]}^{2}} \\  &= \sum_{x,y\in W}{w(x,y){(uI_x+vI_y)}^{2}} \\  &= \sum_{x,y\in W}{w(x,y){(u^2I_x^2+vI_y^2+2uvI_xI_y)}^{2}} \\  &= \sum_{x,y\in W}{w(x,y)\begin{bmatrix}u&v\end{bmatrix}\begin{bmatrix}I_x^2 & I_xI_y\\                                       I_xI_y & I_y^2\end{bmatrix} \begin{bmatrix}u\\v\end{bmatrix}} \\  &= \begin{bmatrix}u&v\end{bmatrix}(\sum_{x,y\in W}{w(x,y) \begin{bmatrix}I_x^2 & I_xI_y\\                                     I_xI_y & I_y^2\end{bmatrix} })\begin{bmatrix}u\\v\end{bmatrix}                                      \\  &= \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\v\end{bmatrix}\end{aligned}\end{equation}</script><p>其中$I_x,I_y$分别为窗口内像素点$(x,y)$在$x$方向上和$y$方向上的梯度值。矩阵$M$为：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}M &= \sum_{(x,y)\in W}{w(x,y) \begin{bmatrix}I_x^2 & I_xI_y\\                                       I_xI_y & I_y^2\end{bmatrix}}\\  &=R^{-1}\begin{bmatrix}\lambda_1 & 0\\0 & \lambda_2\end{bmatrix}R\end{aligned}\end{equation}</script><p>最后使用实对称矩阵对角化处理后得到我们想要的结果。</p><h3 id="1-2-2-计算对应的角点响应函数-R"><a href="#1-2-2-计算对应的角点响应函数-R" class="headerlink" title="1.2.2 计算对应的角点响应函数$R$"></a>1.2.2 计算对应的角点响应函数$R$</h3><p>通过矩阵的梯度变化可以得到协方差矩阵M，协方差矩阵M决定了灰度值的加权变化量。因此通过计算<strong>角点响应函数R</strong>得到每个窗口对应的得分:</p><script type="math/tex; mode=display">R = det(M)-k(trace(M))^2</script><p>其中$det(M)=\lambda_1\lambda_2$为矩阵的行列式，$trace(M)=\lambda_1+\lambda_2$为矩阵的迹。</p><p>$k$是一个经验常数，需要经验确定它的合适大小，通常在(0.04,0.06)之间取值。</p><h3 id="1-2-3-角点判定"><a href="#1-2-3-角点判定" class="headerlink" title="1.2.3 角点判定"></a>1.2.3 角点判定</h3><p>根据R值判断窗口是平面、边缘还是角点：</p><ul><li>平面：$|R|$值非常小，$\lambda_1$和$\lambda_2$都较小，窗口区域的像素点的梯度变化小。</li><li>边缘：$|R|$值为负数，$\lambda_1 \gg \lambda_2$或$\lambda_2 \gg \lambda_1$，像素点的某个方向的梯度幅值变化比较明显，另一个方向上的梯度变化比较弱。</li><li>角点：$|R|$值很大，$(I_x,I_y)$对应的$\lambda_1$和$\lambda_2$都很大。像素点的梯度分布比较散，梯度变化程度比较大。</li></ul><p>如下图所示：</p><p><img src="https://camo.githubusercontent.com/c6eef9fa98b4f1e099bc722b21536c73c6b9bf9f/68747470733a2f2f696d67636f6e766572742e6373646e696d672e636e2f6148523063484d364c7939706257466e5a584d794d4445314c6d4e75596d78765a334d75593239744c324a73623263764e4455784e6a59774c7a49774d5459774e4338304e5445324e6a41744d6a41784e6a41304d6a45784d5441314e4455354f5445744e4451304e6a6b314e5445344c6e42755a773f782d6f73732d70726f636573733d696d6167652f666f726d61742c706e67237069635f63656e746572" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>Harris角点检测：设定一个阈值，高于阈值的像素对应角点。</p><ul><li>补充：角点的非极大值原理—在一个窗口内，如果有很多角点则用值最大的那个角点，其他的角点都删除。</li></ul><h2 id="1-3-Shi-Tomasi角点检测器"><a href="#1-3-Shi-Tomasi角点检测器" class="headerlink" title="1.3 Shi-Tomasi角点检测器"></a>1.3 Shi-Tomasi角点检测器</h2><p><strong>Shi-Tomasi 角点检测</strong>：《Good_Features_to_Track》论文提出的Harris改进版。</p><p>Harris角点检测中每个窗口的分数公式：$R=\lambda_1\lambda_2 - k(\lambda_1+\lambda_2)^2$</p><p>缺陷：Harris角点检测算法的稳定性和k值有关，但k是经验值，不好设定最佳值。</p><p>改进：角点稳定性与矩阵$M$的较小特征值有关，Shi-Tomasi 直接采用较小的特征值作为分数。(如此一来就不用调整k值啦)</p><p>Shi-Tomasi角点检测中每个窗口的分数公式：$R=min(\lambda_1,\lambda_2)$</p><p>判定角点的方式不变：分数大于设定的阈值，即为角点。</p><h2 id="二、OpenCV的Harris算子进行兴趣点检测"><a href="#二、OpenCV的Harris算子进行兴趣点检测" class="headerlink" title="二、OpenCV的Harris算子进行兴趣点检测"></a>二、OpenCV的Harris算子进行兴趣点检测</h2><h2 id="2-1-Harris角点检测"><a href="#2-1-Harris角点检测" class="headerlink" title="2.1 Harris角点检测"></a>2.1 Harris角点检测</h2><p><strong>opencv</strong>提供了实现<strong>Harris</strong>角点检测函数：<a href="https://link.zhihu.com/?target=https%3A//docs.opencv.org/master/dd/d1a/group__imgproc__feature.html%23gac1fc3598018010880e370e2f709b4345">cv2.cornerHarris</a>，下面调用该接口进行<strong>Harris</strong>特征点检测。</p><p>函数：<strong>cv2.cornerHarris(src, blockSize, ksize, k[, dst[, borderType]])​</strong></p><p>函数功能：对于每一个像素 $(x,y)$，在 ($blockSize$ x $blockSize$) 邻域内，计算梯度图的协方差矩阵$M(x,y)$，通过计算角点响应函数得到结果图。该结果图的局部最大值即图像中的角点。</p><p>函数参数：</p><ul><li><strong>src</strong>:待检测的灰度图像(float32类型)</li><li><strong>blockSize</strong>:用于角点检测的领域大小，即窗口尺寸</li><li><strong>ksize</strong>:用于计算梯度图的Sobel算子的尺寸</li><li><strong>k</strong>:用于计算角点响应函数的参数k，取值范围在0.04~0.06之间</li></ul><p>待检测的图片：</p><p><img src="/article/8b3bd9dd/image1.jpg" srcset="/img/loading.gif" alt="image1"></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 检测参数</span>block_size = <span class="hljs-number">3</span>sobel_size = <span class="hljs-number">3</span>k = <span class="hljs-number">0.06</span>image = cv.imread(<span class="hljs-string">'image1.jpg'</span>)print(image.shape)height = image.shape[<span class="hljs-number">0</span>]width = image.shape[<span class="hljs-number">1</span>]channels = image.shape[<span class="hljs-number">2</span>]print(<span class="hljs-string">"width: %s height: %s channel: %s"</span>%(width, height, channels))<span class="hljs-comment"># 将图像转换为灰度图</span>gray_img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)<span class="hljs-comment"># 将数据格式转换为32位浮点数</span>gray_img = np.float32(gray_img)<span class="hljs-comment"># 使用合适值作为输入参数检测角点，得到的结果图用来标出角点</span>corners_img = cv.cornerHarris(gray_img, block_size, sobel_size, k)</code></pre><p>输出：</p><pre><code class="hljs tex">(225, 225, 3)width: 225 height: 225 channel: 3</code></pre><pre><code class="hljs python">kernel = cv.getStructuringElement(cv.MORPH_RECT,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))dst = cv.dilate(corners_img, kernel)<span class="hljs-comment"># dst = cv.dilate(corners_img,None)</span><span class="hljs-comment"># image[dst&gt;0.05*dst.max()] = [255,0,0]</span><span class="hljs-comment"># num = dst &gt; 0.05 * dst.max()</span><span class="hljs-comment"># count = np.sum(num)</span><span class="hljs-comment"># print(count)</span>count = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> range(height):    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(width):        pix = dst[r,c]        <span class="hljs-comment"># 阈值，蓝色标记角点</span>        <span class="hljs-keyword">if</span> pix &gt; <span class="hljs-number">0.05</span> * dst.max():            cv.circle(image,(c,r),<span class="hljs-number">2</span>,(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">-1</span>)            count += <span class="hljs-number">1</span>print(count)cv.imwrite(<span class="hljs-string">'harris_img1.jpg'</span>,image)</code></pre><p>输出：</p><pre><code class="hljs tex">741</code></pre><p>处理后图片如下：</p><p><img src="/article/8b3bd9dd/harris_img1.jpg" srcset="/img/loading.gif" alt="harris_img1"></p><pre><code class="hljs python"><span class="hljs-comment"># 使用点画图，点小一点，好看一丢丢把。。。</span>image[dst&gt;<span class="hljs-number">0.05</span>*dst.max()] = [<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]</code></pre><p><img src="/article/8b3bd9dd/harris_img2.jpg" srcset="/img/loading.gif" alt="harris_img1"></p><h2 id="2-2-Shi-Tomasi角点检测"><a href="#2-2-Shi-Tomasi角点检测" class="headerlink" title="2.2 Shi-Tomasi角点检测"></a>2.2 Shi-Tomasi角点检测</h2><p>opencv提供了实现Shi-Tomasi 角点检测函数：<strong>cv2.goodFeaturesToTrack()</strong>，下面调用该接口进行Harris特征点检测。</p><p>函数：<strong>goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]])</strong></p><p>函数功能：流程大体与Harris相似。不同之处在于窗口分数计算公式不同。在检测到的角点中，所有低于质量水平的角点都会被忽略，合格角点按角点质量进行降序排列，保留质量最高的一个角点，将它附近（最小距离之内）的角点都删掉（类似于非极大值抑制），按这样的方式最后得到 N 个最佳角点。</p><p>函数参数：</p><ul><li><strong>image</strong>：输入灰度图像，float32类型</li><li><strong>maxCorners</strong>：返回角点的最大数目，值为0表表示不设置最大值限制，返回所有检测到的角点。</li><li><strong>qualityLevel</strong>：质量系数（小于1.0的正数，一般在0.01-0.1之间），表示可接受角点的最低质量水平。该系数乘以最好的角点分数（也就是上面较小的那个特征值），作为可接受的最小分数；例如，如果最好的角点分数值为1500且质量系数为0.01，那么所有质量分数小于15的角都将被忽略。</li><li><strong>minDistance</strong>：角之间最小欧式距离，忽略小于此距离的点。</li><li><strong>corners</strong>：输出角点坐标</li><li><strong>mask</strong>：可选的感兴趣区域，指定想要检测角点的区域。</li><li><strong>blockSize</strong>：默认为3，角点检测的邻域大小（窗口尺寸）</li><li><strong>useHarrisDetector</strong>：用于指定角点检测的方法，如果是true则使用Harris角点检测，false则使用Shi Tomasi算法。默认为False。</li><li><strong>k</strong>：默认为0.04，Harris角点检测时使用。</li></ul><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> cv2maxCorners = <span class="hljs-number">600</span>qualityLevel = <span class="hljs-number">0.01</span>minDistance = <span class="hljs-number">10</span>img = cv2.imread(<span class="hljs-string">'image1.jpg'</span>)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)corners = cv2.goodFeaturesToTrack(gray, maxCorners, qualityLevel, minDistance)corners = np.int0(corners)count = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> corners:    x,y = i.ravel()    cv2.circle(img,(x,y), <span class="hljs-number">2</span>, (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">-1</span>)    <span class="hljs-comment"># img[y,x] = [255,0,0]</span>    count += <span class="hljs-number">1</span>cv.imwrite(<span class="hljs-string">'Shi-Tomasi.jpg'</span>,img)img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)print(count)</code></pre><p><img src="Task01-Harris特征点检测器-兴趣点检测//Tomasi.jpg" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Harris和Shi-Tomasi都是基于梯度计算的角点检测方式。</p><p>Harris角点检测的性质：</p><ul><li><p>阈值界定角点数量</p></li><li><p>Harris角点算子对亮度和对比度的变化不敏感</p><p>因为在进行Harris角点检测时，使用了微分算子对图像进行微分运算，而微分运算对图像密度的拉升或收缩和对亮度的抬高或下降不敏感。换言之，对亮度和对比度的仿射变换并不改变Harris响应的极值点出现的位置，但是，由于阈值的选择，可能会影响角点检测的数量。</p><p><img src="/article/8b3bd9dd/image1-3.png" srcset="/img/loading.gif" alt="image-20200623145505861"></p><p>左图表示亮度变化，右图表示对比度变化。</p></li><li><p>Harris角点检测算子具有旋转不变性</p><p>Harris角点检测算子使用的是角点附近的区域灰度二阶矩矩阵。而二阶矩矩阵可以表示成一个椭圆，椭圆的长短轴正是二阶矩矩阵特征值平方根的倒数。当特征椭圆转动时，特征值并不发生变化，所以判断角点响应值RR也不发生变化，由此说明Harris角点检测算子具有旋转不变性。</p></li><li><p>Harris角点检测算子不具有尺度不变性</p><p><img src="/article/8b3bd9dd/image1-1.png" srcset="/img/loading.gif" alt="image-20200623003833148"></p><p>如上图所示，当图像被缩小时，在检测窗口尺寸不变的前提下，在窗口内所包含图像的内容是完全不同的。左侧的图像可能被检测为边缘或曲线，而右侧的图像则可能被检测为一个角点。</p></li></ul><p>基于梯度的角点检测器的缺点：计算复杂度高、图像中的噪声阻碍梯度计算。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%8B%EF%BC%89/Task01%20Harris%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B.md" target="_blank" rel="noopener">Task01 Harris特征点检测器-兴趣点检测</a></p><p><a href="https://zhuanlan.zhihu.com/p/83064609" target="_blank" rel="noopener">角点检测：Harris 与 Shi-Tomasi</a></p><p><a href="https://www.cnblogs.com/zyly/p/9508131.html" target="_blank" rel="noopener">Harris角点检测原理(赋源码)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Harris特征点检测器-兴趣点检测&quot;&gt;&lt;a href=&quot;#Harris特征点检测器-兴趣点检测&quot; class=&quot;headerlink&quot; title=&quot;Harris特征点检测器-兴趣点检测&quot;&gt;&lt;/a&gt;Harris特征点检测器-兴趣点检测&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;特则点&lt;/strong&gt;又称为兴趣点或角点，通常具有旋转不变性和光照不变性和视角不变性等优点，是图像的重要特征之一，常被应用到目标匹配、目标跟踪、三维重建等应用中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点特征&lt;/strong&gt;主要是指图像中的明显点，如突出的角点、边缘端点、权值点等等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;兴趣点提取(检测)算子&lt;/strong&gt;是用于点特征提取的算子&lt;/p&gt;
    
    </summary>
    
    
      <category term="图像处理下" scheme="http://yoursite.com/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8B/"/>
    
    
      <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Introduction_and_Word_Vectors</title>
    <link href="http://yoursite.com/article/fd8e5887.html"/>
    <id>http://yoursite.com/article/fd8e5887.html</id>
    <published>2020-06-24T09:48:44.000Z</published>
    <updated>2020-06-24T16:13:05.785Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Task1-Introduction-and-Word-Vectors"><a href="#Task1-Introduction-and-Word-Vectors" class="headerlink" title="Task1: Introduction and Word Vectors"></a>Task1: Introduction and Word Vectors</h1><p>理论部分</p><ul><li>介绍NLP研究的对象</li><li>如何表示单词的含义</li><li>Word2Vec方法的基本原理</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Task1-Introduction-and-Word-Vectors&quot;&gt;&lt;a href=&quot;#Task1-Introduction-and-Word-Vectors&quot; class=&quot;headerlink&quot; title=&quot;Task1: Introduction an
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2数理统计与描述性统计</title>
    <link href="http://yoursite.com/article/83114b11.html"/>
    <id>http://yoursite.com/article/83114b11.html</id>
    <published>2020-06-24T03:47:17.000Z</published>
    <updated>2020-07-05T03:24:15.810Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、数理统计概念"><a href="#一、数理统计概念" class="headerlink" title="一、数理统计概念"></a>一、数理统计概念</h1><h2 id="1-基本概念释义"><a href="#1-基本概念释义" class="headerlink" title="1.基本概念释义"></a>1.基本概念释义</h2><font color="blue">总体</font>：研究对象的全体，通常用一个随机变量表示。<font color="blue">个体</font>：组成总体的每个基本单元。<a id="more"></a>从总体$X$中随机抽取一部分个体$X_1,X_2,...,X_n$，称$X_1,X_2,...,X_n$为取自$X$的容量为$n$的样本。实际上，数理统计学中的总体是指与总体相联系的某个(某几个)数量指标$X$取值的全体。<font color="red">样本具有两重性，在一次具体抽样后是一组确定的数值。一般叙述中由于采取随机抽样，样本是一组随机变量(结果未知)。</font><p>一般地，用$X_1,X_2,…,X_n$，表示随机样本，取到的值记为$x_1,x_2,…,x_n$称为样本观测值。$n$为样本容量。</p><font color="blue">样本分布</font>：样本作为随机变量的概率分布。显然，样本分布取决于总体的性质和样本的性质。## 2.统计量与抽样数理统计的任务是采集和处理带有随机影响的数据，或者说收集样本并对之进行加工，在此基础上对研究的问题进行分析并作出一定的结论，这一过程称为<font color="blue">统计推断</font>。在统计推断过程中，对样本进行加工整理，实际上就是根据样本计算出一些量，把研究问题相关的信息集中起来。这种根据样本计算出的量就是<font color="blue">统计量</font>。因此，统计量是样本的某种函数。定义：设 $X_1,X_2,...,X_n $ 是总体 $X$ 的一个简单随机样本， $T(X_1, X_2,...,X_n)$ 为一个 $n$ 元连续函数，且 $T$ 中不包含任何关于总体的未知参数，则称 $T(X_1, X_2,...,X_n)$ 是一个统计量，称统计量的分布为抽样分布。常用的统计量<font color="blue">样本均值</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，称$\overline X =  \frac{1} {n} {\sum_{i=1}^{n}X_i}$为样本均值。通常用样本均值来估计总体分布的均值和对有关总体分布均值的假设作检验。</p><font color="blue">样本方差</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，$\overline X$ 为样本均值，称$S^2 =  \frac{1} {n-1} {\sum_{i=1}^{n}(X_i-\overline X)^2}$为样本方差。</p><p>通常用样本方差来估计总体分布的方差和对有关总体分布均值或方差的假设作检验。</p><font color="blue">k阶样本原点矩</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，称$A_k =  \frac{1} {n} {\sum_{i=1}^{n}X_i^k}$</p><p>为样本的 $k$ 阶原点矩（可以看到 $k=1$ 时，相当于样本均值），通常用样本的无阶原点矩来估计总体分布的 $k$ 阶原点矩。（可以看到 $k=1$ 时，相当于样本均值）</p><p>通常用样本的无阶原点矩来估计总体分布的 $k$ 阶原点矩。</p><font color="blue">k阶样本中心矩</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，$\overline X$ 为样本均值，称$M_k =  \frac{1} {n} {\sum_{i=1}^{n}(X_i-\overline X)^k}$为样本的 $k$ 阶中心矩。</p><p>通常用样本的 $k$ 阶中心矩来估计总体分布的 $k$ 阶中心矩。</p><font color="blue">顺序统计量</font><p>设 $X_1,X_2,…,X_n $ 是抽自总体 $X$ 的样本，$x_1,x_2,…，x_n$  为样本观测值。将 $x_1,x_2,…，x_n$  按照从小到大的顺序排列为$x_{(1)}&lt;=x_{(2)}&lt;=…&lt;=x_{(n)}$。</p><p>当样本 $X_1,X_2,…,X_n $ 取值 $x_1,x_2,…，x_n$  时，定义 $X_{(k)}$ 取值 $X_{(k)}(k=1,2，…,n)$，称  $X_{(1)},X_{(2)},…,X_{(n)} $ 为 $X_1,X_2,…,X_n $ 的顺序统计量。</p><p>显然，$X_{(1)} =min {X_i}$ 是样本观察中最小的一个，称为最小顺序统计量。$X_{(n)} =max {X_i}$ 是样本观测值中取值最大的一个，成为最大顺序统计量。称$X_{（r）}$ 为第 $r$ 个顺序统计量。</p><h1 id="二、描述性统计"><a href="#二、描述性统计" class="headerlink" title="二、描述性统计"></a>二、描述性统计</h1><h2 id="1-数据集中趋势的度量"><a href="#1-数据集中趋势的度量" class="headerlink" title="1.数据集中趋势的度量"></a>1.数据集中趋势的度量</h2><font color="blue">平均数</font><p>是表示一组数据集中趋势的量数，是指在一组数据中所有数据之和再除以这组数据的个数。</p><script type="math/tex; mode=display">\overline X =  \frac{1} {n} {\sum_{i=1}^{n}X_i}</script><font color="blue">中位数</font><p>是指在一组数据，按顺序排列后，居于中间位置的数。中位数描述数据中心位置的数字特征。</p><p>对于对称分布的数据，均值与中位数比较接近；对于偏态分布的数据，均值与中位数不同。中位数不受异常值的影响，具有稳健性。</p><script type="math/tex; mode=display">m_p=\left\{\begin{array}{lcl}x_{[np]+1},       &      & {当np不是整数}\\\frac{1}{2}(x_{(np)}+x_{(np+1)})     &      & {当np是整数时}\\\end{array} \right.</script><font color="blue">频数</font><p>同一观测值在一组数据中出现的次数（掷骰子中，一共掷了20次，出现数字5的次数）。</p><font color="blue">众数</font><p>就是一组数据中，出现次数最多的那个数（几个数）。</p><font color="blue">均值 vs 中位数 vs 众数</font><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">优点</th><th style="text-align:center">缺点</th></tr></thead><tbody><tr><td style="text-align:center">均值</td><td style="text-align:center">充分利用所有数据，适用性强</td><td style="text-align:center">容易受极端值影响</td></tr><tr><td style="text-align:center">中位数</td><td style="text-align:center">不受极端值影响</td><td style="text-align:center">缺乏敏感性</td></tr><tr><td style="text-align:center">众数</td><td style="text-align:center">不受极端值影响；当数据具有明显的集中趋势时，代表性好</td><td style="text-align:center">缺乏唯一性</td></tr></tbody></table></div><font color="blue">百分位数</font><p>百分位数是中位数的推广，将数据按从小到大排列后，对于$0 \leq p &lt; 1$，它的p分位点定义为</p><script type="math/tex; mode=display">m_p=\left\{\begin{array}{lcl}x_{[np]+1},       &      & {当np不是整数}\\\frac{1}{2}(x_{(np)}+x_{(np+1)})     &      & {当np是整数时}\\\end{array} \right.</script><p>其中，<strong>[np]</strong>表示<strong>np</strong>的整数部分。所以，0.5分位数（第50百分位数）就是中位数。</p><p><img src="/article/83114b11/百分位数图.png" srcset="/img/loading.gif" alt></p><h3 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h3><p><img src="/article/83114b11/image1" srcset="/img/loading.gif" alt="image-20200624144415651"></p><h2 id="2-数据离散趋势的度量"><a href="#2-数据离散趋势的度量" class="headerlink" title="2.数据离散趋势的度量"></a>2.数据离散趋势的度量</h2><p>表示数据分散（离散，差异）程度的特征量有方差，标准差，极差以及变异系数等。</p><font color="blue">方差</font><p>用来计算每一个变量（观察值）与总体均数之间的差异。实际工作中，总体均数难以得到时，应用样本统计量代替总体参数，经校正后，样本方差计算公式:</p><script type="math/tex; mode=display">s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\overline{x})}^2</script><font color="blue">标准差</font><p>样本方差的开平方成为样本标准差。</p><script type="math/tex; mode=display">s=\sqrt{s^2}=\sqrt{\frac{1}{n-1}\sum_{1}^{n-1}{(x_i-\overline{x})}^2}</script><font color="blue">极差</font>$$R=X_{(n)}-x_{(1)}=\max(x)-\min(x)$$数据越分散，极差越大。<font color="blue">变异系数</font><ul><li>是刻画数据相对分散性的一种度量。变异系数只在平均值不为零时有定义，而且一般适用于平均值大于零的情况。变异系数也被称为<strong>标准离差率</strong>或<strong>单位风险</strong>。</li><li>当需要比较两组数据离散程度大小的时候，如果两组数据的测量尺度相差太大，或者数据量纲的不同，变异系数可以消除测量尺度和量纲的影响。</li></ul><script type="math/tex; mode=display">CV=100\times\frac{s}{x}(\%)</script><font color="blue">四分位差</font><p>样本上、下四分位数之差称为四分位差(或半极差)。</p><script type="math/tex; mode=display">R_1 = Q_3 - Q_1</script><p>它也是度量样本分散性的重要数字特征，特别对于具有异常值的数据，它作为分散性具有稳健性<font color="red">  <strong>（见百分位数示意图）</strong></font>。</p><h3 id="python实现-1"><a href="#python实现-1" class="headerlink" title="python实现"></a>python实现</h3><p><img src="/article/83114b11/image2.PNG" srcset="/img/loading.gif" alt="image2" style="zoom:80%;"></p><h2 id="3-分布特征"><a href="#3-分布特征" class="headerlink" title="3.分布特征"></a>3.分布特征</h2><p> <strong>引言：</strong>描述一个随机变量，不仅要说明它能够取那些值，而且还要关心它取这些值的概率（可能性）。</p><font color="blue">离散变量和连续变量</font><ul><li><font color="blue">离散型随机变量</font>：其数值只能用<strong>自然数或整数</strong>单位表示。例如，<font color="red">  班级人数，电脑台数等，</font>只能按计量单位数计数，这种变量的数值一般用计数方法取得。</li><li><font color="blue">连续变量</font>：在一定区间内可以任意取值的变量。例如，<font color="red">人体测量的身高，体重等。</font></li></ul><font color="blue">概率函数</font>：用函数的形式来表达概率。- **连续型随机变量**的概率函数就叫做**概率密度函数**。- **离散型随机变量**的概率函数就叫做**概率质量函数**。<font color="blue">分布函数</font><p>设$X$是一个随机变量，对任意的实数$x$，令$F(x)=P(X&lt;=x),x \in (-\infty,+\infty)$，则称$F(x)$是随机变量$X$的分布函数(概率累积函数)。</p><p><img src="/article/83114b11/分布函数与密度函数的关系.png" srcset="/img/loading.gif" alt></p><font color="red">密度函数与分布函数关系</font> <font color="blue">正态分布</font>：也称高斯分布，是一个非常常见的连续概率分布，概率密度函数为$$f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\{- \frac{(x-u)^2}{2\sigma^2} \}}, \qquad -\infty<x<+\infty $$ 则称$x$服从$n(u,\sigma^2)$分布。 ![uniform2](2数理统计与描述性统计 uniform2.png) <font color="red">正态分布的概率密度函数曲线 <p><img src="/article/83114b11/标准正态分布.png" srcset="/img/loading.gif" alt="标准正态分布"></p><p>​              <font color="red">标准正态分布和对应区间上积分（面积）的百分比</font> </p><p>这个概念可以推广到一般正态分布。$[u-3\sigma,u+3\sigma]$的概率密度曲线之下的面积占总面积的99.7%，是著名的$3\sigma$原则。</p><h2 id="4-偏度与峰度"><a href="#4-偏度与峰度" class="headerlink" title="4. 偏度与峰度"></a>4. 偏度与峰度</h2><p><font color="blue">偏度（skewness）</font>：也称为偏态，是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。直观看来就是密度函数曲线尾部的相对长度。<strong>偏度刻画的是分布函数（数据）的对称性。</strong></p><p>关于均值对称的数据其偏度系数为0，右侧更分散的数据偏度系数为正，左侧更分散的数据偏度系数为负.</p><ul><li><p><strong>正态分布的偏度为0，两侧尾部长度对称。</strong></p></li><li><p><strong>左偏:</strong>   </p><p>1）若以$b_s$表示偏度。$b_s&lt;0$称分布具有<strong>负偏离</strong>，也称左偏态；</p><p>2）此时数据位于均值左边的比位于右边的少，直观表现为左边的尾部相对于与右边的尾部要长；</p><p>3）<strong>因为有少数变量值很小，使曲线左侧尾部拖得很长；</strong></p></li><li><p><strong>右偏：</strong></p><p>1）$b_s&gt;0$称分布具有<strong>正偏离</strong>，也称右偏态；</p><p>2）此时数据位于均值右边的比位于左边的少，直观表现为右边的尾部相对于与左边的尾部要长；</p><p>3）<strong>因为有少数变量值很大，使曲线右侧尾部拖得很长；</strong></p></li></ul><p><font color="blue">峰度(peakedness;kurtosis)</font>：说明的是分布曲线在平均值处峰值高低的特征数。直观看来，峰度反映了峰部的尖度。样本的峰度是和正态分布相比较而言统计量，如果峰度大于正态分布峰度，峰的形状比较尖，比正态分布峰要陡峭。反之亦然。<strong>峰度刻画的是分布函数的集中和分散程度。</strong></p><p><img src="/article/83114b11/偏态与峰度.png" srcset="/img/loading.gif" alt></p><h3 id="公式与python实现"><a href="#公式与python实现" class="headerlink" title="公式与python实现"></a>公式与python实现</h3><p><font color="blue">样本偏度系数</font>：</p><script type="math/tex; mode=display">g_1 = \frac{n}{(n-1)(n-2)s^3}\sum_{i=1}^{n}{(x_i-\overline{x})}^3=\frac{n^2 u_3}{(n-1)(n-2)s^3}</script><p><font color="blue">样本峰度系数</font>：</p><script type="math/tex; mode=display">g_2 = \frac{n(n+1)}{(n-1)(n-2)(n-3)s^4}\sum_{i=1}^{n}{(x_i-\overline{x})}^4-3\frac{(n-1)^2}{(n-2)(n-3)}</script><p><img src="/article/83114b11/image3.PNG" srcset="/img/loading.gif" style="zoom:80%;"></p></x<+\infty>]]></content>
    
    <summary type="html">
    
      数理统计与描述性统计概念与python简单实现
    
    </summary>
    
    
      <category term="概率统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="概率统计" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
</feed>
