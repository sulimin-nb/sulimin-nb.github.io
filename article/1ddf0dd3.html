<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/pocket%20watch.ico">
  <link rel="icon" type="image/png" href="/img/pocket%20watch.ico">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="向前向前向前ε=( o｀ω′)ノ写博客如喝汤的汤达人">
  <meta name="author" content="sulimin">
  <meta name="keywords" content="机器学习，数据挖掘，计算机视觉">
  <title>自然语言处理-新闻文本分类 - 苏</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="苏" type="application/atom+xml">
</head>


<body>
  <header style="height: 40vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Sulimin</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/target/">
                <i class="iconfont icon-clipcheck"></i>
                每日目标
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-07-21 22:29">
      2020年7月21日 晚上
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      85
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-post-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-post-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p class="note note-info">
                
                  本文最后更新于：2020年8月5日 上午
                
              </p>
            
            <article class="markdown-body">
              <h2 id="赛题理解"><a href="#赛题理解" class="headerlink" title="赛题理解"></a>赛题理解</h2><ul>
<li>赛题名称：零基础入门NLP之新闻文本分类</li>
<li>赛题任务：赛题以自然语言处理为背景，要求选手对新闻文本进行分类，这是一个典型的字符识别问题。</li>
</ul>
<h3 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h3><p>赛题以匿名处理后的新闻数据为赛题数据，数据集报名后可见并可下载。赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐的文本数据。</p>
<p>赛题数据由以下几个部分构成：训练集20w条样本，测试集A包括5w条样本，测试集B包括5w条样本。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。</p>
<h3 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h3><p>处理后的赛题训练数据如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>label</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>57 44 66 56 2 3 3 37 5 41 9 57 44 47 45 33 13 63 58 31 17 47 0 1 1 69 26 60 62 15 21 12 49 18 38 20 50 23 57 44 45 33 25 28 47 22 52 35 30 14 24 69 54 7 48 19 11 51 16 43 26 34 53 27 64 8 4 42 36 46 65 69 29 39 15 37 57 44 45 33 69 54 7 25 40 35 30 66 56 47 55 69 61 10 60 42 36 46 65 37 5 41 32 67 6 59 47 0 1 1 68</td>
</tr>
</tbody>
</table>
</div>
<p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p>
<h3 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h3><p>评价标准为类别<code>f1_score</code>的均值，选手提交结果与实际测试集的类别进行对比，结果越大越好。</p>
<h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>使用<code>Pandas</code>库完成数据读取操作，并对赛题数据进行分析。</p>
<h2 id="数据读取与数据分析"><a href="#数据读取与数据分析" class="headerlink" title="数据读取与数据分析"></a>数据读取与数据分析</h2><p>本章主要内容为数据读取和数据分析，具体使用<code>Pandas</code>库完成数据读取操作，并对赛题数据进行分析构成。</p>
<h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul>
<li>学习使用<code>Pandas</code>读取赛题数据</li>
<li>分析赛题数据的分布规律</li>
</ul>
<h3 id="数据读取-1"><a href="#数据读取-1" class="headerlink" title="数据读取"></a>数据读取</h3><p>赛题数据虽然是文本数据，每个新闻是不定长的，但任然使用csv格式进行存储。因此可以直接用<code>Pandas</code>完成数据读取的操作。</p>
<pre><code class="hljs reasonml">import pandas <span class="hljs-keyword">as</span> pd
train_df = pd.read<span class="hljs-constructor">_csv('..<span class="hljs-operator">/</span><span class="hljs-params">input</span><span class="hljs-operator">/</span><span class="hljs-params">train_set</span>.<span class="hljs-params">csv</span>', <span class="hljs-params">sep</span>='\<span class="hljs-params">t</span>', <span class="hljs-params">nrows</span>=100)</span></code></pre>
<p>这里的<code>read_csv</code>由三部分构成：</p>
<ul>
<li>读取的文件路径，这里需要根据改成你本地的路径，可以使用相对路径或绝对路径；</li>
<li>分隔符<code>sep</code>，为每列分割的字符，设置为<code>\t</code>即可；</li>
<li>读取行数<code>nrows</code>，为此次读取文件的函数，是数值类型（由于数据集比较大，建议先设置为100）；</li>
</ul>
<p><a href="https://camo.githubusercontent.com/3cfb75e212d4e8259de4c4151e52b405dbe256de/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333733303733392e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/3cfb75e212d4e8259de4c4151e52b405dbe256de/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333733303733392e706e67" srcset="/img/loading.gif" alt="task2_train_head"></a></p>
<p>上图是读取好的数据，是表格的形式。第一列为新闻的类别，第二列为新闻的字符。</p>
<h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>在读取完成数据集后，我们还可以对数据集进行数据分析的操作。虽然对于非结构数据并不需要做很多的数据分析，但通过数据分析还是可以找出一些规律的。</p>
<p>此步骤我们读取了所有的训练集数据，在此我们通过数据分析希望得出以下结论：</p>
<ul>
<li>赛题数据中，新闻文本的长度是多少？</li>
<li>赛题数据的类别分布是怎么样的，哪些类别比较多？</li>
<li>赛题数据中，字符分布是怎么样的？</li>
</ul>
<h4 id="句子长度分析"><a href="#句子长度分析" class="headerlink" title="句子长度分析"></a>句子长度分析</h4><p>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度。统计并如下：</p>
<pre><code class="hljs sas"><span class="hljs-name">%pylab</span> inline
train_df[<span class="hljs-string">'text_len'</span>] = train_df[<span class="hljs-string">'text'</span>].apply(lambda <span class="hljs-meta">x</span>: l<span class="hljs-meta">en(</span><span class="hljs-meta">x</span>.split(<span class="hljs-string">' '</span>)))
p<span class="hljs-meta">rint(</span>train_df[<span class="hljs-string">'text_len'</span>].<span class="hljs-meta">describe</span>())</code></pre>
<p>输出结果为：</p>
<pre><code class="hljs yaml"><span class="hljs-string">Populating</span> <span class="hljs-string">the</span> <span class="hljs-string">interactive</span> <span class="hljs-string">namespace</span> <span class="hljs-string">from</span> <span class="hljs-string">numpy</span> <span class="hljs-string">and</span> <span class="hljs-string">matplotlib</span>
<span class="hljs-string">count</span>    <span class="hljs-number">200000.000000</span>
<span class="hljs-string">mean</span>        <span class="hljs-number">907.207110</span>
<span class="hljs-string">std</span>         <span class="hljs-number">996.029036</span>
<span class="hljs-string">min</span>           <span class="hljs-number">2.000000</span>
<span class="hljs-number">25</span><span class="hljs-string">%</span>         <span class="hljs-number">374.000000</span>
<span class="hljs-number">50</span><span class="hljs-string">%</span>         <span class="hljs-number">676.000000</span>
<span class="hljs-number">75</span><span class="hljs-string">%</span>        <span class="hljs-number">1131.000000</span>
<span class="hljs-string">max</span>       <span class="hljs-number">57921.000000</span>
<span class="hljs-attr">Name:</span> <span class="hljs-string">text_len,</span> <span class="hljs-attr">dtype:</span> <span class="hljs-string">float64</span></code></pre>
<p>对新闻句子的统计可以得出，本次赛题给定的文本比较长，每个句子平均由907个字符构成，最短的句子长度为2，最长的句子长度为57921。</p>
<p>下图将句子长度绘制了直方图，可见大部分句子的长度都几种在2000以内。</p>
<pre><code class="hljs routeros">_ = plt.hist(train_df[<span class="hljs-string">'text_len'</span>], <span class="hljs-attribute">bins</span>=200)
plt.xlabel(<span class="hljs-string">'Text char count'</span>)
plt.title(<span class="hljs-string">"Histogram of char count"</span>)</code></pre>
<p><a href="https://camo.githubusercontent.com/656471f35c5df332c6ca027756cfd048324e5727/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333833363930352e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/656471f35c5df332c6ca027756cfd048324e5727/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333833363930352e706e67" srcset="/img/loading.gif" alt="task2_char_hist"></a></p>
<h4 id="新闻类别分布"><a href="#新闻类别分布" class="headerlink" title="新闻类别分布"></a>新闻类别分布</h4><p>接下来可以对数据集的类别进行分布统计，具体统计每类新闻的样本个数。</p>
<pre><code class="hljs less"><span class="hljs-selector-tag">train_df</span><span class="hljs-selector-attr">['label']</span><span class="hljs-selector-class">.value_counts</span>()<span class="hljs-selector-class">.plot</span>(kind=<span class="hljs-string">'bar'</span>)
<span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.title</span>(<span class="hljs-string">'News class count'</span>)
<span class="hljs-selector-tag">plt</span><span class="hljs-selector-class">.xlabel</span>(<span class="hljs-string">"category"</span>)</code></pre>
<p><a href="https://camo.githubusercontent.com/ea8ab6a105f74fa197f29cb09e5dfd97ae6a573e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333932393239362e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/ea8ab6a105f74fa197f29cb09e5dfd97ae6a573e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333932393239362e706e67" srcset="/img/loading.gif" alt="task2_class_hist"></a></p>
<p>在数据集中标签的对应的关系如下：{‘科技’: 0, ‘股票’: 1, ‘体育’: 2, ‘娱乐’: 3, ‘时政’: 4, ‘社会’: 5, ‘教育’: 6, ‘财经’: 7, ‘家居’: 8, ‘游戏’: 9, ‘房产’: 10, ‘时尚’: 11, ‘彩票’: 12, ‘星座’: 13}</p>
<p>从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p>
<h4 id="字符分布统计"><a href="#字符分布统计" class="headerlink" title="字符分布统计"></a>字符分布统计</h4><p>接下来可以统计每个字符出现的次数，首先可以将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</p>
<pre><code class="hljs routeros"><span class="hljs-keyword">from</span> collections import Counter
all_lines = <span class="hljs-string">' '</span>.join(list(train_df[<span class="hljs-string">'text'</span>]))
word_count = Counter(all_lines.split(<span class="hljs-string">" "</span>))
word_count = sorted(word_count.items(), <span class="hljs-attribute">key</span>=lambda d:d[1], reverse = <span class="hljs-literal">True</span>)

<span class="hljs-builtin-name">print</span>(len(word_count))
<span class="hljs-comment"># 6869</span>

<span class="hljs-builtin-name">print</span>(word_count[0])
<span class="hljs-comment"># ('3750', 7482224)</span>

<span class="hljs-builtin-name">print</span>(word_count[-1])
<span class="hljs-comment"># ('3133', 1)</span></code></pre>
<p>从统计结果中可以看出，在训练集中总共包括6869个字，其中编号3750的字出现的次数最多，编号3133的字出现的次数最少。</p>
<p>这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。</p>
<pre><code class="hljs vim">train_df[<span class="hljs-string">'text_unique'</span>] = train_df[<span class="hljs-string">'text'</span>].apply(lambda <span class="hljs-keyword">x</span>: <span class="hljs-string">' '</span>.<span class="hljs-keyword">join</span>(<span class="hljs-keyword">list</span>(<span class="hljs-keyword">set</span>(<span class="hljs-keyword">x</span>.<span class="hljs-keyword">split</span>(<span class="hljs-string">' '</span>)))))
all_lines = <span class="hljs-string">' '</span>.<span class="hljs-keyword">join</span>(<span class="hljs-keyword">list</span>(train_df[<span class="hljs-string">'text_unique'</span>]))
word_count = Counter(all_lines.<span class="hljs-keyword">split</span>(<span class="hljs-string">" "</span>))
word_count = sorted(word_count.<span class="hljs-built_in">items</span>(), key=lambda d:<span class="hljs-keyword">int</span>(d[<span class="hljs-number">1</span>]), <span class="hljs-built_in">reverse</span> = True)

<span class="hljs-keyword">print</span>(word_count[<span class="hljs-number">0</span>])
# (<span class="hljs-string">'3750'</span>, <span class="hljs-number">197997</span>)

<span class="hljs-keyword">print</span>(word_count[<span class="hljs-number">1</span>])
# (<span class="hljs-string">'900'</span>, <span class="hljs-number">197653</span>)

<span class="hljs-keyword">print</span>(word_count[<span class="hljs-number">2</span>])
# (<span class="hljs-string">'648'</span>, <span class="hljs-number">191975</span>)</code></pre>
<h3 id="数据分析的结论"><a href="#数据分析的结论" class="headerlink" title="数据分析的结论"></a>数据分析的结论</h3><p>通过上述分析我们可以得出以下结论：</p>
<ol>
<li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；</li>
<li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；</li>
<li>赛题总共包括7000-8000个字符；</li>
</ol>
<p>通过数据分析，我们还可以得出以下结论：</p>
<ol>
<li>每个新闻平均字符个数较多，可能需要截断；</li>
<li>由于类别不均衡，会严重影响模型的精度；</li>
</ol>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析。</p>
<h3 id="本章作业"><a href="#本章作业" class="headerlink" title="本章作业"></a>本章作业</h3><ol>
<li>假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？</li>
<li>统计每类新闻中出现次数对多的字符</li>
</ol>
<h1 id="基于机器学习的文本分类"><a href="#基于机器学习的文本分类" class="headerlink" title="基于机器学习的文本分类"></a>基于机器学习的文本分类</h1><p>在上一章节，我们对赛题的数据进行了读取，并在末尾给出了两个小作业。如果你顺利完成了作业，那么你基本上对<code>Python</code>也比较熟悉了。在本章我们将使用传统机器学习算法来完成新闻分类的过程，将会结束到赛题的核心知识点。</p>
<h2 id="基于机器学习的文本分类-1"><a href="#基于机器学习的文本分类-1" class="headerlink" title="基于机器学习的文本分类"></a>基于机器学习的文本分类</h2><p>在本章我们将开始使用机器学习模型来解决文本分类。机器学习发展比较广，且包括多个分支，本章侧重使用传统机器学习，从下一章开始是基于深度学习的文本分类。</p>
<h3 id="学习目标-1"><a href="#学习目标-1" class="headerlink" title="学习目标"></a>学习目标</h3><ul>
<li>学会TF-IDF的原理和使用</li>
<li>使用sklearn的机器学习模型完成文本分类</li>
</ul>
<h3 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h3><p>机器学习是对能通过经验自动改进的计算机算法的研究。机器学习通过历史数据<strong>训练</strong>出<strong>模型</strong>对应于人类对经验进行<strong>归纳</strong>的过程，机器学习利用<strong>模型</strong>对新数据进行<strong>预测</strong>对应于人类利用总结的<strong>规律</strong>对新问题进行<strong>预测</strong>的过程。</p>
<p>机器学习有很多种分支，对于学习者来说应该优先掌握机器学习算法的分类，然后再其中一种机器学习算法进行学习。由于机器学习算法的分支和细节实在是太多，所以如果你一开始就被细节迷住了眼，你就很难知道全局是什么情况的。</p>
<p>如果你是机器学习初学者，你应该知道如下的事情：</p>
<ol>
<li>机器学习能解决一定的问题，但不能奢求机器学习是万能的；</li>
<li>机器学习算法有很多种，看具体问题需要什么，再来进行选择；</li>
<li>每种机器学习算法有一定的偏好，需要具体问题具体分析；</li>
</ol>
<p><a href="https://camo.githubusercontent.com/d8e9a12417a2a2a754a874af0ae163bb1bddbb0b/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333232333235332e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/d8e9a12417a2a2a754a874af0ae163bb1bddbb0b/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230333232333235332e6a7067" srcset="/img/loading.gif" alt="machine_learning_overview"></a></p>
<h3 id="文本表示方法-Part1"><a href="#文本表示方法-Part1" class="headerlink" title="文本表示方法 Part1"></a>文本表示方法 Part1</h3><p>在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。</p>
<p>但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。</p>
<h4 id="One-hot"><a href="#One-hot" class="headerlink" title="One-hot"></a>One-hot</h4><p>这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。</p>
<p>One-hot表示方法的例子如下：</p>
<pre><code class="hljs angelscript">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门
句子<span class="hljs-number">2</span>：我 喜 欢 上 海</code></pre>
<p>首先对所有句子的字进行索引，即将每个字确定一个编号：</p>
<pre><code class="hljs 1c">&#123;
	'我': <span class="hljs-number">1</span>, '爱': <span class="hljs-number">2</span>, '北': <span class="hljs-number">3</span>, '京': <span class="hljs-number">4</span>, '天': <span class="hljs-number">5</span>,
  '安': <span class="hljs-number">6</span>, '门': <span class="hljs-number">7</span>, '喜': <span class="hljs-number">8</span>, '欢': <span class="hljs-number">9</span>, '上': <span class="hljs-number">10</span>, '海': <span class="hljs-number">11</span>
&#125;</code></pre>
<p>在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量：</p>
<pre><code class="hljs angelscript">我：[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
爱：[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
...
海：[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]</code></pre>
<h4 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h4><p>Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。</p>
<pre><code class="hljs angelscript">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门
句子<span class="hljs-number">2</span>：我 喜 欢 上 海</code></pre>
<p>直接统计每个字出现的次数，并进行赋值：</p>
<pre><code class="hljs angelscript">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门
转换为 [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]

句子<span class="hljs-number">2</span>：我 喜 欢 上 海
转换为 [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]</code></pre>
<p>在sklearn中可以直接<code>CountVectorizer</code>来实现这一步骤：</p>
<pre><code class="hljs vbnet"><span class="hljs-keyword">from</span> sklearn.feature_extraction.<span class="hljs-keyword">text</span> import CountVectorizer
corpus = [
    <span class="hljs-comment">'This is the first document.',</span>
    <span class="hljs-comment">'This document is the second document.',</span>
    <span class="hljs-comment">'And this is the third one.',</span>
    <span class="hljs-comment">'Is this the first document?',</span>
]
vectorizer = CountVectorizer()
vectorizer.fit_transform(corpus).toarray()</code></pre>
<h4 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h4><p>N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。</p>
<p>如果N取值为2，则句子1和句子2就变为：</p>
<pre><code class="hljs angelscript">句子<span class="hljs-number">1</span>：我爱 爱北 北京 京天 天安 安门
句子<span class="hljs-number">2</span>：我喜 喜欢 欢上 上海</code></pre>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>TF-IDF 分数由两部分组成：第一部分是<strong>词语频率</strong>（Term Frequency），第二部分是<strong>逆文档频率</strong>（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p>
<pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">TF</span><span class="hljs-params">(t)</span></span>= 该词语在当前文档出现的次数 / 当前文档中词语的总数
<span class="hljs-function"><span class="hljs-title">IDF</span><span class="hljs-params">(t)</span></span>= log_e（文档总数 / 出现该词语的文档总数）</code></pre>
<h3 id="基于机器学习的文本分类-2"><a href="#基于机器学习的文本分类-2" class="headerlink" title="基于机器学习的文本分类"></a>基于机器学习的文本分类</h3><p>接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。</p>
<h4 id="Count-Vectors-RidgeClassifier"><a href="#Count-Vectors-RidgeClassifier" class="headerlink" title="Count Vectors + RidgeClassifier"></a>Count Vectors + RidgeClassifier</h4><pre><code class="hljs reasonml">import pandas <span class="hljs-keyword">as</span> pd

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import f1_score

train_df = pd.read<span class="hljs-constructor">_csv('..<span class="hljs-operator">/</span><span class="hljs-params">input</span><span class="hljs-operator">/</span><span class="hljs-params">train_set</span>.<span class="hljs-params">csv</span>', <span class="hljs-params">sep</span>='\<span class="hljs-params">t</span>', <span class="hljs-params">nrows</span>=15000)</span>

vectorizer = <span class="hljs-constructor">CountVectorizer(<span class="hljs-params">max_features</span>=3000)</span>
train_test = vectorizer.fit<span class="hljs-constructor">_transform(<span class="hljs-params">train_df</span>['<span class="hljs-params">text</span>'])</span>

clf = <span class="hljs-constructor">RidgeClassifier()</span>
clf.fit(train_test<span class="hljs-literal">[:<span class="hljs-number">10000</span>]</span>, train_df<span class="hljs-literal">['<span class="hljs-identifier">label</span>']</span>.values<span class="hljs-literal">[:<span class="hljs-number">10000</span>]</span>)

val_pred = clf.predict(train_test<span class="hljs-literal">[<span class="hljs-number">10000</span>:]</span>)
print(f1<span class="hljs-constructor">_score(<span class="hljs-params">train_df</span>['<span class="hljs-params">label</span>'].<span class="hljs-params">values</span>[10000:], <span class="hljs-params">val_pred</span>, <span class="hljs-params">average</span>='<span class="hljs-params">macro</span>')</span>)
# <span class="hljs-number">0.74</span></code></pre>
<h4 id="TF-IDF-RidgeClassifier"><a href="#TF-IDF-RidgeClassifier" class="headerlink" title="TF-IDF + RidgeClassifier"></a>TF-IDF + RidgeClassifier</h4><pre><code class="hljs reasonml">import pandas <span class="hljs-keyword">as</span> pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import f1_score

train_df = pd.read<span class="hljs-constructor">_csv('..<span class="hljs-operator">/</span><span class="hljs-params">input</span><span class="hljs-operator">/</span><span class="hljs-params">train_set</span>.<span class="hljs-params">csv</span>', <span class="hljs-params">sep</span>='\<span class="hljs-params">t</span>', <span class="hljs-params">nrows</span>=15000)</span>

tfidf = <span class="hljs-constructor">TfidfVectorizer(<span class="hljs-params">ngram_range</span>=(1,3)</span>, max_features=<span class="hljs-number">3000</span>)
train_test = tfidf.fit<span class="hljs-constructor">_transform(<span class="hljs-params">train_df</span>['<span class="hljs-params">text</span>'])</span>

clf = <span class="hljs-constructor">RidgeClassifier()</span>
clf.fit(train_test<span class="hljs-literal">[:<span class="hljs-number">10000</span>]</span>, train_df<span class="hljs-literal">['<span class="hljs-identifier">label</span>']</span>.values<span class="hljs-literal">[:<span class="hljs-number">10000</span>]</span>)

val_pred = clf.predict(train_test<span class="hljs-literal">[<span class="hljs-number">10000</span>:]</span>)
print(f1<span class="hljs-constructor">_score(<span class="hljs-params">train_df</span>['<span class="hljs-params">label</span>'].<span class="hljs-params">values</span>[10000:], <span class="hljs-params">val_pred</span>, <span class="hljs-params">average</span>='<span class="hljs-params">macro</span>')</span>)
# <span class="hljs-number">0.87</span></code></pre>
<h2 id="基于深度学习的文本分类"><a href="#基于深度学习的文本分类" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><p>与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。</p>
<h3 id="学习目标-2"><a href="#学习目标-2" class="headerlink" title="学习目标"></a>学习目标</h3><ul>
<li>学习FastText的使用和基础原理</li>
<li>学会使用验证集进行调参</li>
</ul>
<h3 id="文本表示方法-Part2"><a href="#文本表示方法-Part2" class="headerlink" title="文本表示方法 Part2"></a>文本表示方法 Part2</h3><h4 id="现有文本表示方法的缺陷"><a href="#现有文本表示方法的缺陷" class="headerlink" title="现有文本表示方法的缺陷"></a>现有文本表示方法的缺陷</h4><p>在上一章节，我们介绍几种文本表示方法：</p>
<ul>
<li>One-hot</li>
<li>Bag of Words</li>
<li>N-gram</li>
<li>TF-IDF</li>
</ul>
<p>也通过sklean进行了相应的实践，相信你也有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。</p>
<p>与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：FastText、Word2Vec和Bert。在本章我们将介绍FastText，将在后面的内容介绍Word2Vec和Bert。</p>
<h4 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h4><p>FastText是一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。</p>
<p>所以FastText是一个三层的神经网络，输入层、隐含层和输出层。</p>
<p><a href="https://camo.githubusercontent.com/4e01004146c81db5ee15df1b373374b3ff145bfa/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343835363538392e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/4e01004146c81db5ee15df1b373374b3ff145bfa/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343835363538392e706e67" srcset="/img/loading.gif" alt="fast_text"></a></p>
<p>下图是使用keras实现的FastText网络结构：</p>
<p><a href="https://camo.githubusercontent.com/d4f33365b75bdddd0c80a857dc1a9e99789f1600/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343234393436332e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/d4f33365b75bdddd0c80a857dc1a9e99789f1600/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343234393436332e6a7067" srcset="/img/loading.gif" alt="keras_fasttext"></a></p>
<p>FastText在文本分类任务上，是优于TF-IDF的：</p>
<ul>
<li>FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类</li>
<li>FastText学习到的Embedding空间维度比较低，可以快速进行训练</li>
</ul>
<p>如果想深度学习，可以参考论文：</p>
<p>Bag of Tricks for Efficient Text Classification, <a href="https://arxiv.org/abs/1607.01759" target="_blank" rel="noopener">https://arxiv.org/abs/1607.01759</a></p>
<h3 id="基于FastText的文本分类"><a href="#基于FastText的文本分类" class="headerlink" title="基于FastText的文本分类"></a>基于FastText的文本分类</h3><p>FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本： <a href="https://github.com/facebookresearch/fastText/tree/master/python" target="_blank" rel="noopener">https://github.com/facebookresearch/fastText/tree/master/python</a></p>
<ul>
<li>pip安装</li>
</ul>
<pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> fasttext</code></pre>
<ul>
<li>源码安装</li>
</ul>
<pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">https</span>://github.com/facebookresearch/fastText.git
cd fastText
sudo pip install .</code></pre>
<p>两种安装方法都可以安装，如果你是初学者可以优先考虑使用pip安装。</p>
<ul>
<li>分类模型</li>
</ul>
<pre><code class="hljs routeros">import pandas as pd
<span class="hljs-keyword">from</span> sklearn.metrics import f1_score

<span class="hljs-comment"># 转换为FastText需要的格式</span>
train_df = pd.read_csv(<span class="hljs-string">'../input/train_set.csv'</span>, <span class="hljs-attribute">sep</span>=<span class="hljs-string">'\t'</span>, <span class="hljs-attribute">nrows</span>=15000)
train_df[<span class="hljs-string">'label_ft'</span>] = <span class="hljs-string">'__label__'</span> + train_df[<span class="hljs-string">'label'</span>].astype(str)
train_df[[<span class="hljs-string">'text'</span>,<span class="hljs-string">'label_ft'</span>]].iloc[:-5000].to_csv(<span class="hljs-string">'train.csv'</span>, <span class="hljs-attribute">index</span>=None, <span class="hljs-attribute">header</span>=None, <span class="hljs-attribute">sep</span>=<span class="hljs-string">'\t'</span>)

import fasttext
model = fasttext.train_supervised(<span class="hljs-string">'train.csv'</span>, <span class="hljs-attribute">lr</span>=1.0, <span class="hljs-attribute">wordNgrams</span>=2, 
                                  <span class="hljs-attribute">verbose</span>=2, <span class="hljs-attribute">minCount</span>=1, <span class="hljs-attribute">epoch</span>=25, <span class="hljs-attribute">loss</span>=<span class="hljs-string">"hs"</span>)

val_pred = [model.predict(x)[0][0].split(<span class="hljs-string">'__'</span>)[-1] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_df.iloc[-5000:][<span class="hljs-string">'text'</span>]]
<span class="hljs-builtin-name">print</span>(f1_score(train_df[<span class="hljs-string">'label'</span>].values[-5000:].astype(str), val_pred, <span class="hljs-attribute">average</span>=<span class="hljs-string">'macro'</span>))
<span class="hljs-comment"># 0.82</span></code></pre>
<p>此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。</p>
<h3 id="如何使用验证集调参"><a href="#如何使用验证集调参" class="headerlink" title="如何使用验证集调参"></a>如何使用验证集调参</h3><p>在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？</p>
<ul>
<li>通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度</li>
<li>通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合</li>
</ul>
<p><a href="https://camo.githubusercontent.com/3c19cda9d91954875be0b59abe99fad024552d29/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343430333834342e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/3c19cda9d91954875be0b59abe99fad024552d29/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230343430333834342e706e67" srcset="/img/loading.gif" alt="train_val"></a></p>
<p>这里我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致。</p>
<pre><code class="hljs dockerfile">label2id = &#123;&#125;
for i in range(total):
    <span class="hljs-keyword">label</span><span class="bash"> = str(all_labels[i])</span>
    if <span class="hljs-keyword">label</span><span class="bash"> not <span class="hljs-keyword">in</span> label2id:</span>
        label2id[<span class="hljs-keyword">label</span><span class="bash">] = [i]</span>
    else:
        label2id[<span class="hljs-keyword">label</span><span class="bash">].append(i)</span></code></pre>
<p>通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。</p>
<h3 id="本章小结-1"><a href="#本章小结-1" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了FastText的原理和基础使用，并进行相应的实践。然后介绍了通过10折交叉验证划分数据集。</p>
<h3 id="本章作业-1"><a href="#本章作业-1" class="headerlink" title="本章作业"></a>本章作业</h3><ul>
<li>阅读FastText的文档，尝试修改参数，得到更好的分数</li>
<li>基于验证集的结果调整超参数，使得模型性能更优</li>
</ul>
<h2 id="基于深度学习的文本分类-1"><a href="#基于深度学习的文本分类-1" class="headerlink" title="基于深度学习的文本分类"></a>基于深度学习的文本分类</h2><h3 id="学习目标-3"><a href="#学习目标-3" class="headerlink" title="学习目标"></a>学习目标</h3><ul>
<li>了解Transformer的原理和基于预训练语言模型（Bert）的词表示</li>
<li>学会Bert的使用，具体包括pretrain和finetune</li>
</ul>
<h3 id="文本表示方法Part4"><a href="#文本表示方法Part4" class="headerlink" title="文本表示方法Part4"></a>文本表示方法Part4</h3><h4 id="Transformer原理"><a href="#Transformer原理" class="headerlink" title="Transformer原理"></a>Transformer原理</h4><p>Transformer是在”<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a>“中提出的，模型的编码部分是一组编码器的堆叠（论文中依次堆叠六个编码器），模型的解码部分是由相同数量的解码器的堆叠。</p>
<p><a href="https://camo.githubusercontent.com/1da32c07cd7d544113673ba47624f0a47af51ba6/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313034363636382e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1da32c07cd7d544113673ba47624f0a47af51ba6/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313034363636382e706e67" srcset="/img/loading.gif" alt="img"></a></p>
<p>我们重点关注编码部分。他们结构完全相同，但是并不共享参数，每一个编码器都可以拆解成两部分。在对输入序列做词的向量化之后，它们首先流过一个self-attention层，该层帮助编码器在它编码单词的时候能够看到输入序列中的其他单词。self-attention的输出流向一个前向网络（Feed Forward Neural Network），每个输入位置对应的前向网络是独立互不干扰的。最后将输出传入下一个编码器。</p>
<p><a href="https://camo.githubusercontent.com/70959c74cbf7ac3c3dae3fd0eceddf180715cb39/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313131353934352e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/70959c74cbf7ac3c3dae3fd0eceddf180715cb39/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313131353934352e706e67" srcset="/img/loading.gif" alt="img"></a></p>
<p>这里能看到Transformer的一个关键特性，每个位置的词仅仅流过它自己的编码器路径。在self-attention层中，这些路径两两之间是相互依赖的。<strong>前向网络层则没有这些依赖性</strong>，但这些路径在流经前向网络时可以并行执行。</p>
<p>Self-Attention中使用多头机制，使得不同的attention heads所关注的的部分不同。</p>
<p><a href="https://camo.githubusercontent.com/1173d083304d3e60a81342717cd52d3f1a5124e1/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313135333638372e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1173d083304d3e60a81342717cd52d3f1a5124e1/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313135333638372e706e67" srcset="/img/loading.gif" alt="img"></a></p>
<p>编码”it”时，一个attention head集中于”the animal”，另一个head集中于“tired”，某种意义上讲，模型对“it”的表达合成了的“animal”和“tired”两者。</p>
<p>对于自注意力的详细计算，欢迎大家参考<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">Jay Alammar</a>关于Transformer的博客，这里不再展开。</p>
<p>除此之外，为了使模型保持单词的语序，模型中添加了位置编码向量。如下图所示，每行对应一个向量的位置编码。因此，第一行将是我们要添加到输入序列中第一个单词的嵌入的向量。每行包含512个值—每个值都在1到-1之间。因为左侧是用sine函数生成，右侧是用cosine生成，所以可以观察到中间显著的分隔。</p>
<p><a href="https://camo.githubusercontent.com/1598ac84eeeca72eb271d7bc4696d36f564bc759/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313233323731362e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1598ac84eeeca72eb271d7bc4696d36f564bc759/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313233323731362e706e67" srcset="/img/loading.gif" alt="img"></a></p>
<p>编码器结构中值得提出注意的一个细节是，在每个子层中（Self-attention, FFNN），都有残差连接，并且紧跟着<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">layer-normalization</a>。如果我们可视化向量和LayerNorm操作，将如下所示：</p>
<p><a href="https://camo.githubusercontent.com/09ac8f9634bff9681b191c398e4b943d34f972c3/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313935353731332e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/09ac8f9634bff9681b191c398e4b943d34f972c3/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313935353731332e706e67" srcset="/img/loading.gif" alt="img"></a></p>
<h4 id="基于预训练语言模型的词表示"><a href="#基于预训练语言模型的词表示" class="headerlink" title="基于预训练语言模型的词表示"></a>基于预训练语言模型的词表示</h4><p>基于预训练语言模型的词表示由于可以建模上下文信息，进而解决传统静态词向量不能建模“一词多义”语言现象的问题。最早提出的ELMo基于两个单向LSTM，将从左到右和从右到左两个方向的隐藏层向量表示拼接学习上下文词嵌入。而GPT用Transformer代替LSTM作为编码器，首先进行了语言模型预训练，然后在下游任务微调模型参数。但GPT由于仅使用了单向语言模型，因此难以建模上下文信息。为了解决以上问题，研究者们提出了BERT，BERT模型结构如下图所示，它是一个基于Transformer的多层Encoder，通过执行一系列预训练，进而得到深层的上下文表示。</p>
<p><a href="https://camo.githubusercontent.com/f5297c1c8c1e71180cb62aca463503d21122e911/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313331363136372e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/f5297c1c8c1e71180cb62aca463503d21122e911/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313331363136372e706e67" srcset="/img/loading.gif" alt="bert_elmo"></a></p>
<p>ELMo论文题目中Deep是指双向双层LSTM，而更关键的在于context。传统方法生成的单词映射表的形式，即先为每个单词生成一个静态的词向量，之后这个单词的表示就被固定住了，不会跟着上下文的变化而做出改变。事实上，由于一词多义的语言现象，静态词向量是有很大的弊端的。以bank为例，如果训练语料的足够大，事先学好的词向量中混杂着所有的语义。而当下游应用时，即使在新句子中，bank的上下文里包含money等词，我们基本可以确定bank是“银行”的语义而不是在其他上下文中的“河床”的语义，但是由于静态词向量不能跟随上下文而进行变化，所以bank的表示中还是混杂着多种语义。为了解决这一问题，ELMo首先进行了语言模型预训练，然后在下游任务中动态调整Word Embedding，因此最后输出的词表示能够充分表达单词在上下文中的特定语义，进而解决一词多义的问题。</p>
<p>GPT来自于openai，是一种生成式预训练模型。GPT 除了将ELMo中的LSTM替换为Transformer 的Encoder外，更开创了NLP界基于预训练-微调的新范式。尽管GPT采用的也是和ELMo相同的两阶段模式，但GPT在第一个阶段并没有采取ELMo中使用两个单向双层LSTM拼接的结构，而是采用基于自回归式的单向语言模型。</p>
<p>Google在NAACL 2018发表的论文中提出了BERT，与GPT相同，BERT也采用了预训练-微调这一两阶段模式。但在模型结构方面，BERT采用了ELMO的范式，即使用双向语言模型代替GPT中的单向语言模型，但是BERT的作者认为ELMo使用两个单向语言模型拼接的方式太粗暴，因此在第一阶段的预训练过程中，BERT提出掩码语言模型，即类似完形填空的方式，通过上下文来预测单词本身，而不是从右到左或从左到右建模，这允许模型能够自由地编码每个层中来自两个方向的信息。而为了学习句子的词序关系，BERT将Transformer中的三角函数位置表示替换为可学习的参数，其次为了区别单句和双句输入，BERT还引入了句子类型表征。BERT的输入如图所示。此外，为了充分学习句子间的关系，BERT提出了下一个句子预测任务。具体来说，在训练时，句子对中的第二个句子有50％来自与原有的连续句子，而其余50%的句子则是通过在其他句子中随机采样。同时，消融实验也证明，这一预训练任务对句间关系判断任务具有很大的贡献。除了模型结构不同之外，BERT在预训练时使用的无标签数据规模要比GPT大的多。</p>
<p><a href="https://camo.githubusercontent.com/1fa48883f724ed604556d3bb9d241511012d1972/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313334383435362e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1fa48883f724ed604556d3bb9d241511012d1972/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313334383435362e706e67" srcset="/img/loading.gif" alt="bert_input"></a></p>
<p>在第二阶段，与GPT相同，BERT也使用Fine-Tuning模式来微调下游任务。如下图所示，BERT与GPT不同，它极大的减少了改造下游任务的要求，只需在BERT模型的基础上，通过额外添加Linear分类器，就可以完成下游任务。具体来说，对于句间关系判断任务，与GPT类似，只需在句子之间加个分隔符，然后在两端分别加上起始和终止符号。在进行输出时，只需把句子的起始符号[CLS]在BERT最后一层中对应的位置接一个Softmax+Linear分类层即可；对于单句分类问题，也与GPT类似，只需要在句子两段分别增加起始和终止符号，输出部分和句间关系判断任务保持一致即可；对于问答任务，由于需要输出答案在给定段落的起始和终止位置，因此需要先将问题和段落按照句间关系判断任务构造输入，输出只需要在BERT最后一层中第二个句子，即段落的每个单词对应的位置上分别接判断起始和终止位置的分类器；最后，对于NLP中的序列标注问题，输入与单句分类任务一致，不同的是在BERT最后一层中每个单词对应的位置上接分类器即可。</p>
<p><a href="https://camo.githubusercontent.com/265b6f273c9731886f63f66b7beb2805226cee73/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313430393538322e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/265b6f273c9731886f63f66b7beb2805226cee73/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313430393538322e706e67" srcset="/img/loading.gif" alt="bert_task"></a></p>
<p>更重要的是，BERT开启了NLP领域“预训练-微调”这种两阶段的全新范式。在第一阶段首先在海量无标注文本上预训练一个双向语言模型，这里特别值得注意的是，将Transformer作为特征提取器在解决并行性和长距离依赖问题上都要领先于传统的RNN或者CNN，通过预训练的方式，可以将训练数据中的词法、句法、语法知识以网络参数的形式提炼到模型当中，在第二阶段使用下游任务的数据Fine-tuning不同层数的BERT模型参数，或者把BERT当作特征提取器生成BERT Embedding，作为新特征引入下游任务。这种两阶段的全新范式尽管是来自于计算机视觉领域，但是在自然语言处理领域一直没有得到很好的运用，而BERT作为近些年NLP突破性进展的集大成者，最大的亮点可以说不仅在于模型性能好，并且几乎所有NLP任务都可以很方便地基于BERT进行改造，进而将预训练学到的语言学知识引入下游任务，进一步提升模型的性能。</p>
<h3 id="基于Bert的文本分类"><a href="#基于Bert的文本分类" class="headerlink" title="基于Bert的文本分类"></a>基于Bert的文本分类</h3><h4 id="Bert-Pretrain"><a href="#Bert-Pretrain" class="headerlink" title="Bert Pretrain"></a>Bert Pretrain</h4><p>预训练过程使用了Google基于Tensorflow发布的BERT源代码。首先从原始文本中创建训练数据，由于本次比赛的数据都是ID，这里重新建立了词表，并且建立了基于空格的分词器。</p>
<pre><code class="hljs ruby"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WhitespaceTokenizer</span>(<span class="hljs-title">object</span>):</span>
    <span class="hljs-string">""</span><span class="hljs-string">"WhitespaceTokenizer with vocab."</span><span class="hljs-string">""</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, vocab_file)</span></span><span class="hljs-symbol">:</span>
        <span class="hljs-keyword">self</span>.vocab = load_vocab(vocab_file)
        <span class="hljs-keyword">self</span>.inv_vocab = &#123;<span class="hljs-symbol">v:</span> k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> <span class="hljs-keyword">self</span>.vocab.items()&#125;

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, text)</span></span><span class="hljs-symbol">:</span>
        split_tokens = whitespace_tokenize(text)
        output_tokens = []
        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> <span class="hljs-symbol">split_tokens:</span>
            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">in</span> <span class="hljs-keyword">self</span>.<span class="hljs-symbol">vocab:</span>
                output_tokens.append(token)
            <span class="hljs-symbol">else:</span>
                output_tokens.append(<span class="hljs-string">"[UNK]"</span>)
        <span class="hljs-keyword">return</span> output_tokens

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convert_tokens_to_ids</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, tokens)</span></span><span class="hljs-symbol">:</span>
        <span class="hljs-keyword">return</span> convert_by_vocab(<span class="hljs-keyword">self</span>.vocab, tokens)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convert_ids_to_tokens</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, ids)</span></span><span class="hljs-symbol">:</span>
        <span class="hljs-keyword">return</span> convert_by_vocab(<span class="hljs-keyword">self</span>.inv_vocab, ids)</code></pre>
<p>预训练由于去除了NSP预训练任务，因此将文档处理多个最大长度为256的段，如果最后一个段的长度小于256/2则丢弃。每一个段执行按照BERT原文中执行掩码语言模型，然后处理成tfrecord格式。</p>
<pre><code class="hljs vim">def create_segments_from_document(document, max_segment_length):
    <span class="hljs-string">""</span><span class="hljs-string">"Split single document to segments according to max_segment_length."</span><span class="hljs-string">""</span>
    assert <span class="hljs-built_in">len</span>(document) == <span class="hljs-number">1</span>
    document = document[<span class="hljs-number">0</span>]
    document_len = <span class="hljs-built_in">len</span>(document)

    <span class="hljs-built_in">index</span> = <span class="hljs-keyword">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, document_len, max_segment_length))
    other_len = document_len % max_segment_length
    <span class="hljs-keyword">if</span> other_len &gt; max_segment_length / <span class="hljs-number">2</span>:
        <span class="hljs-built_in">index</span>.<span class="hljs-keyword">append</span>(document_len)

    segments = []
    <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-built_in">index</span>) - <span class="hljs-number">1</span>):
        segment = document[<span class="hljs-built_in">index</span>[i]: <span class="hljs-built_in">index</span>[i+<span class="hljs-number">1</span>]]
        segments.<span class="hljs-keyword">append</span>(segment)

    <span class="hljs-keyword">return</span> segments</code></pre>
<p>在预训练过程中，也只执行掩码语言模型任务，因此不再计算下一句预测任务的loss。</p>
<pre><code class="hljs lisp">(<span class="hljs-name">masked_lm_loss</span>, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
    bert_config, model.get_sequence_output(), model.get_embedding_table(),
    masked_lm_positions, masked_lm_ids, masked_lm_weights)

total_loss = masked_lm_loss</code></pre>
<p>为了适配句子的长度，以及减小模型的训练时间，我们采取了BERT-mini模型，详细配置如下。</p>
<pre><code class="hljs json">&#123;
  <span class="hljs-attr">"hidden_size"</span>: <span class="hljs-number">256</span>,
  <span class="hljs-attr">"hidden_act"</span>: <span class="hljs-string">"gelu"</span>,
  <span class="hljs-attr">"initializer_range"</span>: <span class="hljs-number">0.02</span>,
  <span class="hljs-attr">"vocab_size"</span>: <span class="hljs-number">5981</span>,
  <span class="hljs-attr">"hidden_dropout_prob"</span>: <span class="hljs-number">0.1</span>,
  <span class="hljs-attr">"num_attention_heads"</span>: <span class="hljs-number">4</span>,
  <span class="hljs-attr">"type_vocab_size"</span>: <span class="hljs-number">2</span>,
  <span class="hljs-attr">"max_position_embeddings"</span>: <span class="hljs-number">256</span>,
  <span class="hljs-attr">"num_hidden_layers"</span>: <span class="hljs-number">4</span>,
  <span class="hljs-attr">"intermediate_size"</span>: <span class="hljs-number">1024</span>,
  <span class="hljs-attr">"attention_probs_dropout_prob"</span>: <span class="hljs-number">0.1</span>
&#125;</code></pre>
<p>由于我们的整体框架使用Pytorch，因此需要将最后一个检查点转换成Pytorch的权重。</p>
<pre><code class="hljs reasonml">def convert<span class="hljs-constructor">_tf_checkpoint_to_pytorch(<span class="hljs-params">tf_checkpoint_path</span>, <span class="hljs-params">bert_config_file</span>, <span class="hljs-params">pytorch_dump_path</span>)</span>:
    # Initialise PyTorch model
    config = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">BertConfig</span>.</span></span>from<span class="hljs-constructor">_json_file(<span class="hljs-params">bert_config_file</span>)</span>
    print(<span class="hljs-string">"Building PyTorch model from configuration: &#123;&#125;"</span>.format(str(config)))
    model = <span class="hljs-constructor">BertForPreTraining(<span class="hljs-params">config</span>)</span>

    # Load weights from tf checkpoint
    load<span class="hljs-constructor">_tf_weights_in_bert(<span class="hljs-params">model</span>, <span class="hljs-params">config</span>, <span class="hljs-params">tf_checkpoint_path</span>)</span>

    # Save pytorch-model
    print(<span class="hljs-string">"Save PyTorch model to &#123;&#125;"</span>.format(pytorch_dump_path))
    torch.save(model.state<span class="hljs-constructor">_dict()</span>, pytorch_dump_path)</code></pre>
<p>预训练消耗的资源较大，硬件条件不允许的情况下建议<strong>直接下载开源的模型</strong></p>
<h4 id="Bert-Finetune"><a href="#Bert-Finetune" class="headerlink" title="Bert Finetune"></a>Bert Finetune</h4><p><a href="https://camo.githubusercontent.com/6ee5331933f1637d5835522efb64f4951f6213ec/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313532363332362e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/6ee5331933f1637d5835522efb64f4951f6213ec/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313532363332362e706e67" srcset="/img/loading.gif" alt="experimental) Dynamic Quantization on BERT — PyTorch Tutorials 1.5 ..."></a></p>
<p>微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。</p>
<pre><code class="hljs nix">sequence_output, <span class="hljs-attr">pooled_output</span> = \
    self.bert(<span class="hljs-attr">input_ids=input_ids,</span> <span class="hljs-attr">token_type_ids=token_type_ids)</span>

<span class="hljs-keyword">if</span> self.pooled:
    <span class="hljs-attr">reps</span> = pooled_output
<span class="hljs-keyword">else</span>:
    <span class="hljs-attr">reps</span> = sequence_output[:, <span class="hljs-number">0</span>, :]  <span class="hljs-comment"># sen_num x 256</span>

<span class="hljs-keyword">if</span> self.training:
    <span class="hljs-attr">reps</span> = self.dropout(reps)</code></pre>
<h3 id="本章小结-2"><a href="#本章小结-2" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了Bert的原理和使用，具体包括pretrain和finetune两部分。</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/article/fcab5e80.html">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">python01-变量、运算符与数据类型</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/article/1d82fc97.html">
                        <span class="hidden-mobile">论文阅读2</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    function loadValine() {
      addScript('https://cdn.staticfile.org/valine/1.4.14/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "gIlJLkkEOolcxea5daBOIYB5-gzGzoHsz",
          app_key: "Xwqmsb7cDIWaQB2GG2fCnSHf",
          placeholder: "ヾﾉ≧∀≦)o 来呀！吐槽一番吧！",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: true,
          recordIP: true,
          serverURLs: "",
        });
      });
    }
    createObserver(loadValine, 'vcomments');
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" target="_blank" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer>
  (function () {
    // 查询存储的记录
    function getRecord(Counter, target) {
      return new Promise(function (resolve, reject) {
        Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({target})))
          .then(resp => resp.json())
          .then(({results, code, error}) => {
            if (code === 401) {
              throw error;
            }
            if (results && results.length > 0) {
              var record = results[0];
              resolve(record);
            } else {
              Counter('post', '/classes/Counter', {target, time: 0})
                .then(resp => resp.json())
                .then((record, error) => {
                  if (error) {
                    throw error;
                  }
                  resolve(record);
                }).catch(error => {
                console.error('Failed to create', error);
                reject(error);
              });
            }
          }).catch((error) => {
          console.error('LeanCloud Counter Error:', error);
          reject(error);
        });
      })
    }

    // 发起自增请求
    function increment(Counter, incrArr) {
      return new Promise(function (resolve, reject) {
        Counter('post', '/batch', {
          "requests": incrArr
        }).then((res) => {
          res = res.json();
          if (res.error) {
            throw res.error;
          }
          resolve(res);
        }).catch((error) => {
          console.error('Failed to save visitor count', error);
          reject(error);
        });
      });
    }

    // 构建自增请求体
    function buildIncrement(objectId) {
      return {
        "method": "PUT",
        "path": `/1.1/classes/Counter/${ objectId }`,
        "body": {
          "time": {
            '__op': 'Increment',
            'amount': 1
          }
        }
      }
    }

    // 校验是否为有效的 UV
    function validUV() {
      var key = 'LeanCloud_UV_Flag';
      var flag = localStorage.getItem(key);
      if (flag) {
        // 距离标记小于 24 小时则不计为 UV
        if (new Date().getTime() - parseInt(flag) <= 86400000) {
          return false;
        }
      }
      localStorage.setItem(key, new Date().getTime().toString());
      return true;
    }

    function addCount(Counter) {
      var enableIncr = 'true' === 'true' && window.location.hostname !== 'localhost';
      var getterArr = [];
      var incrArr = [];

      // 请求 PV 并自增
      var pvCtn = document.querySelector('#leancloud-site-pv-container');
      if (pvCtn || enableIncr) {
        var pvGetter = getRecord(Counter, 'site-pv').then((record) => {
          incrArr.push(buildIncrement(record.objectId))
          var ele = document.querySelector('#leancloud-site-pv');
          if (ele) {
            ele.innerText = record.time + 1;
            if (pvCtn) {
              pvCtn.style.display = 'inline';
            }
          }
        });
        getterArr.push(pvGetter);
      }

      // 请求 UV 并自增
      var uvCtn = document.querySelector('#leancloud-site-uv-container');
      if (uvCtn || enableIncr) {
        var uvGetter = getRecord(Counter, 'site-uv').then((record) => {
          var vuv = validUV();
          vuv && incrArr.push(buildIncrement(record.objectId))
          var ele = document.querySelector('#leancloud-site-uv');
          if (ele) {
            ele.innerText = record.time + (vuv ? 1 : 0);
            if (uvCtn) {
              uvCtn.style.display = 'inline';
            }
          }
        });
        getterArr.push(uvGetter);
      }

      // 如果是文章，请求文章的浏览数，并自增
      if ('true' === 'true') {
        var viewCtn = document.querySelector('#leancloud-post-views-container');
        if (viewCtn || enableIncr) {
          var target = decodeURI('/article/1ddf0dd3.html');
          var viewGetter = getRecord(Counter, target).then((record) => {
            incrArr.push(buildIncrement(record.objectId))
            if (viewCtn) {
              var ele = document.querySelector('#leancloud-post-views');
              if (ele) {
                ele.innerText = (record.time || 0) + 1;
                viewCtn.style.display = 'inline';
              }
            }
          });
          getterArr.push(viewGetter);
        }
      }

      // 如果启动计数自增，批量发起自增请求
      if (enableIncr) {
        Promise.all(getterArr).then(() => {
          incrArr.length > 0 && increment(Counter, incrArr);
        })
      }
    }

    var app_id = 'gIlJLkkEOolcxea5daBOIYB5-gzGzoHsz'
    var app_key = 'Xwqmsb7cDIWaQB2GG2fCnSHf'
    var server_url = ''

    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${ api_server }/1.1${ url }`, {
          method,
          headers: {
            'X-LC-Id': app_id,
            'X-LC-Key': app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };

      addCount(Counter);
    }

    var api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${ app_id.slice(0, 8).toLowerCase() }.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(resp => resp.json())
        .then(({api_server}) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>






  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "自然语言处理-新闻文本分类&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 50,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  



  
  
    <script type="text/javascript">
      //定义获取词语下标
      var a_idx = 0;
      jQuery(document).ready(function ($) {
        //点击body时触发事件
        $("body").click(function (e) {
          //需要显示的词语
          var a = new Array("富强", "民主", "文明", "和谐", "自由", "平等", "公正", "法治", "爱国", "敬业", "诚信", "友善");
          //设置词语给span标签
          var $i = $("<span/>").text(a[a_idx]);
          //下标等于原来下标+1  余 词语总数
          a_idx = (a_idx + 1) % a.length;
          //获取鼠标指针的位置，分别相对于文档的左和右边缘。
          //获取x和y的指针坐标
          var x = e.pageX, y = e.pageY;
          //在鼠标的指针的位置给$i定义的span标签添加css样式
          $i.css({
            "z-index": 999,
            "top": y - 20,
            "left": x,
            "position": "absolute",
            "font-weight": "bold",
            "color": rand_color()
          });
          // 随机颜色
          function rand_color() {
            return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
          }
          //在body添加这个标签
          $("body").append($i);
          //animate() 方法执行 CSS 属性集的自定义动画。
          //该方法通过CSS样式将元素从一个状态改变为另一个状态。CSS属性值是逐渐改变的，这样就可以创建动画效果。
          //详情请看http://www.w3school.com.cn/jquery/effect_animate.asp
          $i.animate({
            //将原来的位置向上移动180
            "top": y - 180,
            "opacity": 0
            //1500动画的速度
          }, 1500, function () {
            //时间到了自动删除
            $i.remove();
          });
        });
      })
      ;
    </script>
  











  

  

  

  

  

  





</body>
</html>
