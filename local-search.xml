<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Task04-HOG特征描述算子-行人检测</title>
    <link href="/article/760e3967.html"/>
    <url>/article/760e3967.html</url>
    
    <content type="html"><![CDATA[<p>HOG（Histogram of Oriented Gradients）HOG特征在对象检测与模式匹配中是一种常见的特征提取技术（深度学习之前），是基于本地像素块进行特征直方图提取的一种算法，对象局部的变形与光照影响有很好的稳定性，最初是用HOG特征来来识别人像，通过HOG特征提取+SVM训练，可以得到很好的效果，OpenCV已经有相应的接口。</p><p>HOG特征实在2005年CVPR的会议发表，在图像手工特征提取方面具有里程碑式的意义，当时在行人检测领域获得了极大成功。</p><h1 id="4-4-算法实现"><a href="#4-4-算法实现" class="headerlink" title="4.4 算法实现"></a>4.4 算法实现</h1><h2 id="1-HOG特征描述符"><a href="#1-HOG特征描述符" class="headerlink" title="1. HOG特征描述符"></a>1. HOG特征描述符</h2><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>&emsp;&emsp;局部目标的外表和形状可以被局部梯度的分布很好的描述，即使我们不知道对应的梯度和边缘的位置。(本质：梯度的统计信息，梯度主要存在于边缘edge或角落corner的地方)<!--局部目标的外表和形状可以被局部梯度或边缘方向的分布很好的描述，即使我们不知道对应的梯度和边缘的位置。(本质：梯度的统计信息，梯度主要存在于边缘edge或角落corner的地方)--></p><h3 id="宏观"><a href="#宏观" class="headerlink" title="宏观"></a>宏观</h3><p>&emsp;&emsp;特征描述符就是通过提取图像的有用信息，并且丢弃无关信息来简化图像的表示。</p><p>&emsp;&emsp;HOG特征描述符可以将3通道的彩色图像转换成一定长度的特征向量。</p><p>&emsp;&emsp;那么我们就需要定义什么是“有用的”，什么是“无关的”。这里的“有用”，是指对于什么目的有用，显然特征向量对于观察图像是没有用的，但是它对于像图像识别和目标检测这样的任务非常有用。当将这些特征向量输入到类似支持向量机（SVM）这样的图像分类算法中时，会得到较好的结果。</p><p>&emsp;&emsp;那什么样的“特征”对分类任务是有用，比如我们想检测出马路上的车道线，那么我们可以通过边缘检测来找到这些车道线，在这种情况下，边缘信息就是“有用的”，而颜色信息是无关的。</p><p>&emsp;&emsp;方向梯度直方图(HOG)特征描述符常和线性支持向量机(SVM)配合使用，用于训练高精度的目标分类器。</p><h3 id="微观（硬核）"><a href="#微观（硬核）" class="headerlink" title="微观（硬核）"></a>微观（硬核）</h3><p>在HOG特征描述符中，梯度方向的分布，也就是梯度方向的直方图被视作特征。图像的梯度(x和y导数)非常有用，因为边缘和拐角(强度突变的区域)周围的梯度幅度很大，并且边缘和拐角比平坦区域包含更多关于物体形状的信息。</p><p>HOG特征是一种图像局部特征，基本思路是将图像划分为很多小的连通区域，即细胞单元Cell，然后对Cell的<strong>梯度幅值和方向</strong>进行投票统计，形成基于梯度特性的直方图。把直方图在图像更大的范围内(又名区间或者Block)进行归一化。<strong>归一化的块描述符叫做HOG描述子feature descriptor。</strong>将检测窗口中的所有块的HOG描述子组合成最终的特征向量。然后使用SVM分类器进行目标和非目标的二分类（检测）。</p><p>HOG+SVM的工作流程如下：</p><p><img src="/article/760e3967/1328274-20180917143312309-1232187669.png" srcset="/img/loading.gif" alt="img"></p><p>首先对输入的图片进行预处理，然后计算像素点的梯度特性，包括梯度幅值和梯度方向。然后投票统计形成梯度直方图，然后对blocks进行normalize，最后收集到检测窗口的HOG feature(一行多维的vector)放入SVM里进行监督学习，实现行人的检测。接下来对上述HOG的主要步骤进行学习。（车轱辘话又来了一遍┗|｀O′|┛ 嗷~~）</p><p><font color="red">检测窗口在整个图像的所有位置和尺度进行扫描，并对输出的金字塔进行非极大值抑制来检测目标</font>（检测窗口的大小一般为128x64）</p><h2 id="2-HOG特征的原理"><a href="#2-HOG特征的原理" class="headerlink" title="2. HOG特征的原理"></a>2. HOG特征的原理</h2><p>接下来让我们进入到计算图像的HOG特征描述符的具体步骤。<br>以下面这张图片为例（宽高为100x200）:</p><p><img src="/article/760e3967/v2-2ccc671e60031942dca8a129410a0383_720w.jpg" srcset="/img/loading.gif" alt="img"></p><h3 id="图形预处理"><a href="#图形预处理" class="headerlink" title="图形预处理"></a>图形预处理</h3><p>预处理包括灰度化和Gamma变换。</p><p>灰度处理是可选操作，因为灰度图像和彩色图像都可以用于计算梯度图。对于彩色图像，先对三通道颜色值分别计算梯度，然后取梯度值最大的那个作为该像素的梯度。</p><p>然后进行伽马矫正，调节图像对比度，减少光照对图像的影响（包括光照不均和局部阴影），使过曝或者欠曝的图像恢复正常，更接近人眼看到的图像。更详细的内容参考<a href="https://www.cnblogs.com/qiqibaby/p/5325193.html" target="_blank" rel="noopener">图像处理之gamma矫正</a>。</p><p>Gamma矫正公式：$f(I) = I^{\gamma}$，其中$I$表示图像，$\gamma$表示幂指数。($\gamma$越大，图像越暗；为1时，表示没有变化。)<br>如图，当$\gamma$取不同的值时对应的输入输出曲线( $\gamma$=1时输入输出保持一致) ：<br>1） 当$\gamma$&lt;1时，输入图像的低灰度值区域动态范围变大，进而图像低灰度值区域对比度得以增强；在高灰度值区域，动态范围变小，进而图像高灰度值区域对比度得以降低。 最终，图像整体的灰度变亮。</p><p>2） 当$\gamma$&gt;1时，输入图像的低灰度值区域动态范围变小，进而图像低灰度值区域对比度得以降低；在高灰度值区域，动态范围变大，进而图像高灰度值区域对比度得以增强。 最终，图像整体的灰度变暗。</p><p><img src="/article/760e3967/image-1.jpg" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>代码：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> pltimg = cv2.imread(<span class="hljs-string">'*.png'</span>, <span class="hljs-number">0</span>)img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)img2 = np.power(img/float(np.max(img)),<span class="hljs-number">1</span>/<span class="hljs-number">2.2</span>)plt.imshow(img2)plt.axis(<span class="hljs-string">'off'</span>)plt.show()</code></pre><p>放图，左图是$\gamma=0.5$，中图是$\gamma=1$，右图是$\gamma=1.5$：</p><center class="half">    <img src="/article/760e3967/0_5_hog.jpg" srcset="/img/loading.gif" width="200" hight="400"><img src="/article/760e3967/1_hog.jpg" srcset="/img/loading.gif" width="200" hight="400"><img src="/article/760e3967/1_5_hog.jpg" srcset="/img/loading.gif" width="200" hight="400"></center><p>作者在他的博士论文里有提到，对于涉及大量的类内颜色变化，如猫，狗和马等动物，没标准化的RGB图效果更好，而牛，羊的图做gamma颜色校正后效果更好。是否用gamma校正分情况把。(●ˇ∀ˇ●)</p><h1 id="计算图像梯度"><a href="#计算图像梯度" class="headerlink" title="计算图像梯度"></a>计算图像梯度</h1><p>为了得到梯度直方图，那么首先需要计算图像水平方向和垂直方向梯度。可以通过使用以下内核过滤图像实现，分别用于计算水平梯度和垂直梯度。</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}d_x = I(x+1, y) - I(x-1,y) \\d_y = I(x, y+1) - I(x,y-1)\end{aligned}\end{equation}</script><p><img src="/article/760e3967/gradient-kernels.jpg" srcset="/img/loading.gif" alt="Gradient Kernels"></p><p>一般使用特定的卷积核对图像滤波实现，可选用的卷积模板有：sobel算子、Prewitt算子、Roberts模板等等。<br> 可以使用内核大小为1的sobel算子获取相同结果，OpenCV也是如此。<br>利用sobel水平和垂直算子与输入图像卷积计算$dx$、$dy$：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}Sobel_{X} &= \begin{bmatrix}1 \\ 0 \\ -1\end{bmatrix} *             \begin{bmatrix}1 & 2 &  1\end{bmatrix}            =  \begin{bmatrix}1 & 2 & 1\\                             0 & 0 & 0\\                             -1 & -2 & -1\end{bmatrix} \\Sobel_{Y} &= \begin{bmatrix}1 \\ 2 \\  1\end{bmatrix} *             \begin{bmatrix}1 & 0 & -1\end{bmatrix}            =  \begin{bmatrix}1 & 0 & -1\\                             2 & 0 & -2\\                             1 & 0 & -1\end{bmatrix} \\\end{aligned}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}\begin{aligned}d_x = f(x, y)*Sobel_x(x,y) \\d_y = f(x, y)*Sobel_y(x,y)\end{aligned}\end{equation}</script><p>进一步得到图像梯度的幅值：$M(x,y) = \sqrt{d^{2}_{x}(x,y)+d^{2}_{y}(x,y)}$<br>（简化计算，幅值也可以做近似：$M(x,y) = |d_{x}(x,y)|+|d_{y}(x,y)|$）<br>图像梯度的方向：$\theta_{M} = \arctan(d_y/d_x)$</p><p>这里需要注意的是：梯度方向和图像边缘方向是互相正交的。</p><p><img src="/article/760e3967/20160511100108955" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>代码：</p><pre><code class="hljs python">mport cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># Read image</span>img = cv2.imread(<span class="hljs-string">'*.jpg'</span>)img = np.float32(img) / <span class="hljs-number">255.0</span>  <span class="hljs-comment"># 归一化</span><span class="hljs-comment"># 计算x和y方向的梯度</span>gx = cv2.Sobel(img, cv2.CV_32F, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ksize=<span class="hljs-number">1</span>)gy = cv2.Sobel(img, cv2.CV_32F, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, ksize=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算合梯度的幅值和方向（角度）</span>mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=<span class="hljs-literal">True</span>)</code></pre><p>下图展示了梯度：</p><center class="half">    <img src="/article/760e3967/x_hog.jpg" srcset="/img/loading.gif" width="150" hight="300"><img src="/article/760e3967/y_hog.jpg" srcset="/img/loading.gif" width="150" hight="300"><img src="/article/760e3967/mag_hog.jpg" srcset="/img/loading.gif" width="150" hight="300"><img src="/article/760e3967/angle_hog.jpg" srcset="/img/loading.gif" width="150" hight="300"></center><p>第一个图：x-梯度的绝对值，第二个图：y梯度的绝对值 ，第三个图：梯度的幅值，第四个图：角度。<br>注意到，x-梯度在垂直线触发，y-梯度在水平线触发。梯度的幅值在有密集的剧烈改变时触发。当区域很平缓时，梯度没有明显变化。梯度图除去了很多不必要的信息（例如有颜色的背景），强调凸显线条。当你看到梯度图像，很容易想到这张图片有一个人。<br>在每个像素点，梯度有一个幅值和方向。对于有颜色的图像，计算三通道的梯度（如上图所示）。一个像素点的梯度的幅值是三通道中梯度幅值最大的值，角度也是最大梯度对应的角度。</p><h2 id="计算梯度直方图"><a href="#计算梯度直方图" class="headerlink" title="计算梯度直方图"></a>计算梯度直方图</h2><p>&emsp;&emsp; 此时，每一个像素点具有两个值：梯度幅值和梯度方向。<br>&emsp;&emsp; 在这一步中，图像被分成若干个8×8的Cell，如下图所示，例如我们将图像resize至64x128的大小，那么这幅图像就被划分为8x16个8x8的Cell单元，并为每个8×8的Cell计算梯度直方图。当然，Cell的划分也可以是其他值：16x16，8x16等，根据具体的场景确定。</p><p><img src="/article/760e3967/hog-Cells.png" srcset="/img/loading.gif" alt="8x8 Cells of HOG"></p><p>计算梯度直方图之前，先了解一下为什么要把图像分为若干个Cell?<br>这是因为如果对一整张梯度图逐像素计算，其中的有效特征是非常稀疏的，不但运算量大，而且会受到一些噪声干扰。使用特征描述符便提供了紧凑的表示。一个8x8的图像块包含8x8x3=192个像素值。一个8x8的Cell包含了8x8x2 = 128个值（每个像素包括梯度的大小和方向）。128个值将由9-bin的直方图（存储9个值的向量，想想坐标应该就明白了）。同时，计算Cell上的梯度直方图更具鲁棒性。逐像素计算梯度会产生噪音，直方图表示对噪音更不敏感。</p><p>&emsp;&emsp; 好，回到主题。<br>&emsp;&emsp; 在HOG中，每个8x8的Cell的梯度直方图本质是一个由9个数值组成的向量， 对应于0、20、40、60…160的梯度方向(角度)。那么原本Cell中8x8x2 = 128个值就由长度为9的向量来表示，用这种梯度直方图的表示方法，大大降低了计算量，同时又对光照等环境变化更加地鲁棒。<br>如下图所示，左图是衣服64x128的图像，被划分为8x16个8x8的Cell；中间的图像表示一个Cell中的梯度矢量，箭头朝向代表梯度方向，箭头长度代表梯度大小。<br>右图是 8×8 的Cell中表示梯度的原始数值，注意角度的范围介于0到180度之间，而不是0到360度， 这被称为“无符号”梯度，因为两个完全相反的方向被认为是相同的。0$^\circ$和180$^\circ$是相同的。（经验表明这样处理对于行人检测效果更好。）</p><p><img src="/article/760e3967/20200613153022928.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>接下来，计算Cell中像素的梯度直方图，将0-180度分成9等份，称为9个bins，分别是0，20，40…160。然后对每个bin中梯度的贡献进行统计：</p><p><img src="/article/760e3967/hog-histogram-1.png" srcset="/img/loading.gif" alt="Histogram computation in HOG"></p><p>&emsp;&emsp;这里采用加权投票统计，比如上面方向图中蓝圈包围的像素，角度为80度，这个像素对应的幅值为2，所以在直方图80度对应的bin加上2。红圈包围的像素，角度为10度，介于0度和20度之间，其幅值为4，那么这个梯度值就被按比例分给0度和20度对应的bin，也就是各加上2。</p><p><img src="/article/760e3967/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMzA1NjcxMy1mMDgzNzBkZGVmMmVhYzE1LnBuZw" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>&emsp;&emsp;再比如：（如上图所示）某像素的梯度幅值为13.6，方向为36，36度两侧的角度bin分别为20度和40度，那么按一定加权比例分别在20度和40度对应的bin加上梯度值，加权公式为：<br>20度对应的bin：(（40-36）/20) x13.6，<strong>分母的20表示20等份，其中4份给20度对应的bin</strong>；<br>40度对应的bin：(（36-20）/20) x13.6，<strong>分母的20表示20等份,其中16份给20度对应的bin</strong>；<br>&emsp;&emsp;还有一个细节需要注意，如果某个像素的梯度角度大于160度，也就是在160度到180度之间，那么把这个像素对应的梯度值按比例分给0度和160度对应的bin。如左下图绿色圆圈中的角度为165度，幅值为85，则按照同样的加权方式将85分别加到0度和160度对应的bin中。</p><p><img src="/article/760e3967/20200613160337781.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>&emsp;&emsp;对整个Cell进行投票统计，最终得到9-bin直方图：</p><p><img src="/article/760e3967/20200613161947892.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>&emsp;&emsp;可以看到直方图中，0度和160附近有很大的权重，说明了大多数像素的梯度向上或者向下，也就是这个Cell是个横向边缘。</p><h2 id="Block归一化"><a href="#Block归一化" class="headerlink" title="Block归一化"></a>Block归一化</h2><p>&emsp;&emsp;HOG特征将8×8的一个局部区域作为一个Cell，再以2×2个Cell作为一组，称为一个block，也就是说一个block表示16x16的区域。<br>&emsp;&emsp;由于每个Cell有9个值，一个block（2×2个Cell）则有36个值，HOG是通过滑动窗口的方式来得到block的，如下图所示：</p><p><img src="/article/760e3967/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi0yMzI0MTc3NDIwYzBhNzlkNDhkODQ0YjVlMTI0ZjFmM19iLndlYnA" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>为什么需要分Block呢？<br>我们已经为图像的8×8单元构造了基于梯度的直方图，但是图像的梯度对整体光照很敏感。这意味着对于特定的图像，图像的某些部分与其他部分相比会非常明亮。虽然不能从图像中完全消除，但是可以通过使用16×16个块来对梯度进行归一化来减少这种光照变化的影响。比如通过将所有像素值除以2来使图像变暗，那么梯度幅值将减小一半，因此直方图中的值也将减小一半。 </p><p>&emsp;&emsp;接下来对Block进行归一化。（再再再一次强调，归一化的目的是为了降低光照/迁移的影响）：<br>&emsp;&emsp;归一化的方法有很多：L1-norm、L2-norm、max/min等等，一般选择L2-norm。</p><script type="math/tex; mode=display">v←\frac{v}{\sqrt{\|v\|_2^2+\xi^2}} \quad (\xi是一个很小的数，主要是为了防止分母为0)；</script><p>&emsp;&emsp;例如对于一个[128，64，32]的三维向量来说，模长是$\sqrt{128^2+64^2+32^2}=146.64$，这叫做向量的L2范数。将这个向量的每个元素除以146.64就得到了归一化向量 [0.87, 0.43, 0.22]。<br>&emsp;&emsp;现在有一个新向量，是第一个向量的2倍 [128x2, 64x2, 32x2]，也就是 <code>[256, 128, 64]</code>，我们将这个向量进行归一化，你可以看到归一化后的结果与第一个向量归一化后的结果相同。所以，对向量进行归一化可以消除整体光照的影响。<br>&emsp;&emsp;知道了如何归一化，现在来对block的梯度直方图进行归一化（注意不是Cell），一个block有4个直方图，将这4个直方图拼接成长度为36的向量，然后对这个向量进行归一化。<br>&emsp;&emsp;因为使用的是滑动窗口，滑动步长为8个像素，一个Cell大小，每滑动一次，就在这个窗口上进行归一化计算得到长度为36的向量，并重复这个过程。如上图所示。</p><h2 id="获得HOG描述子"><a href="#获得HOG描述子" class="headerlink" title="获得HOG描述子"></a>获得HOG描述子</h2><p>每一个16x16大小的block将会得到一个长度为36x1的特征向量，并进行归一化。 那会得到多大的特征向量呢？<br>对于上图被划分8 x16个Cell ，每个block有2x2个Cell的话，那么Cell的个数为：(8-1)x(16-1)=105。<br>每个16x16 block由36x1维向量，合并所有105个block的特征，最终得到由36 x105=3780维向量表示的特征描述符。<br>获得HOG特征向量，就可以用来可视化和分类了。对于多维的HOG特征，SVM就可以排上用场了。</p><p>介绍以下Dalal等人的训练方法：</p><ol><li>提取正负样本的HOG特征；</li><li>用正负样本训练一个初始的分类器，然后由分类器生产检测器；</li><li>然后用初始分类器在负样本原图上进行行人检测，检测出来的矩形区域自然都是分类错误的负样本，这就是所谓的难例(hard examples)；</li><li>提取难例的HOG特征并结合第一步中的特征，重新训练，生成最终的检测器 ；</li></ol><p>这种二次训练的处理过程显著提高了每个检测器的表现，一般可以使得每个窗口的误报率(FPPW False Positives Per Window)下降5%。</p><h2 id="使用HOG特征数据"><a href="#使用HOG特征数据" class="headerlink" title="使用HOG特征数据"></a>使用HOG特征数据</h2><p>HOG特征本身是不支持旋转不变性与多尺度检测的，但是通过构建高斯金字塔实现多尺度的开窗检测就会得到不同分辨率的多尺度检测支持，如下图所示。详细内容可参考<a href="https://www.cnblogs.com/ronny/p/3886013.html" target="_blank" rel="noopener">尺度空间理论</a></p><p><img src="/article/760e3967/1328274-20180924221334307-1701938273.png" srcset="/img/loading.gif" alt="img"></p><p>OpenCV中HOG多尺度对象检测API如下：</p><pre><code class="hljs python">virtual void cv::HOGDescriptor::detectMultiScale(    InputArray  img,    std::vector&lt; Rect &gt; &amp;   foundLocations,    double  hitThreshold = <span class="hljs-number">0</span>,    Size    winStride = Size(),    Size    padding = Size(),    double  scale = <span class="hljs-number">1.05</span>,    double  finalThreshold = <span class="hljs-number">2.0</span>,    bool    useMeanshiftGrouping = false )Img-表示输入图像foundLocations-表示发现对象矩形框hitThreshold-表示SVM距离度量，默认<span class="hljs-number">0</span>表示，表示特征与SVM分类超平面之间winStride-表示窗口步长padding-表示填充scale-表示尺度空间finalThreshold-最终阈值，默认为<span class="hljs-number">2.0</span>useMeanshiftGrouping-不建议使用，速度太慢拉</code></pre><p>在<a href="https://sulimin-nb.github.io/article/ad28cab2.html">人脸检测之Haar分类器</a>这一节我们利用haar特征和级联分类器Adaboost检测人脸时我们使用过detectMultiScale()函数，级联分类器对象尝试在输入图像的不同尺度下检测对象，该函数有一个比较重要的参数scaleFactor(一般设置为1.3)，表示一个比率：即在每层金字塔中所获得的图像与上一层图像的比率，scaleFactor越小，金字塔的层数就越多，计算就越慢，计算量也会更大，但是计算结果相对更精确。</p><h1 id="基于OpenCV的简单实现"><a href="#基于OpenCV的简单实现" class="headerlink" title="基于OpenCV的简单实现"></a>基于OpenCV的简单实现</h1><p>代码：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    src = cv.imread(<span class="hljs-string">"*.jpg"</span>)    cv.imshow(<span class="hljs-string">"input"</span>, src)        hog = cv.HOGDescriptor()    hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())    <span class="hljs-comment"># Detect people in the image</span>    (rects, weights) = hog.detectMultiScale(src,                                            winStride=(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>),                                            padding=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>),                                            scale=<span class="hljs-number">1.2</span>,                                            useMeanshiftGrouping=<span class="hljs-literal">False</span>)    <span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> rects:        cv.rectangle(src, (x, y), (x + w, y + h), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)    cv.imshow(<span class="hljs-string">"hog-detector"</span>, src)    cv.imwrite(<span class="hljs-string">"hog-detector.jpg"</span>,src)    cv.waitKey(<span class="hljs-number">0</span>)    cv.destroyAllWindows()</code></pre><p>待检测图片：</p><p><img src="/article/760e3967/v2-2ccc671e60031942dca8a129410a0383_720w.jpg" srcset="/img/loading.gif" alt="v2-2ccc671e60031942dca8a129410a0383_720w"></p><p>检测图片(有点不完美，调参调不动了，先酱~)：</p><p><img src="/article/760e3967/hog-detector.jpg" srcset="/img/loading.gif" alt="hog-detector"></p><h3 id="可视化："><a href="#可视化：" class="headerlink" title="可视化："></a>可视化：</h3><p><strong>feature.log函数：</strong></p><ul><li><code>image</code>：可以是灰度图或者彩色图；</li><li><code>orientations</code>：就是把180度分成几份，也就是bin的数量；</li><li><code>pixels_per_Cell</code>：一个Cell里包含的像素个数；</li><li><code>Cells_per_block</code>：一个block包含的Cell个数；</li><li><code>visualize</code>：是否返回一个hog图像用于显示，下面会显示这张图；</li></ul><p>为了显示效果，把Cell的尺寸改为(16, 16)，对于每一个Cell，画出它归一化后的梯度直方图。如下图所示，我们可以很明显的看出一个人的轮廓。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> skimage <span class="hljs-keyword">import</span> feature, exposure<span class="hljs-keyword">import</span> cv2image = cv2.imread(<span class="hljs-string">'hog.jpg'</span>)fd, hog_image = feature.hog(image, orientations=<span class="hljs-number">9</span>, pixels_per_Cell=(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>),                    Cells_per_block=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), visualize=<span class="hljs-literal">True</span>)<span class="hljs-comment"># Rescale histogram for better display</span>hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>))cv2.imshow(<span class="hljs-string">'img'</span>, image)cv2.imshow(<span class="hljs-string">'hog'</span>, hog_image_rescaled)hog_image_rescaled = <span class="hljs-number">255.0</span> * hog_image_rescaledcv2.imwrite(<span class="hljs-string">'edge_hog.jpg'</span>, hog_image_rescaled)cv2.waitKey(<span class="hljs-number">0</span>)==ord(<span class="hljs-string">'q'</span>)</code></pre><p><img src="/article/760e3967/edge_hog.jpg" srcset="/img/loading.gif" alt="edge_hog"></p><h1 id="手动实现HOG特征"><a href="#手动实现HOG特征" class="headerlink" title="手动实现HOG特征"></a>手动实现HOG特征</h1><p>虽然opencv已经实现了HOG算法，但是手动实现的目的是为了加深我们对HOG的理解。参考了博客<a href="https://www.cnblogs.com/zyly/p/9651261.html" target="_blank" rel="noopener">基于传统图像处理的目标检测与识别</a><br>代码主要包括以下步骤：</p><ol><li>图像灰度化，归一化处理；</li><li>首先计算图像每一个像素点的梯度幅值和角度；</li><li>计算输入图像的每个Cell单元的梯度直方图(注意，我们在实现梯度直方图的时候，使用到的是双线性插值，这和上面介绍的理论略微有区别)，形成每个Cell的descriptor，比如输入图像为128×64 可以得到16×8个Cell，每个Cell由9个bin组成；</li><li>将2×2个Cell组成一个block，一个block内所有Cell的特征串联起来得到该block的HOG特征descriptor，并进行归一化处理，将图像内所有block的HOG特征descriptor串联起来得到该图像的HOG特征descriptor，这就是最终分类的特征向量；</li></ol><pre><code class="hljs python"><span class="hljs-comment">#代码来源GitHub:https://github.com/PENGZhaoqing/Hog-feature</span><span class="hljs-comment">#https://blog.csdn.net/ppp8300885/article/details/71078555</span><span class="hljs-comment">#https://www.leiphone.com/news/201708/ZKsGd2JRKr766wEd.html</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> math<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hog_descriptor</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">'''</span><span class="hljs-string">    HOG描述符的实现</span><span class="hljs-string">    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, img, Cell_size=<span class="hljs-number">8</span>, bin_size=<span class="hljs-number">9</span>)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        构造函数</span><span class="hljs-string">            默认参数，一个block由2x2个Cell组成，步长为1个Cell大小 </span><span class="hljs-string">        args:</span><span class="hljs-string">            img：输入图像(更准确的说是检测窗口)，这里要求为灰度图像  对于行人检测图像大小一般为128x64 即是输入图像上的一小块裁切区域</span><span class="hljs-string">            Cell_size：细胞单元的大小 如8，表示8x8个像素</span><span class="hljs-string">            bin_size：直方图的bin个数</span><span class="hljs-string">        '''</span>        self.img = img        <span class="hljs-string">'''</span><span class="hljs-string">        采用Gamma校正法对输入图像进行颜色空间的标准化（归一化），目的是调节图像的对比度，降低图像局部</span><span class="hljs-string">        的阴影和光照变化所造成的影响，同时可以抑制噪音。采用的gamma值为0.5。 f(I)=I^γ</span><span class="hljs-string">        '''</span>        self.img = np.sqrt(img*<span class="hljs-number">1.0</span> / float(np.max(img)))        self.img = self.img * <span class="hljs-number">255</span>        <span class="hljs-comment">#print('img',self.img.dtype)   #float64</span>        <span class="hljs-comment">#参数初始化</span>        self.Cell_size = Cell_size        self.bin_size = bin_size        self.angle_unit = <span class="hljs-number">180</span> / self.bin_size  <span class="hljs-comment">#这里采用180°</span>        <span class="hljs-keyword">assert</span> type(self.bin_size) == int, <span class="hljs-string">"bin_size should be integer,"</span>        <span class="hljs-keyword">assert</span> type(self.Cell_size) == int, <span class="hljs-string">"Cell_size should be integer,"</span>        <span class="hljs-keyword">assert</span> <span class="hljs-number">180</span> % self.bin_size == <span class="hljs-number">0</span>, <span class="hljs-string">"bin_size should be divisible by 180"</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        计算图像的HOG描述符，以及HOG-image特征图</span><span class="hljs-string">        '''</span>        height, width = self.img.shape        <span class="hljs-string">'''</span><span class="hljs-string">        1、计算图像每一个像素点的梯度幅值和角度</span><span class="hljs-string">        '''</span>        gradient_magnitude, gradient_angle = self.global_gradient()        gradient_magnitude = abs(gradient_magnitude)        <span class="hljs-string">'''</span><span class="hljs-string">        2、计算输入图像的每个Cell单元的梯度直方图，形成每个Cell的descriptor 比如输入图像为128x64 可以得到16x8个Cell，每个Cell由9个bin组成</span><span class="hljs-string">        '''</span>        Cell_gradient_vector = np.zeros((int(height / self.Cell_size), int(width / self.Cell_size), self.bin_size))        <span class="hljs-comment">#遍历每一行、每一列</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(Cell_gradient_vector.shape[<span class="hljs-number">0</span>]):            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(Cell_gradient_vector.shape[<span class="hljs-number">1</span>]):                <span class="hljs-comment">#计算第[i][j]个Cell的特征向量</span>                Cell_magnitude = gradient_magnitude[i * self.Cell_size:(i + <span class="hljs-number">1</span>) * self.Cell_size,                                 j * self.Cell_size:(j + <span class="hljs-number">1</span>) * self.Cell_size]                Cell_angle = gradient_angle[i * self.Cell_size:(i + <span class="hljs-number">1</span>) * self.Cell_size,                             j * self.Cell_size:(j + <span class="hljs-number">1</span>) * self.Cell_size]                Cell_gradient_vector[i][j] = self.Cell_gradient(Cell_magnitude, Cell_angle)        <span class="hljs-comment">#将得到的每个Cell的梯度方向直方图绘出，得到特征图</span>        hog_image = self.render_gradient(np.zeros([height, width]), Cell_gradient_vector)                <span class="hljs-string">'''</span><span class="hljs-string">        3、将2x2个Cell组成一个block，一个block内所有Cell的特征串联起来得到该block的HOG特征descriptor</span><span class="hljs-string">           将图像image内所有block的HOG特征descriptor串联起来得到该image（检测目标）的HOG特征descriptor，</span><span class="hljs-string">           这就是最终分类的特征向量</span><span class="hljs-string">        '''</span>        hog_vector = []        <span class="hljs-comment">#默认步长为一个Cell大小，一个block由2x2个Cell组成，遍历每一个block</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(Cell_gradient_vector.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>):            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(Cell_gradient_vector.shape[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>):                <span class="hljs-comment">#提取第[i][j]个block的特征向量</span>                block_vector = []                block_vector.extend(Cell_gradient_vector[i][j])                block_vector.extend(Cell_gradient_vector[i][j + <span class="hljs-number">1</span>])                block_vector.extend(Cell_gradient_vector[i + <span class="hljs-number">1</span>][j])                block_vector.extend(Cell_gradient_vector[i + <span class="hljs-number">1</span>][j + <span class="hljs-number">1</span>])                <span class="hljs-string">'''块内归一化梯度直方图，去除光照、阴影等变化，增加鲁棒性'''</span>                <span class="hljs-comment">#计算l2范数</span>                mag = <span class="hljs-keyword">lambda</span> vector: math.sqrt(sum(i ** <span class="hljs-number">2</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> vector))                   magnitude = mag(block_vector) + <span class="hljs-number">1e-5</span>                <span class="hljs-comment">#归一化</span>                <span class="hljs-keyword">if</span> magnitude != <span class="hljs-number">0</span>:                    normalize = <span class="hljs-keyword">lambda</span> block_vector, magnitude: [element / magnitude <span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> block_vector]                    block_vector = normalize(block_vector, magnitude)                hog_vector.append(block_vector)                   <span class="hljs-keyword">return</span> np.asarray(hog_vector), hog_image    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">global_gradient</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        分别计算图像沿x轴和y轴的梯度</span><span class="hljs-string">        '''</span>        gradient_values_x = cv2.Sobel(self.img, cv2.CV_64F, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, ksize=<span class="hljs-number">5</span>)        gradient_values_y = cv2.Sobel(self.img, cv2.CV_64F, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, ksize=<span class="hljs-number">5</span>)        <span class="hljs-comment">#计算梯度幅值 这个计算的是0.5*gradient_values_x + 0.5*gradient_values_y</span>        <span class="hljs-comment">#gradient_magnitude = cv2.addWeighted(gradient_values_x, 0.5, gradient_values_y, 0.5, 0)</span>        <span class="hljs-comment">#计算梯度方向</span>        <span class="hljs-comment">#gradient_angle = cv2.phase(gradient_values_x, gradient_values_y, angleInDegrees=True)</span>        gradient_magnitude, gradient_angle = cv2.cartToPolar(gradient_values_x,gradient_values_y,angleInDegrees=<span class="hljs-literal">True</span>)                <span class="hljs-comment">#角度大于180°的，减去180度</span>        gradient_angle[gradient_angle&gt;<span class="hljs-number">180.0</span>] -= <span class="hljs-number">180</span>         <span class="hljs-comment">#print('gradient',gradient_magnitude.shape,gradient_angle.shape,np.min(gradient_angle),np.max(gradient_angle))</span>        <span class="hljs-keyword">return</span> gradient_magnitude, gradient_angle    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Cell_gradient</span><span class="hljs-params">(self, Cell_magnitude, Cell_angle)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        为每个细胞单元构建梯度方向直方图</span><span class="hljs-string">        </span><span class="hljs-string">        args:</span><span class="hljs-string">            Cell_magnitude：Cell中每个像素点的梯度幅值</span><span class="hljs-string">            Cell_angle：Cell中每个像素点的梯度方向</span><span class="hljs-string">        return：</span><span class="hljs-string">            返回该Cell对应的梯度直方图，长度为bin_size</span><span class="hljs-string">        '''</span>        orientation_centers = [<span class="hljs-number">0</span>] * self.bin_size        <span class="hljs-comment">#遍历Cell中的每一个像素点</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(Cell_magnitude.shape[<span class="hljs-number">0</span>]):            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(Cell_magnitude.shape[<span class="hljs-number">1</span>]):                <span class="hljs-comment">#梯度幅值</span>                gradient_strength = Cell_magnitude[i][j]                <span class="hljs-comment">#梯度方向</span>                gradient_angle = Cell_angle[i][j]                <span class="hljs-comment">#双线性插值</span>                min_angle, max_angle, weight = self.get_closest_bins(gradient_angle)                orientation_centers[min_angle] += (gradient_strength * (<span class="hljs-number">1</span> - weight))                orientation_centers[max_angle] += (gradient_strength *weight)        <span class="hljs-keyword">return</span> orientation_centers    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_closest_bins</span><span class="hljs-params">(self, gradient_angle)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        计算梯度方向gradient_angle位于哪一个bin中，这里采用的计算方式为双线性插值</span><span class="hljs-string">        具体参考：https://www.leiphone.com/news/201708/ZKsGd2JRKr766wEd.html</span><span class="hljs-string">        例如：当我们把180°划分为9个bin的时候，分别对应对应0,20,40,...160这些角度。</span><span class="hljs-string">              角度是10，副值是4，因为角度10介于0-20度的中间(正好一半)，所以把幅值</span><span class="hljs-string">              一分为二地放到0和20两个bin里面去。</span><span class="hljs-string">        args:</span><span class="hljs-string">            gradient_angle:角度</span><span class="hljs-string">        return：</span><span class="hljs-string">            start,end,weight：起始bin索引，终止bin的索引，end索引对应bin所占权重</span><span class="hljs-string">        '''</span>        idx = int(gradient_angle / self.angle_unit)        mod = gradient_angle % self.angle_unit        <span class="hljs-keyword">return</span> idx % self.bin_size, (idx + <span class="hljs-number">1</span>) % self.bin_size, mod / self.angle_unit    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">render_gradient</span><span class="hljs-params">(self, image, Cell_gradient)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        将得到的每个Cell的梯度方向直方图绘出，得到特征图</span><span class="hljs-string">        args：</span><span class="hljs-string">            image：画布,和输入图像一样大 [h,w]</span><span class="hljs-string">            Cell_gradient：输入图像的每个Cell单元的梯度直方图,形状为[h/Cell_size,w/Cell_size,bin_size]</span><span class="hljs-string">        return：</span><span class="hljs-string">            image：特征图</span><span class="hljs-string">        '''</span>        Cell_width = self.Cell_size / <span class="hljs-number">2</span>        max_mag = np.array(Cell_gradient).max()        <span class="hljs-comment">#遍历每一个Cell</span>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(Cell_gradient.shape[<span class="hljs-number">0</span>]):            <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(Cell_gradient.shape[<span class="hljs-number">1</span>]):                <span class="hljs-comment">#获取第[i][j]个Cell的梯度直方图</span>                Cell_grad = Cell_gradient[x][y]                <span class="hljs-comment">#归一化</span>                Cell_grad /= max_mag                angle = <span class="hljs-number">0</span>                angle_gap = self.angle_unit                <span class="hljs-comment">#遍历每一个bin区间</span>                <span class="hljs-keyword">for</span> magnitude <span class="hljs-keyword">in</span> Cell_grad:                    <span class="hljs-comment">#转换为弧度</span>                    angle_radian = math.radians(angle)                    <span class="hljs-comment">#计算起始坐标和终点坐标，长度为幅值(归一化),幅值越大、绘制的线条越长、越亮</span>                    x1 = int(x * self.Cell_size + Cell_width + magnitude * Cell_width * math.cos(angle_radian))                    y1 = int(y * self.Cell_size + Cell_width + magnitude * Cell_width * math.sin(angle_radian))                    x2 = int(x * self.Cell_size + Cell_width - magnitude * Cell_width * math.cos(angle_radian))                    y2 = int(y * self.Cell_size + Cell_width - magnitude * Cell_width * math.sin(angle_radian))                    cv2.line(image, (y1, x1), (y2, x2), int(<span class="hljs-number">255</span> * math.sqrt(magnitude)))                    angle += angle_gap        <span class="hljs-keyword">return</span> image        <span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    <span class="hljs-comment">#加载图像</span>    img = cv2.imread(<span class="hljs-string">'./image/person.jpg'</span>)            width = <span class="hljs-number">64</span>    height = <span class="hljs-number">128</span>    img_copy = img[<span class="hljs-number">320</span>:<span class="hljs-number">320</span>+height,<span class="hljs-number">570</span>:<span class="hljs-number">570</span>+width][:,:,::<span class="hljs-number">-1</span>]        gray_copy = cv2.cvtColor(img_copy,cv2.COLOR_BGR2GRAY)        <span class="hljs-comment">#显示原图像</span>    plt.figure(figsize=(<span class="hljs-number">6.4</span>,<span class="hljs-number">2.0</span>*<span class="hljs-number">3.2</span>))    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)    plt.imshow(img_copy)        <span class="hljs-comment">#HOG特征提取</span>    hog = Hog_descriptor(gray_copy, Cell_size=<span class="hljs-number">8</span>, bin_size=<span class="hljs-number">9</span>)        hog_vector, hog_image = hog.extract()    print(<span class="hljs-string">'hog_vector'</span>,hog_vector.shape)    print(<span class="hljs-string">'hog_image'</span>,hog_image.shape)        <span class="hljs-comment">#绘制特征图</span>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)    plt.imshow(hog_image, cmap=plt.cm.gray)        plt.show()</code></pre><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>HOG算法具有以下优点：                            </p><ul><li>核心思想是所检测的局部物体外形能够被梯度或边缘方向的分布所描述，HOG能较好地捕捉局部形状信息，对几何和光学变化都有很好的不变性；             </li><li>HOG是在密集采样的图像块中求取的，在计算得到的HOG特征向量中隐含了该块与检测窗口之间的空间位置关系。                           </li></ul><p>HOG算法具有以下缺点：                 </p><ul><li>特征描述子获取过程复杂，维数较高，导致实时性差；                </li><li>很难处理遮挡问题，人体姿势动作幅度过大或物体方向改变也不易检测（这个问题后来在<a href="http://blog.csdn.net/masibuaa/article/details/17924671" target="_blank" rel="noopener">DPM</a>中采用可变形部件模型的方法得到了改善）；                  </li><li>跟SIFT相比，HOG没有选取主方向，也没有旋转梯度方向直方图，因而本身不具有旋转不变性（较大的方向变化），其旋转不变性是通过采用不同旋转方向的训练样本来实现的；</li><li>跟SIFT相比，HOG本身不具有尺度不变性，其尺度不变性是通过缩放检测窗口图像的大小来实现的；</li><li>此外，由于梯度的性质，HOG对噪点相当敏感，在实际应用中，在block和Cell划分之后，对于得到各个区域，有时候还会做一次高斯平滑去除噪点。               </li></ul><p><em>Q : How do you eat an elephant ?</em><br><em>A : One bite at a time!</em></p><p>参考链接：</p><p><a href="https://www.learnopencv.com/histogram-of-oriented-gradients/" target="_blank" rel="noopener">Histogram of Oriented Gradients(强烈推荐)</a><br><a href="https://cloud.tencent.com/developer/article/1419615" target="_blank" rel="noopener">HOG特征详解与行人检测</a><br><a href="https://www.cnblogs.com/zyly/p/9651261.html" target="_blank" rel="noopener">基于传统图像处理的目标检测与识别(很全面)</a><br><a href="https://zhuanlan.zhihu.com/p/85829145" target="_blank" rel="noopener">一文讲解方向梯度直方图HOG</a></p>]]></content>
    
    
    <categories>
      
      <category>图像处理下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>leetcode50</title>
    <link href="/article/553b1469.html"/>
    <url>/article/553b1469.html</url>
    
    <content type="html"><![CDATA[<h1 id="实现power-x-n-，即计算x的n次幂函数。"><a href="#实现power-x-n-，即计算x的n次幂函数。" class="headerlink" title="实现power(x,n)，即计算x的n次幂函数。"></a>实现power(x,n)，即计算x的n次幂函数。</h1><p>示例1：</p><pre><code class="hljs python">输入: <span class="hljs-number">2.00000</span>, <span class="hljs-number">10</span>输出: <span class="hljs-number">1024.00000</span></code></pre><p>示例2：</p><pre><code class="hljs python">输入: <span class="hljs-number">2.10000</span>, <span class="hljs-number">3</span>输出: <span class="hljs-number">9.26100</span></code></pre><p>示例3：</p><pre><code class="hljs python">输入: <span class="hljs-number">2.00000</span>, <span class="hljs-number">-2</span>输出: <span class="hljs-number">0.25000</span>解释: <span class="hljs-number">2</span><span class="hljs-number">-2</span> = <span class="hljs-number">1</span>/<span class="hljs-number">22</span> = <span class="hljs-number">1</span>/<span class="hljs-number">4</span> = <span class="hljs-number">0.25</span></code></pre><p>说明：</p><ul><li>-100.0 &lt; x &lt; 100.0</li><li>$n$是32位有符号整数，其数值范围是$[-2^{31},2^{31}-1]$。</li></ul><p><strong>快速幂算法</strong>：递归和迭代两个版本。<br>当指数$n$为负数时，可以通过计算$x^{-n}$取倒数得到结果，因此只需要考虑$n$为自然数的情况。</p><h2 id="方法一：快速幂-递归-从右到左"><a href="#方法一：快速幂-递归-从右到左" class="headerlink" title="方法一：快速幂+递归(从右到左)"></a>方法一：快速幂+递归(从右到左)</h2><p>快速幂算法的本质是分治算法。<br>例子：</p><script type="math/tex; mode=display">x \rightarrow x^2 \rightarrow x^4 \rightarrow x^8 \rightarrow x^{16} \rightarrow x^{32} \rightarrow x^{64}</script><script type="math/tex; mode=display">x \rightarrow x^2 \rightarrow x^4 \rightarrow x^9 \rightarrow x^{19} \rightarrow x^{38} \rightarrow x^{77}</script><p>第一个直接把上一次的结果平方，第二个把上一个结果平方后，额外乘一个$x$。</p><ul><li><p>计算$x^n$时，递归计算出$y=x^{\lfloor n/2 \rfloor}$</p></li><li><p>根据递归计算的结果，如果$n$为偶数，那么$x^n=y^2$；如果$n$为奇数，那么$x^n=y^2*x$；</p></li><li>递归的边界为$n=0$，任意数的0次方均为1。</li></ul><p>由于每次递归都会使得指数减少一半，因此递归的层数为$O(logn)$，算法可以在很快的时间内得到结果。</p><p>复杂度分析：</p><ul><li>时间复杂度：$O(logn)$，即为递归的层数。</li><li>空间复杂度：$O(logn)$，即为递归的层数，这是由于递归的函数调用会使用栈空间。</li></ul><p>上代码，目前只会python（猛男落泪，等以后会了别的再加上）：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myPow</span><span class="hljs-params">(self, x, n)</span>:</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">quickMul</span><span class="hljs-params">(N)</span>:</span>            <span class="hljs-keyword">if</span> N == <span class="hljs-number">0</span>:                <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>            y = quickMul(N // <span class="hljs-number">2</span>)            <span class="hljs-keyword">return</span> y * y <span class="hljs-keyword">if</span> N % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> y * y * x        <span class="hljs-keyword">return</span> quickMul(n) <span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1.0</span> / quickMul(-n)</code></pre><h2 id="方法二：快速幂-迭代-从左到右"><a href="#方法二：快速幂-迭代-从左到右" class="headerlink" title="方法二：快速幂 + 迭代(从左到右)"></a>方法二：快速幂 + 迭代(从左到右)</h2><p>递归需要使用额外的栈空间。递归-&gt;迭代。<br>如果整数$n$的二进制拆分为 $n = 2^{i_0} + 2^{i_1} + \ldots + 2^{i_k}$<br>借助整数的二进制拆分，得到迭代计算的方法，一般地，如果整数$n$的二进制拆分为：</p><script type="math/tex; mode=display">n = 2^{i_0} + 2^{i_1} + \ldots + 2^{i_k}</script><p>那么</p><script type="math/tex; mode=display">x^n = x^{2^{i_0}} * x^{2^{i_1}} * \ldots * x^{2^{i_k}}</script><p>从$x$不断地进行平方，得到$x^2,x^4,x^8,x^{16},\ldots,$如果$n$的第$k$个（从右往左，从0开始计数）二进制位为1，那么我们就将对应的贡献$x^{2^k}$计入答案。<br>上代码：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">myPow</span><span class="hljs-params">(self, x, n)</span>:</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">quickMul</span><span class="hljs-params">(N)</span>:</span>            ans = <span class="hljs-number">1.0</span>            <span class="hljs-comment"># 贡献的初始值为x</span>            x_contribute = x            <span class="hljs-comment"># 在对N进行二进制拆分的同时计算答案</span>            <span class="hljs-keyword">while</span> N &gt; <span class="hljs-number">0</span>:                <span class="hljs-keyword">if</span> N % <span class="hljs-number">2</span> == <span class="hljs-number">1</span>:                    <span class="hljs-comment"># 如果N二进制表示的最低位为1，那么需要计入贡献</span>                    ans *= x_contribute                <span class="hljs-comment"># 将贡献不断地平方</span>                x_contribute *= x_contribute                <span class="hljs-comment"># 舍弃N二进制表示的最低位，这样每次只要判断最低位即可</span>                N // = <span class="hljs-number">2</span>            <span class="hljs-keyword">return</span> ans        <span class="hljs-keyword">return</span> quickMul(n) <span class="hljs-keyword">if</span> n &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1.0</span> / quickMul(-n)</code></pre><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://leetcode-cn.com/problems/powx-n/solution/50-powx-n-kuai-su-mi-qing-xi-tu-jie-by-jyd/" target="_blank" rel="noopener">优秀题解(推荐)</a>看了这个大佬的几个题解了，高手，思路很清晰，个人账号里面有一系列套题，剑指Offer之类的，非常具有参考价值。</p><p><font color="blue">第一次写，不是很流畅，思路和手都不太行。下周再做一遍，做完就把这行删掉，冲鸭</font><br>如此一来，每天水一篇，居然成为了日更博主，哈哈哈哈哈<br>不足的博文在后期有时间都会进行修改，做一个有质量(没人气)的博主，奥里给O(∩_∩)O。</p>]]></content>
    
    
    <categories>
      
      <category>leetcode</category>
      
      <category>分治、回溯</category>
      
    </categories>
    
    
    <tags>
      
      <tag>leetcode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LSTM_Elmo</title>
    <link href="/article/3b406ffc.html"/>
    <url>/article/3b406ffc.html</url>
    
    <content type="html"><![CDATA[<p>一、词的表示<br>one-hot编码：稀疏，向量之间均是正交关系，不能表示语义。<br>word class：将相同属性的词归为一类，但分类标准过意单一、片面。如cat、dag、bird都属于动物，但是dog、cat属于爬行动物，而bird属于飞行动物，难以区分。<br>word embedding：使用词向量表示词特征，相似的词、向量接近，在空间较近。但无法解决一词多义问题，即每个word只有一个词向量。<br>contextualized word embedding：同一个词在不同的上下文中有不同的表示。</p><p>二、LSTM概述<br>  长短记忆神经网络——通常称作LSTM，是一种特殊的RNN，能够学习长的依赖关系。是为了避免长依赖问题而精心设计的。记住较长的历史信息实际上是他们的默认行为，而不是他们努力学习的东西。<br>1<br>2.1 标准RNN结构如下(单个tanh层)</p><p>2.2 lstm 结构图如下</p><p>2.3 lstm的4层交互结构说明:<br>LSTM 的4大交互模块    结构片段    说明<br>遗忘门<br>遗忘门：由sigmoid决定丢弃那些信息，输出01序列，序列作用在单元格Ct-1上，1表示全保留，0全丢弃.<br>输入门<br>分为2步：①输入门：sigmoid决定更新哪些值<br>② tanh：创建候选向量Ct<br>旧值更新操作<br>将Ct-1更新成Ct, 对应实际删除旧主题信息，并添加新信息<br>输出门<br>决定输出什么，sigmoid决定输出哪些部分。然后tanh+输出哪些部分 得到输出.<br>三、ELMO模型概述<br>基于RNN的语言模型，直接从句子学习，无需标注。<br>1<br>3.1 问题引入<br>给定句子：“潮水 退了 就 知道 谁 没穿 裤子”</p><p>问题①：红框ELMO内部究竟是啥玩意?<br>问题②：是直接将词输入到ELMO模型吗？<br>问题③：ELMO输出的h1、h2、蓝柱是什么?<br>3.2 ELMO的内部构造图<br>elmo模型是一个多层(2层)双向的LSTM组成的LM(底层lstm捕获到句法、语义方面信息，如POS，高层捕获到词义的上下文信息，如语义消歧)<br>1</p><p>3.3 如何得到输入词的词向量<br>词向量的构建是动态的过程，会随着模型参数更新而更新。对于中文，原始输入的是词语；而对于类似英文，每个单词又是由多个字母构成，中文的词语等同于英文中的单词，词语中的词等同于单词中的字母(同一粒度)，本文中是针对英文而言的，中文类似</p><p>即input→词向量表示：对每个词进行字符卷积操作，从而得到词向量。(w2c系列是通过look_up_table得到词向量)</p><p>格式(batch, n_token, max_char, char_dim)，batch表示同时有多少个句子(样本)并行卷积，n_token表示句子在未去重条件下的单词数，char_dim表示每个字符的特征长度，max_char表示句子中词的最大字符数量。<br>使用大小”高为1，宽为n_width, 通道数为char_dim”(即维度[1, n_width, char_dim])的卷积核进行卷积, 得到[n_token, max_char-n_width+1]的特征图(备注：词字符少的感觉有个填充对齐操作以达到max_char大小)<br>最终再对图的每行进行一个max_pooling操作，得到batch个[n_token]。如输入(8, 16, 12, 64)，卷积核(1, 5, 64)， 即并行计算8个句子样本，样本最多有16单词，词中最大的字符数量是12个，每个字符用64个特征表示，最终获取8个(16, 12-5+1)特征图。<br>即为词向量<br>词向量卷积过程示例<br>示例：假设batch为1，句子 “I like China very much” , 如何卷积的?<br>即初始维度：(1, n_token, max_char, char_dim)，卷积核为(1, 2, char_dim), 卷积过程如下：</p><p>3.4 loss函数以及最终输出<br>3.4.1 loss函数<br>    训练是使用负对数似然作为损失函数。<br>1</p><p>3.4.2 输出向量表示<br>elmo在train过程中，每个词都由 正反向LSTM中间向量、正反向结果向量和静态向量(init时向量，也会同步更细)表示，故每个词都会有2L+1个表示向量：</p><p>3.4.3 最终结果计算<br>    ELMO考虑既考虑最后一层lstm的输出，同时兼顾静态词向量、中间词向量, 最终的结果与w2c类似，是预测位置上的可能概率分布。故输出结果表示如下：<br>1</p><p>3.5 ELMO事项说明<br>在实际任务中使用ELMO向量：<br> 结合监督任务训练时，可冻结(如设置权重标为0不更新该参数)EMLO模型中LSTM层的参数(静态词向量、正反向lstm中的参数[指的是权重，不是api的参数项])，只训练最后一层E函数中的权重值。此时可加入dropout(随机丢掉某些层的影响)或L2正则化(使E结果更接近lstm层输出的平均值)。<br> 结合方式有：</p><ol><li>直接在输入层的词向量拼上elmo表示</li><li>将elmo直接作用在RNN的输出上<br>改进：</li><li>ELMO对输出结果采用拼接方式融合特征，‘可能’ 弱于Bert一体化的融合方式</li><li>ELMO使用了LSTM作为特征抽取器，而非Transformer(研究表明Transformer特征提取能力远强于LSTM)</li><li>由RNN导致训练时间</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Task03-Haar特征描述算子-人脸监测</title>
    <link href="/article/ad28cab2.html"/>
    <url>/article/ad28cab2.html</url>
    
    <content type="html"><![CDATA[<h1 id="3-1-算法由来"><a href="#3-1-算法由来" class="headerlink" title="3.1 算法由来"></a>3.1 算法由来</h1><p>&emsp;&emsp;Haar-like特征最早是由Papageorgiou等应用于人脸表示，2001年，Viola和Jones两位大牛发表了今典的《Rapid Object Detection using a Boosted Cascade of Simple Features》和《Robust Real-Time Face Detection》，在AdaBoost算法基础上，使用Haar-like小波特征和积分图方法进行人脸监测，并对AdaBoost训练出的强分类器进行级联。<br>&emsp;&emsp;这两个大咖不是最早提出使用小波特征的，单他们设计了针对人脸监测更有效的特征，可以说是人脸检测史上里程碑式的一笔了，当时这个算法被称为Viola-Jones检测器。又过了一段时间，Rainer Liehart和Jochen Maydt两位大咖把这个检测器进行扩展，最终形成了OpenCV现在的Haar分类器。</p><p>&emsp;&emsp;AdaBoost是Freund和Schapire在1995年提出的算法，是对传统Boosting算法的一大提升。Boosting算法的核心思想，是将弱学习方法提升成强学习算法，也就是“三个臭皮匠顶一个诸葛亮”。<a id="more"></a></p><font color="blue">Haar分类器 = Haar-like特征 + 积分图方法 + AdaBoost + 级联</font><p>Haar分类器算法的要点如下：</p><ol><li>使用Haar-like特征特征做监测</li><li>使用积分图(Integral Image)对Haar-like特征求值进行加速。</li><li>使用AdaBoost算法训练区分人脸和非人脸的强分类器。</li><li>使用筛选式级联把强分类器级联到一起，提高准确率。</li></ol><h1 id="3-2-算法理解"><a href="#3-2-算法理解" class="headerlink" title="3.2 算法理解"></a>3.2 算法理解</h1><h2 id="一、Haar-like特征"><a href="#一、Haar-like特征" class="headerlink" title="一、Haar-like特征"></a>一、Haar-like特征</h2><p>viola牛们提出的Haar-like特征：</p><p><img src="/article/ad28cab2/1346291481_8797.jpg" srcset="/img/loading.gif" alt="img"></p><p>Lienhart等牛们提出的Haar-like特征：</p><p><img src="/article/ad28cab2/图片4.png" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;Haar特征分为四类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形。<font color="blue">并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和</font>。Haar特征值反映了图形的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，单矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向(水平、垂直、对角)的结构。</p><p><img src="/article/ad28cab2/1328274-20180802215641326-115308739.png" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;图中的A, B和D这类特征，特征数值计算公式为：v=Σ白-Σ黑。图C中，计算公式：v=Σ白-2*Σ黑。将黑色区域像素和乘以2，是为了使两种矩形区域中像素数目一致。我们希望当把矩形放到人脸区域计算出来的特征值和放到非人脸区域计算出来的特征值差别越大越好，这样就可以用来区分人脸和非人脸。<br>&emsp;&emsp;OpenCV(2.4.11版本)所使用的共计14种Haar特征，包括5种Basic特征、3种Core特征和6种Title(即$45^\circ$旋转)特征。mode为BASIC,使用前五个。mode为CORE，使用前8个。mode为ALL，使用全部14种。</p><p><img src="/article/ad28cab2/20180527174145244" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;通过改变特征模板的大小和位置，可在图像子窗口中穷举出大量的特征。上图的特征模板称为“特征原型”；特征原型在图像子窗口中扩展（平移伸缩）得到的特征称为“矩形特征”；矩形特征的值称为“特征值”。</p><p><img src="/article/ad28cab2/1328274-20180815213149483-1209887019.png" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;上图中两个矩形特征，表示出人脸的某些特征。比如中间一幅表示眼睛区域的颜色比脸颊区域的颜色深，右边一幅表示鼻梁两侧比鼻梁的颜色要深。同样，其他目标，如眼睛等，也可以用一些矩形特征来表示。<font color="blue">使用矩形特征比单纯地使用像素点具有很大的优越性，并且速度更快。</font><br>&emsp;&emsp;矩形特征可位于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征，如：在24*24像素大小的检测窗口内矩形特征数量可以达到16万个。这样就有两个问题需要解决了：</p><p>（1）如何快速计算那么多的特征？—-积分图大显神通；<br>（2）哪些矩形特征才是对分类器分类最有效的？—-如通过AdaBoost算法来训练。</p><h2 id="二、Haar-like特征计算-积分图"><a href="#二、Haar-like特征计算-积分图" class="headerlink" title="二、Haar-like特征计算-积分图"></a>二、Haar-like特征计算-积分图</h2><p>&emsp;&emsp;<font color="blue">积分图是只遍历一次图像就可求出图像中所有区域像素和的快速算法</font>，大大提高了图像特征值计算的效率。<br>&emsp;&emsp;积分图的主要思想是将图像从起点开始到各个点形成的矩形区域像素之和作为一个数组的元素提前保存在数组中。当要计算某个区域的像素和时，直接索引数组的元素进行线性计算即可，从而加快了计算速度。<br>&emsp;&emsp;积分图是一种能够描述全局信息的举证表示方法。积分图的构造方式是位置位置（𝑖,𝑗）处的值𝑖𝑖(𝑖,𝑗)是原图像(𝑖,𝑗)左上角方向所有像素𝑓(𝑘,𝑙)的和：</p><script type="math/tex; mode=display">𝑖𝑖(𝑖,𝑗)=\sum_{𝑘≤𝑖,𝑙≤𝑗}{𝑓(𝑘,𝑙)}</script><p>积分图构建算法：<br>1、用𝑠(𝑖,𝑗)表示行方向的累加和，初始化𝑠(𝑖,−1)=0；<br>2、使用𝑖𝑖(𝑖,𝑗)表示一个积分图像，初始化𝑖𝑖(−1,𝑖)=0；<br>3、逐行扫描图像，递归计算每个像素(𝑖,𝑗)行方向的累加和𝑠(𝑖,𝑗)和积分图像𝑖𝑖(𝑖,𝑗)的值：</p><script type="math/tex; mode=display">𝑠(𝑖,𝑗)=𝑠(𝑖,𝑗−1)+𝑓(𝑖,𝑗) \\𝑖𝑖(𝑖,𝑗)=𝑖𝑖(𝑖−1,𝑗)+𝑠(𝑖,𝑗)</script><p>4、扫描图像一遍，当到达图像右下角像素时，积分图像𝑖𝑖就构建好了。<br>积分图构造好之后，图像中任何矩阵区域像素累加和都可以通过简单运算得到如图所示：</p><p><img src="/article/ad28cab2/1328274-20180815205201934-1228753116.png" srcset="/img/loading.gif" alt="img"></p><p>设D的四个顶点分别为$\alpha,\beta,\gamma,\delta$，则D的像素和可以表示为：</p><script type="math/tex; mode=display">D_{sum} = ii(\alpha)+ii(\beta)-(ii(\gamma)+ii(\delta))</script><p>Haar-like特征值是两个矩阵像素和的差，同样可以在常数时间内完成。</p><h2 id="三、计算Haar特征值"><a href="#三、计算Haar特征值" class="headerlink" title="三、计算Haar特征值"></a>三、计算Haar特征值</h2><p>&emsp;&emsp;由二已知，一个区域的像素值的和，可以由该区域的端点的积分图计算。由前面特征模板的特征值的定义可推出，<font color="blue">矩形特征的特征值由特征端点的积分图计算得到</font>。以A矩形特征为例，如下图，使用积分图计算该特征值：</p><p><img src="/article/ad28cab2/1328274-20180816090314814-408684664.png" srcset="/img/loading.gif" alt="img"></p><p>区域A的像素值：$𝑖𝑖(5)+𝑖𝑖(1)−𝑖𝑖(2)−𝑖𝑖(4)$</p><p>区域B的像素值：$𝑖𝑖(6)+𝑖𝑖(2)−𝑖𝑖(5)−𝑖𝑖(3)$</p><p>所以：该矩形特征的特征值为：</p><script type="math/tex; mode=display">𝑖𝑖(5)+𝑖𝑖(1)−𝑖𝑖(2)−𝑖𝑖(4)−[𝑖𝑖(6)+𝑖𝑖(2)−𝑖𝑖(5)−𝑖𝑖(3)]</script><script type="math/tex; mode=display">=[𝑖𝑖(5)−𝑖𝑖(4)]+[𝑖𝑖(3)−𝑖𝑖(2)]−[𝑖𝑖(2)−𝑖𝑖(1)]−[𝑖𝑖(6)−𝑖𝑖(5)]</script><p>&emsp;&emsp;所以，矩形特征的特征值，只与特征矩形的端点的积分图有关。利用矩形特征的端点的积分图，再进行简单的加减运算，即可得到特征值，如此一来，特征的计算速度大大提高，也提高了目标的检测速度。<br>了解特征值计算后，接下来展示不同特征值的含义。</p><p>&emsp;&emsp;下图展示了20X20子窗口里面全部78,460个矩形特征对在全部2,706个人脸样本和4,381个非人脸样本的特征值平均值的分布图。由分布看出，特征的绝大部分的特征值平均值都是分布在0前后的范围内。<br>&emsp;&emsp;出乎意料的是，人脸样本与非人脸样本的分布曲线差别并不大，不过注意到特征值大于或小于某个值后，分布曲线出现了一致性的差别。说明了绝大部分特征对于识别人脸和非人脸的能力是很微小的，但是存在一些特征及相应的阈值，可以有效第区别人脸样本和非人脸样本。<img src="/article/ad28cab2/image-20200704225306093.png" srcset="/img/loading.gif" alt="image-20200704225306093"></p><p>&emsp;&emsp;为了更好地说明部分特征对于人脸和非人脸的区分作用，从78,460个矩形特征中随机抽取了两个特征A和B，这两个特征遍历2,706 个人脸样本和4,381 个非人脸样本，计算了每张图像对应的特征值，最后将特征值进行了从小到大的排序，并按照这个新的顺序表绘制了分布图如下所示：</p><center class="half">    <img src="/article/ad28cab2/20160921003959958" srcset="/img/loading.gif" width="350"><img src="/article/ad28cab2/20160921004018287" srcset="/img/loading.gif" width="350"></center><p>&emsp;&emsp;可以看出，矩形特征A在人脸样本和非人脸样本中的特征值分布相似，所以区分人脸和非人脸的能力很差。下面看矩形特征B在人脸样本和非人脸样本中特征值的分布：</p><center class="half">    <img src="/article/ad28cab2/20160921004748821" srcset="/img/loading.gif" width="350"><img src="/article/ad28cab2/20160921004808352" srcset="/img/loading.gif" width="350"></center><p>&emsp;&emsp;可以看出，矩形特征B的特征值分布，尤其是0点的位置，在人脸样本和非人脸样本中差别比较大，所以可以更好地实现对人脸分类。<br>&emsp;&emsp;特征 A 和特征 B 的表现大相径庭。<br>&emsp;&emsp;特征 A 对人脸和非人脸样本的特征值为0的点几乎处于相同位置（46.5%，51.5%），且都在所有特征的中间范围 (-5%)。这说明矩形特征 A对于人脸和非人脸几乎没有分辨能力。<br>&emsp;&emsp;特征 B 对非人脸样本的分布，符合我们的预想，特征值为 0的点处于所有特征的中间范围(59.4%)，这说明特征B也“ 看不到” 非人脸的特点。但是对于人脸样本，特征 B 表现了很一致的倾向性，93.4%的特征在 0 点的一侧，与非人脸样本的相差 34%。这说明特征 B 能够相当可靠地分辨人脸和非人脸。<br>&emsp;&emsp;上述的分析说明，确实存在优势的矩形特征，能够在一定的置信范围内区分人脸和非人脸。由于是使用统计的方法计算人脸图像和非人脸图像的差别，因此最后得到的区分阈值，是在某个概率范围内准确地进行区分。<br>上述总结如下：<br>（1）在检测窗口通过平移+缩放可以产生一系列Haar特征，这些特征由于位置和大小不同，分类效果也不同；<br>（2）通过计算Haar特征的特征值，可以有将图像矩阵映射为1维特征值，有效实现了降维。</p><h3 id="Haar特征值归一化-也可以采取标准归一化"><a href="#Haar特征值归一化-也可以采取标准归一化" class="headerlink" title="Haar特征值归一化(也可以采取标准归一化)"></a>Haar特征值归一化(也可以采取标准归一化)</h3><p>&emsp;&emsp;从上图中可发现，仅仅在正样本组中12*8大小的Haar特征计算出的特征值变化范围就达到了-2000~+6000，跨度非常大。这种跨度大的特征不利于量化评定特征值，这时候归一化就排上用场了。<font color="blue">对特征值进行归一化，压缩特征值范围。</font><br>&emsp;&emsp;假设当前检测窗口中的图像像素为$i(x,y)$，当前检测窗口为$w\times h$大小(例如上图中20x20大小)。<br>OpenCV采用如下方式归一化：<br>1、计算检测窗口中图像的灰度值和灰度值平方和：<font color="red">(这里的$i^2$怎么用积分图表示？？)</font></p><script type="math/tex; mode=display">sum=\sum{i(x,y)} \\sq_{sum}=\sum{i^2(x,y)}</script><p>2、计算平均值：</p><script type="math/tex; mode=display">mean = \frac{sum}{w*h}\\sq_{mean}=\frac{sq_{sum}}{w*h}</script><p>3、计算归一化因子：</p><script type="math/tex; mode=display">varNormFactor=\sqrt{sq_{mean}-mean^2}</script><p>4、归一化特征值：</p><script type="math/tex; mode=display">normValue=\frac{featureValue}{varNormFactor}</script><p>之后使用归一化的特征值𝑛𝑜𝑟𝑚𝑉𝑎𝑙𝑢𝑒与阈值对比。</p><h2 id="四、Adaboost级联分类器"><a href="#四、Adaboost级联分类器" class="headerlink" title="四、Adaboost级联分类器"></a>四、Adaboost级联分类器</h2><p>OpenCV中的Adaboost级联分类器的结构，弱分类器-&gt;强分类器-&gt;级联分类器；</p><h3 id="1-级联分类器"><a href="#1-级联分类器" class="headerlink" title="1.级联分类器"></a>1.级联分类器</h3><p>Adaboost级联分类器：<br>&emsp;&emsp;级联分类模型是树状结构，每一层—强分类器是树状结构，强分类器中的每一个弱分类器也是树状结构。如下图所示，每一个stage都代表一级强分类器：<br><img src="/article/ad28cab2/1328274-20180816100358784-868305710.png" srcset="/img/loading.gif" alt="img"><br>&emsp;&emsp;当检测窗口通过所有的强分类器时被认为是正样本。每一个强分类器对负样本的判别准确率高，任一级强分类器检测为负样本，就不再继续调用下面的强分类器，直接丢弃，减少了很多的检测时间。<br>&emsp;&emsp;一副图像中待检测的区域中很多是负样本，级联分类器在初期就会抛弃很多负样本的复杂检测，所以级联分类器的速度是非常快的，只有正样本才会送到下一个强分类器再次进行检验，这样就保证了最后输出的正样本的伪正(false positive)的可能性非常低。</p><h3 id="2-级联分类器的训练"><a href="#2-级联分类器的训练" class="headerlink" title="2.级联分类器的训练"></a>2.级联分类器的训练</h3><p>1)首先需要训练出m个弱分类器，然后把m个弱分类器按照一定的组合策略，得到一个强分类器，如下图所示：<br>2)重复1步骤n次得到n个强分类器，按照级联的方式组合起来，得到最终的Haar分类器。<br><img src="/article/ad28cab2/1328274-20180816150118683-1847072909.png" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;一个弱分类器是和上图类似的决策树，最基本的弱分类器只包含一个Haar-like特征，即只包含一层决策树(树桩stump)。<br>&emsp;&emsp;以20*20图像为例，78,460个特征，若直接利用AdaBoost训练，78,460个弱分类器组合成强分类器，工作量是极其巨大的。所以必须要有个筛选过程。</p><p><font color="blue">首先筛选出T个优秀的最优弱分类器(特征值)，然后把T个最优弱分类器传给AdaBoost进行训练。</font><br><strong>训练最优弱分类器</strong><br>&emsp;&emsp;人脸样本2000张，非人脸样本4000张，这些样本都经过了归一化，大小都是20X20的图像。那么，对于78,460中的任一特征$f_i$，我们计算该特征在这2000人脸样本，4000非人脸样本上的值，得到6000个特征值。将这些特征值排序，然后选取一个最佳的特征值。在该特征值下，对于特征$f_i$而言，样本的加权错误率最低。<br>&emsp;&emsp;在确定了训练子窗口(20x20的图像)的矩形特征数量(78,460)和特征值后，需要对每一个特征$f$，训练一个弱分类器$h(x,f,\rho,\theta)$：</p><script type="math/tex; mode=display">h(x,f,\rho,\theta)=\begin{cases}1,   {\rho f(x)<\rho \theta}\\0,  {other}\end{cases}</script><p>&emsp;&emsp;其中$𝑥$代表一个检测子窗口，$𝑓$为矩形特征，$\theta$为阈值，$\rho$指示不等号的方向。对每个特征$𝑓$，训练一个弱分类器$ℎ(𝑥,𝑓,\rho,\theta)$，就是确定$𝑓$的最优阈值，使得这个弱分类器对所有的训练样本分类误差最小。<br>这里的最优不是指强分类器，只是一个误差相对较低的弱分类器。<br><strong>弱分类器训练的具体步骤：</strong><br>1、对于每个特征$f$，计算所有训练样本的特征值，并将其排序：<br>2、扫描一遍排好序的特征值，对排好序的表中的每个元素，计算下面四个值：<br>计算全部正例的权重和$𝑇^+$；<br>计算全部负例的权重和$T^-$；<br>计算该元素前之前的正例的权重和$𝑆^+$；<br>计算该元素前之前的负例的权重和$𝑆^−$；</p><p><img src="/article/ad28cab2/20160921003914442" srcset="/img/loading.gif" alt="这里写图片描述"></p><p>3、选取当前元素的特征值$F_{k,j}$和它前面的一个特征值$F_{k,j-1}$之间的数作为阈值，所得到的弱分类器就在当前元素处把样本分开 —— 也就是说这个阈值对应的弱分类器将当前元素前的所有元素分为非人脸（或人脸），而把当前元素后（含）的所有元素分为人脸（或非人脸）。该阈值的分类误差为：</p><script type="math/tex; mode=display">e=\min{(S^+ + (T^- - S^-), S^- + ( T^+ - S^+))}</script><p><strong>公式说明：前一个分类误差计算前提为阈值前为非人脸，后一个分类误差计算前提为阈值前为人脸。</strong><br>&emsp;&emsp;把排序表从头到尾扫描一遍就可以为弱分类器选择使分类误差最小的阈值（最优阈值），即选取了一个最优弱分类器。在本例中，一共有78,460个特征，因此会得到78,460个最优分类器。<br>结合三，可得出以下分析：<br>&emsp;&emsp;阈值$\theta$的含义就清晰可见了。方向指示符$p$ 用以改变不等号的方向。一个弱学习器（一个特征）的要求仅仅是：它能够以稍低于50%的错误率来区分人脸和非人脸图像，因此上面提到只能在某个概率范围内准确地进行区分就已经完全足够。按照这个要求，可以把所有错误率低于50% 的矩形特征都找到（适当地选择阈值，对于固定的训练集，几乎所有的矩形特征都可以满足上述要求）。每轮训练，将选取当轮中的最佳弱分类器（在算法中，迭代$T$次即是选择$T$个最佳弱分类器），最后将每轮得到的最佳弱分类器按照一定方法提升（Boosting）为强分类器。<!--什么意思？？？是先把弱分类器选出来，再Adaboost，那为啥要迭代，直接选前T个不就好了？？？黑人问号脸，啊啊啊啊，是我太蠢，还是讲话太糊,懂了一部分是最惨的--><br>AdaBoost算法的目的是从训练数据中学习一系列弱分类器，然后把这些弱分类器组合成一个强分类器。<br><strong>AdaBoost强分类器的训练步骤：</strong>参考<a href="https://blog.csdn.net/nk_wavelet/article/details/52601567" target="_blank" rel="noopener">AdaBoost人脸监测(3)</a> <a href="https://www.shuzhiduo.com/A/nAJvK1jx5r/" target="_blank" rel="noopener">图</a></p><p><font color="red">这里的公式还没有仔细厘清，先无脑抄了，整理好了再把这里删掉</font><br>给定训练数据集$T=\left(x_{i}, y_{i}\right), i=1,2,3, \ldots N$，共$N$个样本，其中样本$x \in X$，实例空间$x \in R^n$，$y_i \in Y=\{-1, +1\}$。$T$为迭代次数。<br><strong>步骤1：</strong>初始化训练数据的权值分布。每个训练数据初始都被赋予相同的权值：$\frac{1}{N}$</p><script type="math/tex; mode=display">D_1 = (w_{11},w_{12},\ldots,w_{1N}), \quad w_{1i}=\frac{1}{N}, \quad i=1,2,\ldots,N.</script><p><strong>步骤2：</strong>进行T轮迭代，用$t=1,2,3, \ldots T$表示迭代的次数。<br>①使用具有权值分布$D_m$的训练数据集测试，得到基本分类器(选取分类误差最低的阈值设计基本分类器)：$h_m(x):X \rightarrow \{−1, +1 \}$<br>②计算$h_m（x）$在训练数据集上的分类误差率：</p><script type="math/tex; mode=display">e_m = P(h_m(x_i)\neq y_i) = \sum_{i=1}^{N}{w_{mi}I(h_m(x_i) \neq y_i)}</script><p>由上述式子可知，$h_m(x)$在训练数据集上的误差$e_m$就是被$h_m(x)$误分类样本的权值之和。</p><script type="math/tex; mode=display">ω_{t,i}=\frac{ω_{t,i}}{\sum\limits_{j=1}^{n}ω_{t,j}}</script><p>②对每个(种)特征$f_j$，训练一个弱分类器$h_j$（如上），每个分类器只使用一种Haar特征进行训练。分类误差为：</p><script type="math/tex; mode=display">ε_j=\sum\limits_{i}ω_i|h_j(x_i)-y_i|</script><p>③从②确定的弱分类器中，找出一个具有最小分类误差的弱分类器$h_t$;<br>④更新每个样本对应的权重:</p><script type="math/tex; mode=display">\omega_{t+1, i}=\omega_{t, i} \beta_{t}^{1-e_{i}}</script><p>这里，如果样本$x_i$被正确分类，则$e_i=0$，否则$e_i=1$，而</p><script type="math/tex; mode=display">\beta_t=\frac{ε_t}{1-ε_t}</script><p>4、最终形成的强分类器组成为：</p><script type="math/tex; mode=display">h(x)=\left\{\begin{array}{cc}1 & \sum_{t-1}^{T} \alpha_{t} h_{t}(x) \geq 1 / 2 \sum_{t=1}^{T} \alpha_{t} \\0 & \text { otherwise }\end{array}\right.</script><p>其中：</p><script type="math/tex; mode=display">\alpha_t=log\frac{1}{\beta_t}</script><p>&emsp;&emsp;在使用Adaboost算法训练分类器之前，需要准备好正、负样本，根据样本特点选择和构造特征集。<br>&emsp;&emsp;由算法的训练过程可知，当弱分类器对样本分类正确，样本的权重会减小；而分类错误时，样本的权重会增加。这样，后面的分类器会加强对错分样本的训练。最后，组合所有的弱分类器形成强分类器，通过比较这些弱分类器投票的加权和与平均投票结果来检测图像。<br>&emsp;&emsp;OpenCV中，强分类器是由多个弱分类器“并列”构成，即强分类器中的弱分类器是两两相互独立的。在检测目标时，每个弱分类器独立运行并输出结果，然后把当前强分类器中每一个弱分类器的输出值相加，与本级强分类器的阈值相比，当且仅当结果大于阈值，认为当前检测窗口通过了该级强分类器。</p><h3 id="3-级联分类器的检测"><a href="#3-级联分类器的检测" class="headerlink" title="3.级联分类器的检测"></a>3.级联分类器的检测</h3><p>&emsp;&emsp;整合好了级联分类器，接下来就是高光时刻：检测！<br>&emsp;&emsp;检测是以显示中的一副图片作为输入，然后对图片进行多区域、多尺度的检测。</p><ul><li>多区域检测是指遍历整张图片。</li><li>多尺度检测有两种策略<br>1）搜索窗口的大小不变，不断缩放图片，在这种情况下需要对每个缩放后的图片进行区域特征值的运算，效率较低。<br>2）不断扩大搜索窗口，对图片进行特征值运算，效率较高。<strong>因为1）需要计算大小不同的图片的积分图，而2）只需对原图进行一次积分图即可。不同的搜索窗口的特征值计算在线性时间内即可完成。</strong></li></ul><p>&emsp;&emsp;放大+平移获得的子特征有多少个呢？Rainer Lienhart在其论文中做出了解释，假设检测窗口大小为$W<em>H$，矩形特征大小为$w</em>h$，X和Y表示矩形特征在水平和垂直方向上能放大的最大比例系数：</p><p><img src="/article/ad28cab2/20180527174205634" srcset="/img/loading.gif" alt="img"></p><p>&emsp;&emsp;如图5，在检测窗口window中，一般矩形特征的数量为：$XY(W+1-w\frac{X+1}{2})(    H+1-h\frac{Y+1}{2})$。对于之前x3特征在24*24大小的检测窗口中（W=H=24,w=3,h=1,X=8,Y=24），一共能产生27600个子特征。具体参考<a href="https://blog.csdn.net/liulina603/article/details/8617281" target="_blank" rel="noopener">Haar特征与积分图</a>。<br>&emsp;&emsp;无论哪种搜索方法，都会向级联分类器输入大量的子窗口图像，子窗口图像在筛选式级联分类器中依次被每一个节点筛选，丢弃或通过。最后利用并查集合并检测结果窗口。重叠窗口个数大于阈值时，选取其中之一作为检测结果，其余丢弃。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h3><p>总结Haar分类器的五大训练步骤：</p><ol><li>准备人脸、非人脸样本集</li><li>计算特征值和积分图</li><li>筛选出T个优秀的最优弱分类器(特征值)</li><li>把T个最优弱分类器传给AdaBoost进行训练，得到一个强分类器</li><li>重复3. 4.，将得到的n个强分类器级联成一个Adaboost级联分类器。</li></ol><p>以20*20窗口为例，有78,460个特征，筛选出T个优秀的最优弱分类器，然后把T个最优弱分类器传给AdaBoost进行训练得到一个强分类器，最后将n个强分类器级联成一个Adaboost级联分类器。</p><h2 id="五、分类器的检测"><a href="#五、分类器的检测" class="headerlink" title="五、分类器的检测"></a>五、分类器的检测</h2><p>OpenCV自带了训练器和检测器。如果想从头训练一个分类器检测汽车、飞机等其他物体，可以使用OpenCV构建。细节参考这里：<a href="http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html" target="_blank" rel="noopener">Cascade Classifier Training</a>。<br>OpenCV自带的检测器在OpenCV库文件\haarcascades文件\xx.xml文件中。在我的电脑里，具体路径为C:\Users\susu\AppData\Roaming\Python\Python37\site-packages\cv2\data\中。这个文件夹下包含了检测人脸、眼睛、鼻子和嘴等部位的检测器。均需要正面、直立的人体图像。<br>xml文件包含了检测器的相关信息。具体可以参考<a href="https://blog.csdn.net/playezio/article/details/80471000" target="_blank" rel="noopener">haar+adaboost代码讲解(OpenCV)</a>。</p><h1 id="3-3-人脸检测-OpenCV简单实现"><a href="#3-3-人脸检测-OpenCV简单实现" class="headerlink" title="3.3 人脸检测(OpenCV简单实现)"></a>3.3 人脸检测(OpenCV简单实现)</h1><h2 id="一、静态图片中的人脸检测"><a href="#一、静态图片中的人脸检测" class="headerlink" title="一、静态图片中的人脸检测"></a>一、静态图片中的人脸检测</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图像</span>img = cv2.imread(<span class="hljs-string">'image2.jpg'</span>, <span class="hljs-number">1</span>)<span class="hljs-comment"># 创建人脸和人眼的级联分类器，加载.xml分类器文件，即Haar特征的分类器（上一篇使用LBP特征的分类器）</span>face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_eye.xml'</span>)<span class="hljs-comment"># 进行人脸检测，scaleFactor表示图像的压缩率，minNeighbors表示每个人脸矩形的邻近数目</span>faces = face_cascade.detectMultiScale(img, scaleFactor=<span class="hljs-number">1.3</span>, minNeighbors=<span class="hljs-number">5</span>)<span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> faces:    <span class="hljs-comment"># 使用矩形框出人脸</span>    img = cv2.rectangle(img, (x, y), (x+w, y+h), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)    face_area = img[y:y+h, x:x+w]    <span class="hljs-comment"># 在人脸上检测人眼</span>    eyes = eye_cascade.detectMultiScale(face_area)    <span class="hljs-comment"># 使用矩形框出人眼</span>    <span class="hljs-keyword">for</span> (ex, ey, ew, eh) <span class="hljs-keyword">in</span> eyes:        cv2.rectangle(face_area, (ex, ey), (ex+ew, ey+eh), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'img'</span>, img)cv2.waitKey(<span class="hljs-number">0</span>)cv2.destroyAllWindows()cv2.imwrite(<span class="hljs-string">'output.jpg'</span>, img)</code></pre><h2 id="二、实时人脸监测"><a href="#二、实时人脸监测" class="headerlink" title="二、实时人脸监测"></a>二、实时人脸监测</h2><pre><code class="hljs python"><span class="hljs-comment"># 调用电脑摄像头进行实时人脸+眼睛识别</span><span class="hljs-keyword">import</span> cv2face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_eye.xml'</span>)smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_smile.xml'</span>)<span class="hljs-comment"># 调用摄像头</span>cap = cv2.VideoCapture(<span class="hljs-number">0</span>)cv2.namedWindow(<span class="hljs-string">'Dynamic'</span>)<span class="hljs-keyword">while</span>(<span class="hljs-literal">True</span>):    <span class="hljs-comment"># 获取摄像头拍摄到的画面</span>    ret, frame = cap.read()    <span class="hljs-comment"># ret为True表明图片读取成功。</span>    faces = face_cascade.detectMultiScale(frame, <span class="hljs-number">1.3</span>, <span class="hljs-number">2</span>)    img = frame    <span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> faces:        <span class="hljs-comment"># 画出人脸框，蓝色，画笔宽度</span>        img = cv2.rectangle(img, (x, y), (x+w, y+h), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)        <span class="hljs-comment"># 框选出人脸区域，在人脸区域而不是全图中进行人眼检测，节省计算资源</span>        face_area = img[y:y+h, x:x+w]        eyes = eye_cascade.detectMultiScale(face_area, <span class="hljs-number">1.3</span>, <span class="hljs-number">10</span>)        <span class="hljs-comment"># 画出人眼框，绿色，画笔宽度为1</span>        <span class="hljs-keyword">for</span> (ex, ey, ew, eh) <span class="hljs-keyword">in</span> eyes:            eye_area = cv2.rectangle(face_area, (ex, ey), (ex+ew, ey+eh), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)        smile = smile_cascade.detectMultiScale(face_area, scaleFactor=<span class="hljs-number">1.16</span>, minNeighbors=<span class="hljs-number">65</span>, minSize=(<span class="hljs-number">25</span>, <span class="hljs-number">25</span>), flags=cv2.CASCADE_SCALE_IMAGE)        <span class="hljs-keyword">for</span> (ex, ey, ew, eh) <span class="hljs-keyword">in</span> smile:            cv2.rectangle(face_area, (ex, ey, ex+ew, ey+eh), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">1</span>)            cv2.putText(img, <span class="hljs-string">'Smile'</span>, (x, y<span class="hljs-number">-7</span>), <span class="hljs-number">3</span>, <span class="hljs-number">1.2</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>, cv2.LINE_AA)    cv2.imshow(<span class="hljs-string">'frame2'</span>, img)    <span class="hljs-comment"># 按下q键退出</span>    <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">5</span>) &amp; <span class="hljs-number">0xFF</span> == ord(<span class="hljs-string">'q'</span>):        <span class="hljs-keyword">break</span>cap.release()cv2.destroyAllWindows()</code></pre><p>简单分析上面的代码:<br>&emsp;&emsp;创建一个级联分类器对象，加载xml检测器，进行人脸监测。</p><pre><code class="hljs python">face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)</code></pre><p>&emsp;&emsp;将人脸图像放入监测。<strong>不同于LBP检测器，这里彩色的图像也是可以的。</strong></p><pre><code class="hljs python">detectMultiScale(image[,scaleFactor[,minNeighbors[,flags,[minSize,[maxSize]]]]])</code></pre><p>1）image：待检测的输入图像<br>2）scaleFactor：每一个图像的尺度参数。默认值为1.1。scaleFactor参数控制两个不同大小窗口扫描的间距。参数过大，可能会错过正确的人脸区域。<br>3）minNeighbors：每一个级联矩形应该保留的邻近个数，默认为3.minNeighbors控制与检测率。默认值为3，表明至少有3次重叠监测，认为人脸确实存在。<br>4）minSize：目标最小尺寸<br>5）maxSize：目标最大尺寸</p><p>这次想写得太多了，啥都想加，加到最后好累。。。sad，不知道应该挑几个重点还是都来。awsl</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.cnblogs.com/zyly/p/9410563.html" target="_blank" rel="noopener">人脸监测之Haar分类器</a><br><a href="https://blog.csdn.net/liulina603/article/details/8617281" target="_blank" rel="noopener">Haar特征与积分图(推荐)</a><br><a href="https://blog.csdn.net/nk_wavelet/article/details/52601567" target="_blank" rel="noopener">AdaBoost人脸检测介绍(3)</a></p>]]></content>
    
    
    <categories>
      
      <category>图像处理下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pandas下综合练习</title>
    <link href="/article/67aeaf8.html"/>
    <url>/article/67aeaf8.html</url>
    
    <content type="html"><![CDATA[<h1 id="一、端午节的淘宝粽子交易"><a href="#一、端午节的淘宝粽子交易" class="headerlink" title="一、端午节的淘宝粽子交易"></a>一、端午节的淘宝粽子交易</h1><h2 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h2><p>（1）请删除最后一列为缺失值的行，并求所有在杭州发货的商品单价均值。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'端午粽子数据.csv'</span>)df.head()</code></pre><p><img src="/article/67aeaf8/image-20200701220417701.png" srcset="/img/loading.gif" alt="image-20200701220417701">列名中有些包含了空格，先去除。</p><pre><code class="hljs python">df.columns = df.columns.str.strip()df.columns</code></pre><p><img src="/article/67aeaf8/image-20200701220535004.png" srcset="/img/loading.gif" alt="image-20200701220535004"><br>然后将最后一列为空的行去除，使用dropna方法。再查看最后一列为空的行。</p><pre><code class="hljs python">df_clean = df.dropna(axis=<span class="hljs-number">0</span>,subset=[df.columns[<span class="hljs-number">-1</span>]])df_clean[df_clean[df_clean.columns[<span class="hljs-number">-1</span>]].isna()]</code></pre><p><img src="/article/67aeaf8/image-20200701220754043.png" srcset="/img/loading.gif" alt="image-20200701220754043"><br>确认已删除完毕。<br>将所有在杭州交易的记录取出。</p><pre><code class="hljs python">df_h = df_clean[df_clean[<span class="hljs-string">'发货地址'</span>].str.contains(<span class="hljs-string">'杭州'</span>)]</code></pre><p>查看其中价格列不能转换为float的行。</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">is_number</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">try</span>:        float(x)        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>    <span class="hljs-keyword">except</span> (SyntaxError, ValueError) <span class="hljs-keyword">as</span> e:        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>df_h[~df_h[<span class="hljs-string">'价格'</span>].map(is_number)]</code></pre><p><img src="/article/67aeaf8/image-20200701221103969.png" srcset="/img/loading.gif" alt="image-20200701221103969"></p><p>将其价格改为45。计算价格转换为float类型并计算均值。</p><pre><code class="hljs python">df_h.loc[<span class="hljs-number">4376</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45</span>df_h[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>).mean()</code></pre><p><img src="/article/67aeaf8/image-20200701221222671.png" srcset="/img/loading.gif" alt="image-20200701221222671"></p><p>（2）商品标题带有”嘉兴”但发货地却不在嘉兴的商品有多少条记录？<br>直接使用contain方法求取即可。</p><pre><code class="hljs python">df_bj = df_clean[df_clean[<span class="hljs-string">'标题'</span>].str.contains(<span class="hljs-string">'嘉兴'</span>)]df_bj_nj =df_bj[~(df_bj[<span class="hljs-string">'发货地址'</span>].str.contains(<span class="hljs-string">'嘉兴'</span>))]df_bj_nj.shape</code></pre><p>（3）请按照分位数将价格分为“高、较高、中、较低、低” 5个类别，再将类别结果插入到标题一列之后，最后对类别列进行降序排序。<br>查看价格列中异常值。</p><pre><code class="hljs python">df_clean[~df_clean[<span class="hljs-string">'价格'</span>].map(is_number)]</code></pre><p><img src="/article/67aeaf8/image-20200701221526292.png" srcset="/img/loading.gif" alt="image-20200701221526292"><br>修改为对应的正常值。</p><pre><code class="hljs python">df_clean.loc[<span class="hljs-number">538</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45.9</span>df_clean.loc[<span class="hljs-number">4376</span>][<span class="hljs-string">'价格'</span>] = <span class="hljs-number">45</span></code></pre><p>使用qcut方法将价格按数量等分为5类。并重命名为“高、较高、中、较低、低” 5个类别。</p><pre><code class="hljs python">q_cut = pd.qcut(df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>),<span class="hljs-number">5</span>)df_clean.loc[:,<span class="hljs-string">'类别'</span>] = q_cut.cat.rename_categories([<span class="hljs-string">'低'</span>,<span class="hljs-string">'较低'</span>,<span class="hljs-string">'中'</span>,<span class="hljs-string">'较高'</span>,<span class="hljs-string">'高'</span>])</code></pre><p>将列顺序按要求排列。并将表格按照类别进行降序排列。</p><pre><code class="hljs python">df_clean = df_clean[[<span class="hljs-string">'标题'</span>,<span class="hljs-string">'类别'</span>,<span class="hljs-string">'价格'</span>,<span class="hljs-string">'付款人数'</span>,<span class="hljs-string">'店铺'</span>,<span class="hljs-string">'发货地址'</span>]]df_clean = df_clean.sort_values(<span class="hljs-string">'类别'</span>, ascending=<span class="hljs-literal">False</span>)</code></pre><p><img src="/article/67aeaf8/image-20200701221901144.png" srcset="/img/loading.gif" alt="image-20200701221901144"><br>（4）付款人数一栏有缺失值吗？若有则请利用上一问的分类结果对这些缺失值进行合理估计并填充。<br>查看付款人数一栏有缺失值的行。</p><pre><code class="hljs python">df_clean[df_clean[<span class="hljs-string">'付款人数'</span>].isna()]</code></pre><p><img src="/article/67aeaf8/image-20200701221950329.png" srcset="/img/loading.gif" alt="image-20200701221950329"></p><pre><code class="hljs python">df_clean[<span class="hljs-string">'价格'</span>] = df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'float'</span>)df_clean.sort_values(<span class="hljs-string">'价格'</span>, ascending=<span class="hljs-literal">False</span>)df_clean[<span class="hljs-string">'发货地址'</span>].value_counts()</code></pre><pre><code class="hljs gml">def replace(<span class="hljs-symbol">x</span>):    try:        <span class="hljs-symbol">x</span> = str(<span class="hljs-symbol">x</span>)        <span class="hljs-keyword">if</span> <span class="hljs-string">'万'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'万'</span>)            <span class="hljs-keyword">return</span> float(<span class="hljs-symbol">x</span>[:i]) * <span class="hljs-number">10000</span>        <span class="hljs-keyword">if</span> <span class="hljs-string">'+'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'+'</span>)        elif <span class="hljs-string">'人'</span> in <span class="hljs-symbol">x</span>:            i = <span class="hljs-symbol">x</span>.index(<span class="hljs-string">'人'</span>)        <span class="hljs-keyword">else</span>:            # print(<span class="hljs-symbol">x</span>)            <span class="hljs-keyword">return</span> None        <span class="hljs-keyword">return</span> int(<span class="hljs-symbol">x</span>[:i])    except:        print(<span class="hljs-symbol">x</span>)        <span class="hljs-keyword">return</span> None</code></pre><pre><code class="hljs python">df_clean[<span class="hljs-string">'付款人数'</span>] = df_clean[<span class="hljs-string">'付款人数'</span>].map(replace)df_clean = df_clean.set_index(<span class="hljs-string">'价格'</span>)df_clean[<span class="hljs-string">'付款人数'</span>] = df_clean[<span class="hljs-string">'付款人数'</span>].interpolate(method=<span class="hljs-string">'index'</span>)df_clean = df_clean.reset_index()df_clean = df_clean[[<span class="hljs-string">'标题'</span>,<span class="hljs-string">'类别'</span>,<span class="hljs-string">'价格'</span>,<span class="hljs-string">'付款人数'</span>,<span class="hljs-string">'店铺'</span>,<span class="hljs-string">'发货地址'</span>]]</code></pre><p>（5）请将数据后四列合并为如下格式的Series:商品发货地为xx，店铺为xx，共计xx人付款，单价为xx。</p><pre><code class="hljs prolog">result = <span class="hljs-string">'商品发货地为'</span>+df_clean[<span class="hljs-string">'发货地址'</span>]+<span class="hljs-string">',店铺为'</span>+df_clean[<span class="hljs-string">'店铺'</span>] \                    +<span class="hljs-string">',共计'</span>+df_clean[<span class="hljs-string">'付款人数'</span>].astype(<span class="hljs-string">'str'</span>)+<span class="hljs-string">',单价为'</span>\                   + df_clean[<span class="hljs-string">'价格'</span>].astype(<span class="hljs-string">'str'</span>)+<span class="hljs-string">'元'</span></code></pre><pre><code class="hljs awk">result.str.extract(<span class="hljs-string">r'商品发货地为(?P&lt;发货地址&gt;[\w]+),店铺为(?P&lt;店铺&gt;[\w]+) \</span><span class="hljs-string">                    ,共计(?P&lt;付款人数&gt;[\w]+),单价为(?P&lt;价格&gt;[?\d.\dh]+)元'</span>)</code></pre><p>提取的时候失败了。。</p><h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><pre><code class="hljs routeros">df3 = pd.read_csv(<span class="hljs-string">'摩拜单车数据.csv'</span>)df3[<span class="hljs-string">'start_time'</span>] = pd.to_datetime(df3[<span class="hljs-string">'start_time'</span>].values)df3[<span class="hljs-string">'end_time'</span>] = pd.to_datetime(df3[<span class="hljs-string">'end_time'</span>].values)df3[<span class="hljs-string">'run'</span>] = df3[<span class="hljs-string">'end_time'</span>]-df3[<span class="hljs-string">'start_time'</span>]result1 = df3[df3[<span class="hljs-string">'start_time'</span>].dt.dayofweek.isin([5,6])]result2 = df3[df3[<span class="hljs-string">'start_time'</span>].dt.dayofweek.isin([0,1,2,3,4])]result1.index = result1[<span class="hljs-string">'start_time'</span>].dt.strftime(<span class="hljs-string">'%Y-%m-%d'</span>)result2.index = result2[<span class="hljs-string">'start_time'</span>].dt.strftime(<span class="hljs-string">'%Y-%m-%d'</span>)result1.index.name = <span class="hljs-string">'day'</span>result2.index.name = <span class="hljs-string">'day'</span>result2.head()group1 = result1.groupby([<span class="hljs-string">'day'</span>])num1 = 0num2 = 0group1.size()<span class="hljs-comment"># for name, group in group1:</span><span class="hljs-keyword">for</span> name,<span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> group1:    <span class="hljs-builtin-name">print</span>(name)    num1 += 1    <span class="hljs-keyword">if</span> num1 ==1:        result_1 = group[<span class="hljs-string">'run'</span>].sum()    <span class="hljs-keyword">else</span>:        result_1 += group[<span class="hljs-string">'run'</span>].sum()result_1 = result_1 / num1<span class="hljs-builtin-name">print</span>(result_1)group2 = result2.groupby(<span class="hljs-string">'day'</span>)<span class="hljs-keyword">for</span> name,<span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> group2:    num2 += 1    <span class="hljs-keyword">if</span> num2 ==1:        result_2 = group[<span class="hljs-string">'run'</span>].sum()    <span class="hljs-keyword">else</span>:        result_2 += group[<span class="hljs-string">'run'</span>].sum()result_2 = result_2 / num2<span class="hljs-builtin-name">print</span>(result_2)</code></pre><p><img src="/article/67aeaf8/image-20200701222344893.png" srcset="/img/loading.gif" alt="image-20200701222344893"></p><p><img src="/article/67aeaf8/image-20200701222354535.png" srcset="/img/loading.gif" alt="image-20200701222354535"></p><pre><code class="hljs nginx"><span class="hljs-attribute">result_1</span> &gt; result_2</code></pre><p><img src="/article/67aeaf8/image-20200701222414991.png" srcset="/img/loading.gif" alt="image-20200701222414991"></p><p>周末的工作量是比工作日当天用车量更大</p><p>（2）</p><pre><code class="hljs python">result2[<span class="hljs-string">"start_hour"</span>] = result2[<span class="hljs-string">'start_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">0</span>]))result2[<span class="hljs-string">"end_hour"</span>] = result2[<span class="hljs-string">'end_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">0</span>]))result2[<span class="hljs-string">"start_min"</span>] = result2[<span class="hljs-string">'start_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                          (<span class="hljs-string">":"</span>)[<span class="hljs-number">1</span>]))result2[<span class="hljs-string">'end_min'</span>] = result2[<span class="hljs-string">'end_time'</span>].astype(<span class="hljs-string">'str'</span>).apply(<span class="hljs-keyword">lambda</span> x: int(x.split()[<span class="hljs-number">1</span>].split\                                                            (<span class="hljs-string">":"</span>)[<span class="hljs-number">1</span>]))</code></pre><pre><code class="hljs python">result2_e = result2[(result2[<span class="hljs-string">'start_hour'</span>]&lt;<span class="hljs-number">9</span>) | ((result2[<span class="hljs-string">'start_hour'</span>]==<span class="hljs-number">9</span>) &amp; (result2[<span class="hljs-string">'start_min'</span>] &lt; <span class="hljs-number">30</span>))]result2_e = result2_e[(result2_e[<span class="hljs-string">'end_hour'</span>]&gt;<span class="hljs-number">7</span>) | ((result2_e[<span class="hljs-string">'end_hour'</span>]==<span class="hljs-number">7</span>) &amp; (result2_e[<span class="hljs-string">'end_min'</span>] &gt; <span class="hljs-number">30</span>))]result2_a = result2[(result2[<span class="hljs-string">'start_hour'</span>]&lt;<span class="hljs-number">19</span>) &amp; (result2[<span class="hljs-string">'end_hour'</span>]&gt;<span class="hljs-number">17</span>)]</code></pre><pre><code class="hljs python">result2_e.shape[<span class="hljs-number">0</span>]group2_e = result2_e.groupby(<span class="hljs-string">'day'</span>)list_e = []list_n = []<span class="hljs-keyword">for</span> name, group <span class="hljs-keyword">in</span> group2_e:    list_n.append(name)    list_e.append(group[<span class="hljs-string">'run'</span>].sum())group2_a = result2_a.groupby(<span class="hljs-string">'day'</span>)list_a = []list_n2 = []<span class="hljs-keyword">for</span> name, group <span class="hljs-keyword">in</span> group2_a:    list_n2.append(name)    list_a.append(group[<span class="hljs-string">'run'</span>].sum())result_day = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(list_a)):    <span class="hljs-keyword">if</span> list_e[i] &gt; list_a[i]:        result_day.append(list_n[i])    <span class="hljs-keyword">else</span>:        <span class="hljs-keyword">pass</span></code></pre><pre><code class="hljs python">result_day</code></pre><p><img src="/article/67aeaf8/image-20200701223448673.png" srcset="/img/loading.gif" alt="image-20200701223448673"></p>]]></content>
    
    
    <categories>
      
      <category>pandas下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4方差分析</title>
    <link href="/article/2ad44fb9.html"/>
    <url>/article/2ad44fb9.html</url>
    
    <content type="html"><![CDATA[<h1 id="方差分析"><a href="#方差分析" class="headerlink" title="方差分析"></a>方差分析</h1><h2 id="1-概要"><a href="#1-概要" class="headerlink" title="1 概要"></a>1 概要</h2><p><strong>方差分析</strong>(<strong>Analysis of variance, ANOVA</strong>) 主要研究分类变量作为自变量时，对因变量的影响是否是显著的。</p><p>方差分析的方法是由20世纪的统计学家Ronald Aylmer Fisher在1918年到1925年之间提出并陆续完善起来的，该方法刚开始是用于解决田间实验的数据分析问题，因此，方差分析的学习是和实验设计、实验数据的分析密不可分的。</p><p>实验设计和方差分析都有自己相应的语言。因此，在这里我们通过一个焦虑症治疗的实例，先了解一些术语，并且思考一下，方差分析主要用于解决什么样的问题。<a id="more"></a></p><p>以焦虑症治疗为例，现有两种治疗方案：认知行为疗法（CBT）和眼动脱敏再加工法（EMDR）。我们招募10位焦虑症患者作为志愿者，随机分配一半的人接受为期五周的CBT，另外一半接受为期五周的EMDR，设计方案如表1-1所示。在治疗结束时，要求每位患者都填写状态特质焦虑问卷（STAI），也就是一份焦虑度测量的自我评测报告。</p><p><strong>表1-1 单因素组间方差分析</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">CBT</th><th style="text-align:center">EMDR</th></tr></thead><tbody><tr><td style="text-align:center">s1</td><td style="text-align:center">s6</td></tr><tr><td style="text-align:center">s2</td><td style="text-align:center">s7</td></tr><tr><td style="text-align:center">s3</td><td style="text-align:center">s8</td></tr><tr><td style="text-align:center">s4</td><td style="text-align:center">s9</td></tr><tr><td style="text-align:center">s5</td><td style="text-align:center">s10</td></tr></tbody></table></div><p>在这个实验设计中，治疗方案是两水平（CBT、EMDR）的<strong>组间因子</strong>。之所以称其为<strong>组间因子</strong>，是因为每位患者都仅被分配到一个组别中，没有患者同时接受CBT和EMDR。表中字母s代表受试者（患者）。STAI是<strong>因变量</strong>，治疗方案是<strong>自变量</strong>。由于在每种治疗方案下观测数相等，因此这种设计也称为<strong>均衡设计</strong>（balanced design）；若观测数不同，则称作<strong>非均衡设计</strong>（unbalanced design）。</p><p>因为仅有一个类别型变量，表1的统计设计又称为<strong>单因素方差分析（one-way ANOVA）</strong>，或进一步称为<strong>单因素组间方差分析</strong>。方差分析主要通过F检验来进行效果评测，若治疗方案的F检验显著，则说明五周后两种疗法的STAI得分均值不同。</p><p>假设你只对CBT的效果感兴趣，则需将10个患者都放在CBT组中，然后在治疗五周和六个月后分别评价疗效，设计方案如表1-2所示。</p><p><strong>表1-2 单因素组内方差分析</strong></p><div class="table-container"><table><thead><tr><th></th><th>时间</th><th></th></tr></thead><tbody><tr><td><strong>患者</strong></td><td>5周</td><td>6个月</td></tr><tr><td>s1</td><td></td><td></td></tr><tr><td>s2</td><td></td><td></td></tr><tr><td>s3</td><td></td><td></td></tr><tr><td>s4</td><td></td><td></td></tr><tr><td>s5</td><td></td><td></td></tr><tr><td>s6</td><td></td><td></td></tr><tr><td>s7</td><td></td><td></td></tr><tr><td>s8</td><td></td><td></td></tr><tr><td>s9</td><td></td><td></td></tr><tr><td>s10</td><td></td></tr></tbody></table></div><p>此时，时间（time）是两水平（五周、六个月）的<strong>组内因子</strong>。因为每位患者在所有水平下都进行了测量，所以这种统计设计称<strong>单因素组内方差分析</strong>；又由于每个受试者都不止一次被测量，也称作<strong>重复测量方差分析</strong>。当时间的F检验显著时，说明患者的STAI得分均值在五周和六个月间发生了改变。</p><p>现假设你对治疗方案差异和它随时间的改变都感兴趣，则将两个设计结合起来即可：随机分配五位患者到CBT，另外五位到EMDR，在五周和六个月后分别评价他们的STAI结果（见表1-3）。</p><p><strong>表1-3 含组间和组内因子的双因素方差分析</strong></p><div class="table-container"><table><thead><tr><th></th><th></th><th>时间</th><th></th></tr></thead><tbody><tr><td><strong>疗法</strong></td><td><strong>患者</strong></td><td>5周</td><td>6个月</td></tr><tr><td>CBT</td><td>s1</td><td></td><td></td></tr><tr><td></td><td>s2</td><td></td><td></td></tr><tr><td></td><td>s3</td><td></td><td></td></tr><tr><td></td><td>s4</td><td></td><td></td></tr><tr><td></td><td>s5</td><td></td><td></td></tr><tr><td>EMDR</td><td>s6</td><td></td><td></td></tr><tr><td></td><td>s7</td><td></td><td></td></tr><tr><td></td><td>s8</td><td></td><td></td></tr><tr><td></td><td>s9</td><td></td><td></td></tr><tr><td></td><td>s10</td><td></td></tr></tbody></table></div><p>疗法（therapy）和时间（time）都作为因子时，我们既可分析疗法的影响（时间跨度上的平均）和时间的影响（疗法类型跨度上的平均），又可分析疗法和时间的交互影响。前两个称作<strong>主效应</strong>，交互部分称作<strong>交互效应</strong>。</p><p>当设计包含两个甚至更多的因子时，便是<strong>因素方差分析设计</strong>，比如两因子时称作<strong>双因素方差分析</strong>，三因子时称作三因素方差分析，以此类推。若因子设计包括组内和组间因子，又称作<strong>混合模型方差分析</strong>，当前的例子就是典型的双因素混合模型方差分析。</p><p>本例中，你将做三次F检验：疗法因素一次，时间因素一次，两者交。</p><p>15214互因素一次。若疗法结果显著，说明CBT和EMDR对焦虑症的治疗效果不同；若时间结果显著，说明焦虑度从五周到六个月发生了变化；若两者交互效应显著，说明两种疗法随着时间变化对焦虑症治疗影响不同（也就是说，焦虑度从五周到六个月的改变程度在两种疗法间是不同的）。</p><p>现在，我们对上面的实验设计稍微做些扩展。众所周知，抑郁症对病症治疗有影响，而且抑郁症和焦虑症常常同时出现。即使受试者被随机分配到不同的治疗方案中，在研究开始时，两组疗法中的患者抑郁水平就可能不同，任何治疗后的差异都有可能是最初的抑郁水平不同导致的，而不是由于实验的操作问题。抑郁症也可以解释因变量的组间差异，因此它常称为<strong>混淆因素</strong>（confounding factor）。由于你对抑郁症不感兴趣，它也被称作<strong>干扰变数</strong>（nuisance variable）。</p><p>假设招募患者时使用抑郁症的自我评测报告，比如白氏抑郁症量表（BDI），记录了他们的抑郁水平，那么你可以在评测疗法类型的影响前，对任何抑郁水平的组间差异进行统计性调整。本案例中，BDI为<strong>协变量</strong>，该设计为<strong>协方差分析</strong>（ANCOVA）。</p><p>以上设计只记录了单个因变量情况（STAI），为增强研究的有效性，可以对焦虑症进行其他的测量（比如家庭评分、医师评分，以及焦虑症对日常行为的影响评价）。当因变量不止一个时，设计被称作<strong>多元方差分析</strong>（MANOVA）， 若协变量也存在， 那么就叫<strong>多元协方差分析</strong>（MANCOVA）。</p><p>下面我们主要介绍单因素方差分析与双因素方差分析的原理与实现。</p><h2 id="2-单因素方差分析"><a href="#2-单因素方差分析" class="headerlink" title="2 单因素方差分析"></a>2 单因素方差分析</h2><h3 id="2-1-推导过程"><a href="#2-1-推导过程" class="headerlink" title="2.1 推导过程"></a>2.1 推导过程</h3><p>接下来我们使用种小麦的例子，去帮助理解方差分析里涉及的一些变量。</p><p>假设我们现在有若干品种的小麦，要在某一地区播种，我们想知道这些品种的产量有没有显著区别，为此我们先设计了一个田间实验，取一大块地将其分成形状大小都相同的$n$小块．设供选择的品种有$k$个，我们打算其中的$n_1$小块种植品种1, $n_2$小块种植品种2，等等，$n_1 + n_2 + … n_k = n$.</p><p>接下来，我们使用方差分析的方法去看不同小麦品种的产量是否有显著差异。</p><p>设问题中涉及一个因素$A$，有$k$个水平，如上例的$k$个种子品种，以$Y_{ij}$记第$i$个水平的第$j$个观察值，如上例$Y_{ij}$是种植品种$i$的第$j$小块地上的亩产量。模型为</p><script type="math/tex; mode=display">Y_{ij} = a_i + e_{ij},  j = 1,...,n_i, i = 1,...,k\qquad(2.1)</script><p>$a_i$表示水平$i$的理论平均值，称为水平$i$的效应。在小麦例子中，$a_i$就是品种$i$的平均亩产量，$e_{ij}$就是随机误差。并且我们假定：</p><script type="math/tex; mode=display">E(e_{ij})=0, 0<Var(e_{ij})={\sigma}^2<\infty,一切e_{ij}独立同分布\qquad(2.2)</script><p>因素$A$的各水平的高低优劣，取决于其理论平均$a_{i}$的大小。故对模型(2.1)，我们头一个关心的事情，就是诸$a_{i}$是否全相同。 如果是，则表示因素$A$对所考察的指标$Y$其实无影响．这时我们就说因素A的效应不显著，否则就说它显著。当然，在实际应用中，所谓“显著”，是指诸$a_{i}$之间的差异要大到一定的程度．这个 “一定的程度”，是从其实用上的意义着眼，而“统计显著性”，则是与随机误差相比而言．这点在下文的讨论中会有所体现．我们把所要检验的假设写为：</p><script type="math/tex; mode=display">H_0:a_1=a_2=\cdots=a_k \qquad (2.3)</script><p>为检验该假设，我们需要分析，为什么各个$Y_{ij}$会有差异？从模型(2.1)来看，无非两个原因：一是各$a_{i}$可能有差异．例如，若 $a_1&gt;a_2$, 这就使$Y_{1j}$倾向于大于$Y_{2j}$；二是随机误差的存在。这一分析启发了如下的想法：找一个衡量全部$y_{ij}$的变异的量：</p><script type="math/tex; mode=display">SS= \sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y} \right )^2, \qquad \bar{Y}=\sum_{i=1}^{k}\sum_{j=1}^{n_i}Y_{ij}/n \qquad (2.4)</script><p>$SS$愈大，表示$Y_{ij}$之间的差异越大。</p><p>接下来，把$SS$分为两部分，一部分表示随机误差的影响，记为$SS_e$；另一部分表示因素$A$的各水平理论平均值$a_i$不同带来的影响，记为$SS_A$。</p><p>关于$SS_e$，先固定一个$i$，此时对应的所有观测值$Y_{i1},Y_{i2},\cdots,Y_{in}$，他们之间的差异与每个水平的理论平均值不等无关，而是取决于随机误差，反映这些观察值差异程度的量是$\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2$，其中</p><script type="math/tex; mode=display">\bar{Y_i}=(Y_{i1}+Y_{i2}+\cdots+Y_{in})/n_i,\quad i=1, 2,\cdots,n \qquad (2.5)</script><p>$\bar{Y_i}$可以视为对$a_i$的估计。把上述平方和做累加得：</p><script type="math/tex; mode=display">SS_e=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \qquad (2.6)</script><p>可求得$SS_A$:</p><script type="math/tex; mode=display">\begin{align}SS_A &= SS-SS_e \\ &=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y} \right )^2-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})-(\bar{Y}-\bar{Y_i}) \right )^2-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})^2-2(Y_{ij}-\bar{Y_i})(\bar{Y}-\bar{Y_i})+(\bar{Y}-\bar{Y_i})^2 \right )-\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&=\sum_{i=1}^{k}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2 - 2\sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( (Y_{ij}-\bar{Y_i})(\bar{Y}-\bar{Y_i}) \right )+ \sum_{i=1}^{k}\sum_{j=1}^{n_i}(\bar{Y}-\bar{Y_i})^2 - \sum_{i=1}^{k}\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2 \\&= \sum_{i=1}^{k}\sum_{j=1}^{n_i}(\bar{Y_i}-\bar{Y})^2 - 2\sum_{i=1}^{k}\left ( (\bar{Y}-\bar{Y_i})\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i}) \right ) \quad (ps:\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})=0) \\&= \sum_{i=1}^{k}n_i(\bar{Y_i}-\bar{Y})^2 \qquad (2.7)\end{align}</script><p>因为$\bar{Y_i}$可以视为对$a_i$的估计，$a_i$的差异越大，$\bar{Y_i}$之间的差异也越大，所以$SS_A$可以用来衡量不同水平之间的差异程度。</p><p>在统计学上，通常称$SS$为<strong>总平方和</strong>，$SS_A$为<strong>因素$A$的平方和</strong>，$SS_e$为<strong>误差平方和</strong>，分解式$SS=SS_A+SS_e$为该模型的<strong>方差分析</strong>。</p><p>基于上面的分析，我们可以得到假设（5.3）的一个检验方法：当比值$SS_A/SS_e$大于某一给定界限时，否定$H_0$，不然就接受$H_0$。为了构造$F$分布的检验统计量，我们假定随机误差$e_{ij}$满足正态分布$N(0, \sigma^2)$，同时我们也假定观察值$Y_{ij}$符合正态分布，此时，记</p><script type="math/tex; mode=display">MS_A = SS_A/(k-1), \quad MS_e = SS_e/(n-k) \qquad (2.8)</script><p>当$H_0$成立时，有：</p><script type="math/tex; mode=display">MS_A / MS_e \sim F_{k-1, n-k} \qquad (2.9)</script><p>据（5.9），在给定显著性水平$\alpha$时，即得（5.3）的假设$H_0$的检验如下：</p><script type="math/tex; mode=display">当MS_A / MS_e \leqslant  F_{k-1, n-k}(\alpha)时，接受H_0，不然就拒绝H_0 \qquad (2.10)</script><p>$MS_A$和$MS_e$分别被称为<strong>因素$A$和随机误差的平均平方和</strong>。被除数$k-1$和$n-k$，分别称为这两个平方和的<strong>自由度</strong>。$MS_e$的自由度为什么是$n-k$呢？因为平方和$\sum_{j=1}^{n_i}\left ( Y_{ij}-\bar{Y_i} \right )^2$的自由度为$n_i-1$，故对$i$求和，$SS_e$的自由度就是$n-k$。那么，$MS_A$的自由度为什么是$k-1$呢？因为一共有$k$个平均值$a_1,\cdots,a_k$等$k-1$个，故自由度为$k-1$，两者自由度之和为$n-1$，恰好是总平方和的自由度。</p><p>到这里，我们可以做出方差分析表如表2-1</p><p><strong>2-1 单因素方差分析的方差分析表</strong></p><div class="table-container"><table><thead><tr><th>项目</th><th>$SS$</th><th>自由度</th><th>$MS$</th><th>$F$比</th><th>显著性</th></tr></thead><tbody><tr><td>$A$</td><td>$SS_A$</td><td>$k-1$</td><td>$MS_A$</td><td>$MS_A / MS_e$</td><td><em>, *</em>, 或无</td></tr><tr><td>误差</td><td>$SS_e$</td><td>$n-k$</td><td>$MS_e$</td><td></td><td></td></tr><tr><td>总和</td><td>$SS$</td><td>$n-1$</td><td></td><td></td></tr></tbody></table></div><p>在上表中，对于显著性一栏，一般来说，我们把算出的$F$比，即$MS_A / MS_e$，与$F_{k-1, n-k}(0.05)=c_1$和$F_{k-1, n-k}(0.01)=c_2$比较。若$MS_A / MS_e&gt;c_2$，用**表示，表明A因素的效应是高度显著的，即在$\alpha=0.01$的显著性水平下，拒绝原假设（5.3）。同理，$c_2<MS_A ms_e<c_1$用$\ast $表示，$ms_a ms_e>c_1$时不显著。</MS_A></p><h3 id="2-2-代码实例"><a href="#2-2-代码实例" class="headerlink" title="2.2 代码实例"></a>2.2 代码实例</h3><h4 id="单因素方差分析的R语言实现"><a href="#单因素方差分析的R语言实现" class="headerlink" title="单因素方差分析的R语言实现"></a>单因素方差分析的R语言实现</h4><p>单因素方差分析中，你感兴趣的是比较分类因子定义的两个或多个组别中的因变量均值。以multcomp包中的cholesterol数据集为例，50个患者均接受降低胆固醇药物治疗（trt）五种疗法中的一种疗法。其中三种治疗条件使用药物相同，分别是20mg一天一次（1time）、10mg一天两次（2times）和5mg一天四次（4times）。剩下的两种方式（drugD和drugE）代表候选药物。</p><pre><code class="hljs angelscript">&gt; library(multcomp)&gt; attach(cholesterol)&gt; &gt; # 统计各组样本大小&gt; table(trt) trt <span class="hljs-number">1</span>time <span class="hljs-number">2</span>times <span class="hljs-number">4</span>times  drugD  drugE     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span>     <span class="hljs-number">10</span> &gt; &gt; # 各组均值&gt; aggregate(response, by=list(trt), FUN=mean)  Group<span class="hljs-number">.1</span>        x<span class="hljs-number">1</span>   <span class="hljs-number">1</span>time  <span class="hljs-number">5.78197</span><span class="hljs-number">2</span>  <span class="hljs-number">2</span>times  <span class="hljs-number">9.22497</span><span class="hljs-number">3</span>  <span class="hljs-number">4</span>times <span class="hljs-number">12.37478</span><span class="hljs-number">4</span>   drugD <span class="hljs-number">15.36117</span><span class="hljs-number">5</span>   drugE <span class="hljs-number">20.94752</span>&gt; &gt; # 各组标准差&gt; aggregate(response, by=list(trt), FUN=sd)  Group<span class="hljs-number">.1</span>        x<span class="hljs-number">1</span>   <span class="hljs-number">1</span>time <span class="hljs-number">2.878113</span><span class="hljs-number">2</span>  <span class="hljs-number">2</span>times <span class="hljs-number">3.483054</span><span class="hljs-number">3</span>  <span class="hljs-number">4</span>times <span class="hljs-number">2.923119</span><span class="hljs-number">4</span>   drugD <span class="hljs-number">3.454636</span><span class="hljs-number">5</span>   drugE <span class="hljs-number">3.345003</span>&gt; &gt; # 进行方差分析&gt; fit &lt;- aov(response ~ trt)&gt; summary(fit)            Df Sum Sq Mean Sq F value   Pr(&gt;F)    trt          <span class="hljs-number">4</span> <span class="hljs-number">1351.4</span>   <span class="hljs-number">337.8</span>   <span class="hljs-number">32.43</span> <span class="hljs-number">9.82e-13</span> ***Residuals   <span class="hljs-number">45</span>  <span class="hljs-number">468.8</span>    <span class="hljs-number">10.4</span>                     ---Signif. codes:  <span class="hljs-number">0</span> ‘***’ <span class="hljs-number">0.001</span> ‘**’ <span class="hljs-number">0.01</span> ‘*’ <span class="hljs-number">0.05</span> ‘.’ <span class="hljs-number">0.1</span> ‘ ’ <span class="hljs-number">1</span></code></pre><p>方差分析的结果中，各项数字的含义可以参照表2-1。</p><h4 id="查看各水平对应的组均值的差异"><a href="#查看各水平对应的组均值的差异" class="headerlink" title="查看各水平对应的组均值的差异"></a>查看各水平对应的组均值的差异</h4><p>gplots包中的plotmeans()可以用来绘制带有置信区间的组均值图形。如图9-1所示，图形展示了带有95%的置信区间的各疗法均值，可以清楚看到它们之间的差异。</p><pre><code class="hljs lisp">library(<span class="hljs-name">gplots</span>)plotmeans(<span class="hljs-name">response</span> ~ trt, xlab=<span class="hljs-string">"Treatment"</span>, ylab=<span class="hljs-string">"Response"</span>,          main=<span class="hljs-string">"Mean Plot\nwith 95% CI"</span>)detach(<span class="hljs-name">cholesterol</span>)</code></pre><p><img src="/article/2ad44fb9/2-1.png" srcset="/img/loading.gif" alt="2-1"></p><p><strong>2-1 五种降低胆固醇药物疗法的均值，含95%的置信区间</strong></p><h4 id="多重比较"><a href="#多重比较" class="headerlink" title="多重比较"></a>多重比较</h4><p>虽然ANOVA对各疗法的F检验表明五种药物疗法效果不同，但是并没有告诉你哪种疗法与其他疗法不同。多重比较可以解决这个问题。例如，TukeyHSD()函数提供了对各组均值差异的成对检验。</p><pre><code class="hljs angelscript">&gt; TukeyHSD(fit)  Tukey multiple comparisons of means    <span class="hljs-number">95</span>% family-wise confidence levelFit: aov(formula = response ~ trt)$trt                  diff        lwr       upr     p adj<span class="hljs-number">2</span>times<span class="hljs-number">-1</span>time   <span class="hljs-number">3.44300</span> <span class="hljs-number">-0.6582817</span>  <span class="hljs-number">7.544282</span> <span class="hljs-number">0.1380949</span><span class="hljs-number">4</span>times<span class="hljs-number">-1</span>time   <span class="hljs-number">6.59281</span>  <span class="hljs-number">2.4915283</span> <span class="hljs-number">10.694092</span> <span class="hljs-number">0.0003542</span>drugD<span class="hljs-number">-1</span>time    <span class="hljs-number">9.57920</span>  <span class="hljs-number">5.4779183</span> <span class="hljs-number">13.680482</span> <span class="hljs-number">0.0000003</span>drugE<span class="hljs-number">-1</span>time   <span class="hljs-number">15.16555</span> <span class="hljs-number">11.0642683</span> <span class="hljs-number">19.266832</span> <span class="hljs-number">0.0000000</span><span class="hljs-number">4</span>times<span class="hljs-number">-2</span>times  <span class="hljs-number">3.14981</span> <span class="hljs-number">-0.9514717</span>  <span class="hljs-number">7.251092</span> <span class="hljs-number">0.2050382</span>drugD<span class="hljs-number">-2</span>times   <span class="hljs-number">6.13620</span>  <span class="hljs-number">2.0349183</span> <span class="hljs-number">10.237482</span> <span class="hljs-number">0.0009611</span>drugE<span class="hljs-number">-2</span>times  <span class="hljs-number">11.72255</span>  <span class="hljs-number">7.6212683</span> <span class="hljs-number">15.823832</span> <span class="hljs-number">0.0000000</span>drugD<span class="hljs-number">-4</span>times   <span class="hljs-number">2.98639</span> <span class="hljs-number">-1.1148917</span>  <span class="hljs-number">7.087672</span> <span class="hljs-number">0.2512446</span>drugE<span class="hljs-number">-4</span>times   <span class="hljs-number">8.57274</span>  <span class="hljs-number">4.4714583</span> <span class="hljs-number">12.674022</span> <span class="hljs-number">0.0000037</span>drugE-drugD    <span class="hljs-number">5.58635</span>  <span class="hljs-number">1.4850683</span>  <span class="hljs-number">9.687632</span> <span class="hljs-number">0.0030633</span>&gt; par(las=<span class="hljs-number">2</span>)&gt; par(mar=c(<span class="hljs-number">5</span>,<span class="hljs-number">8</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>))&gt; plot(TukeyHSD(fit))</code></pre><p>成对比较图形如图2-2所示。图形中置信区间包含0的疗法说明差异不显著（p&gt;0.05）。</p><p><img src="/article/2ad44fb9/2-2.png" srcset="/img/loading.gif" alt="2-2"></p><p><strong>图2-2 Tukey HSD均值成对比较图</strong></p><h4 id="评估检验的假设条件"><a href="#评估检验的假设条件" class="headerlink" title="评估检验的假设条件"></a>评估检验的假设条件</h4><p>根据2.1中我们讲的关于方差分析的推导中，我们知道，方差分析结果的有效性是建立在一系列假设条件之上的，因此，在我们使用方差分析模型时，需要评估进行方差分析的数据，是否符合模型使用的假设条件。</p><h5 id="正态性检验"><a href="#正态性检验" class="headerlink" title="正态性检验"></a>正态性检验</h5><p>第一，在建立模型时，我们假设因变量是服从正态分布的，需要进行正态性检验。</p><p>正态性检验的方法有两种，一是通过QQ图进行检验。</p><pre><code class="hljs routeros"><span class="hljs-comment"># QQ plot</span>library(car)qqPlot(lm(response ~ trt, <span class="hljs-attribute">data</span>=cholesterol),       <span class="hljs-attribute">simulate</span>=<span class="hljs-literal">TRUE</span>, <span class="hljs-attribute">main</span>=<span class="hljs-string">"Q-Q Plot"</span>, <span class="hljs-attribute">labels</span>=<span class="hljs-literal">FALSE</span>)</code></pre><p><img src="/article/2ad44fb9/2-3.png" srcset="/img/loading.gif" alt="2-3"></p><p>除此之外，R里面也提供了一些package来进行正态性检验。</p><h6 id="K-S-test"><a href="#K-S-test" class="headerlink" title="K-S test"></a>K-S test</h6><p>统计学里, Kolmogorov–Smirnov 检验(亦称：K–S 检验)是用来检验数据是否符合某种分布的一种非参数检验。其原假设$H_0$:两个数据分布一致或者数据符合理论分布。在R语言里，我们可以使用<code>ks.test(x, pnorm)</code>进行正态性检验，若结果中的p值大于0.05，则数据符合正态分布。</p><h6 id="Anderson–Darling-test"><a href="#Anderson–Darling-test" class="headerlink" title="Anderson–Darling test"></a><strong>Anderson–Darling test</strong></h6><p>Anderson–Darling检验是一种用来检验给定的样本是否来自于某个确定的概率分布的统计检验方法。在R语言中，我们可以从<strong>nortest包</strong>中的<code>ad.test()</code>进行检验。若结果中的p值大于0.05，则数据符合正态分布。</p><h6 id="Shapiro-Wilk-test"><a href="#Shapiro-Wilk-test" class="headerlink" title="Shapiro-Wilk test"></a><strong>Shapiro-Wilk test</strong></h6><p>Shapiro-Wilk检验在小样本情况下，是很普通的正态性检验方法，<code>Shapiro.test()</code>在默认安装的<strong>stats</strong>包中。原假设$H_0$: 数据符合正态分布。</p><h6 id="Lilliefor-test"><a href="#Lilliefor-test" class="headerlink" title="Lilliefor test"></a><strong>Lilliefor test</strong></h6><p>Lilliefor test是基于Kolmogorov–Smirnov test的一种正态性检验。原假设$H_0$: 数据符合正态分布，<code>lillie.test()</code>也在<strong>nortest包</strong>中。</p><h5 id="方差齐性检验"><a href="#方差齐性检验" class="headerlink" title="方差齐性检验"></a>方差齐性检验</h5><p>因为方差分析的实质是检验多个水平的均值是否有显著差异，如果各个水平的观察值方差差异太大，只检验均值之间的差异就没有意义了，所以要进行方差齐性检验。</p><p>Bartlett test可以用来检验数据的方差齐性。</p><pre><code class="hljs armasm">&gt; <span class="hljs-keyword">bartlett.test(response </span>~ trt, <span class="hljs-meta">data</span><span class="hljs-symbol">=cholesterol</span>)<span class="hljs-keyword">Bartlett </span>test of homogeneity of variances<span class="hljs-symbol">data</span>:  response <span class="hljs-keyword">by </span>trt<span class="hljs-keyword">Bartlett's </span>K-squared = <span class="hljs-number">0</span>.<span class="hljs-number">57975</span>, df = <span class="hljs-number">4</span>, p-value = <span class="hljs-number">0</span>.<span class="hljs-number">9653</span></code></pre><p>Bartlett检验表明五组的方差并没有显著不同（p=0.97）。其他检验如Fligner-Killeen检验<br>（<code>fligner.test()</code>函数）和Brown-Forsythe检验（<strong>HH包</strong>中的<code>hov()</code>函数）此处没有做演示，但它们获得的结果与Bartlett检验相同。</p><p>不过，方差齐性分析对离群点非常敏感。可利用<strong>car包</strong>中的<code>outlierTest()</code>函数来检测离群点：</p><pre><code class="hljs yaml"><span class="hljs-string">outlierTest(fit)</span><span class="hljs-literal">No</span> <span class="hljs-string">Studentized</span> <span class="hljs-string">residuals</span> <span class="hljs-string">with</span> <span class="hljs-string">Bonferroni</span> <span class="hljs-string">p</span> <span class="hljs-string">&lt;</span> <span class="hljs-number">0.05</span><span class="hljs-string">Largest</span> <span class="hljs-string">|rstudent|:</span>   <span class="hljs-string">rstudent</span> <span class="hljs-string">unadjusted</span> <span class="hljs-string">p-value</span> <span class="hljs-string">Bonferroni</span> <span class="hljs-string">p</span><span class="hljs-number">19</span> <span class="hljs-number">2.251149</span>           <span class="hljs-number">0.029422</span>           <span class="hljs-string">NA</span></code></pre><p>从输出结果来看，并没有证据说明胆固醇数据中含有离群点（当p&gt;1时将产生NA）。因此根据正态性检验、方差齐性检验和离群点检验，该数据似乎可以用ANOVA模型拟合得很好。<strong>这些方法反过来增强了我们对于所得结果的信心。</strong></p><h2 id="3-双因素方差分析"><a href="#3-双因素方差分析" class="headerlink" title="3 双因素方差分析"></a>3 双因素方差分析</h2><h3 id="3-1-推导过程"><a href="#3-1-推导过程" class="headerlink" title="3.1 推导过程"></a>3.1 推导过程</h3><p>在很多种情况下，只考虑一个指标对观察值的影响，显然是不够的，这时就会用到多因素方差分析。双因素方差分析和多因素方差分析在原理上是相似的，这里为了书写简便，我们只以双因素方差分析为例进行推导。</p><p>还是以田间实验的例子帮助理解推导过程，我们设有两个因素$A, B$，分别有$k, l$个水平（例如$A$为品种，有$k$个；$B$为播种量，考虑$l$种不同的数值，如20斤／亩，25斤／亩，……）．$A$的水平$i$与$B$的水平$j$的组合记为$(i,j)$，其试验结果记为 $Y_{ij}, i = 1, · · ·, k,j = 1,…, l$．统计模型定为</p><script type="math/tex; mode=display">Y_{ij} = \mu + a_i + b_j + e_{ij}，i= 1, · · ·, k,j = 1,· · ·, l\qquad (3.1)</script><p>为解释这模型，首先把右边分成两部分：$e_{ij}$为随机误差，它包含了未加控制的因素($A,B$以外的因素）及大量随机因素的影响．假定</p><script type="math/tex; mode=display">E(e_{ij})=0, 0<Var(e_{ij})={\sigma}^2<\infty,一切e_{ij}独立同分布\qquad(3.2)</script><p>另一部分$\mu + a_i + b_j$，它显示水平组合$(i,j)$的平均效应．它可以又分解为三部分：$\mu$是总平均（一切水平组合效应的平均），是一个基准．$a_i$表示由$A$的水平$i$带来的增加部分，称为因素$A$的水平$i$的效应．$b_j$有类似的解释．调整$\mu$的值，我们可以补充要求：</p><script type="math/tex; mode=display">a_1+···+a_k=0,b_1+···+b_l=0 \qquad (3.3)</script><p>如果$(3.3)$式不成立，则分别把$\mu$换为 $\mu + \bar{a}+\bar{b}$，$a_i$换为$a_i-\bar{a}$，$b_j$换为$b_j-\bar{b}$，则$(3.1)$式不变，而$(3.3)$式成立。</p><p>约束条件$(3.3)$给了$a_i，b_j$的意义一种更清晰的解释：$a_i&gt;0$ 表示A的水平$i$的效应在$A$的全部水平的平均效应之上，$a_i&lt;0$ 则相反。另外，这个约束条件也给了$\mu，a_i,b_j$的 一个适当的估计法：把$Y_{ij}$对一切$i,j$相加．注意到$(3.3)$，有</p><script type="math/tex; mode=display">\sum_{i=1}^{k}\sum_{j=1}^{l}Y_{ij}= kl\mu+\sum_{i=1}^{k}\sum_{j=1}^{l}e_{ij} \qquad (3.4)</script><p>由$(3.2)$得，</p><script type="math/tex; mode=display">\bar{Y}=\sum_{i=1}^{k}\sum_{j=1}^{l}Y_{ij}/kl \qquad (3.5)</script><p>是$\mu$的一个无偏估计。其次，有</p><script type="math/tex; mode=display">\sum_{j=1}^{l}Y_{ij}=l\mu+la+\sum_{j=1}^{l}e_{ij} \qquad (3.6)</script><p>于是，记</p><script type="math/tex; mode=display">\bar{Y_i}=\sum_{j=1}^{l}Y_{ij}/l, \quad \bar{Y_j}=\sum_{i=1}^{k}Y_{ij}/k \qquad (3.7)</script><p>由$(3.7)$知，$\bar{Y_j}$为$\mu+a_i$的一个无偏估计。于是得到$a_i$的一个无偏估计为</p><script type="math/tex; mode=display">\hat{a_i}=\bar{Y_i}-\bar{Y}, i=1,\cdots,k \qquad(3.8)</script><p>同理，</p><script type="math/tex; mode=display">\hat{b_j}=\bar{Y_j}-\bar{Y}, j=1,\cdots,l \qquad(3.9)</script><p>$\hat{a_i},\hat{b_j}$适合约束条件$(3.3)$。</p><p>下面进行方差分析，要设法把总平方和</p><script type="math/tex; mode=display">SS=\sum_{i=1}^{k}\sum_{j=1}^{l}(Y_{ij}-\bar{Y})^2</script><p>分解为三部分：$SS_A,SS_B,SS_e$，分别表示因素$A,B$和随机误差的影响。这种分解的主要目的是假设检验：</p><script type="math/tex; mode=display">H_{0A}:a_1=\cdots=a_k=0 \qquad(3.10)</script><p>和</p><script type="math/tex; mode=display">H_{0B}:b_1=\cdots=b_k=0 \qquad(3.11)</script><p>$H_0A$成立表示因素$A$对指标其实无影响。在实际问题中，绝对无影响的场合少见，但如影响甚小以致被随机误差所掩盖时，这种影响事实上等于没有。因此，拿$SS_A$和$SS_e$的比作为检验统计量正符合这一想法．</p><p>接下来讲一下方差分解的小技巧：</p><script type="math/tex; mode=display">Y_{ij}-\bar{Y}=(\bar{Y_i}-\bar{Y}) + (\bar{Y_j}-\bar{Y})+(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})</script><p>两边平方，对$i,j$求和，结合约束条件(3.3)，注意到</p><script type="math/tex; mode=display">\sum_{i=1}^{l}(\bar{Y_{i}}-\bar{Y})=0， \sum_{j=1}^{k}(\bar{Y_{j}}-\bar{Y})=0，</script><script type="math/tex; mode=display">\sum_{i=1}^{k}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})=\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})=0</script><script type="math/tex; mode=display">\sum_{i=1}^{k}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y}) = k\bar{Y_j}\\=\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y}) = l\bar{Y_i}</script><p>即知所有交叉积之和皆为0，而得到</p><script type="math/tex; mode=display">\begin{align}SS&=l\sum_{i=1}^{k}(\bar{Y_{i}}-\bar{Y})^2+k\sum_{j=1}^{l}(\bar{Y_{j}}-\bar{Y})^2+\sum_{i=1}^{k}\sum_{j=1}^{l}(Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y})^2 \\&=SS_A + SS_B + SS_e \qquad(3.12)\end{align}</script><p>第一个平方和可以作为因素$A$的影响的衡量，从前述$\bar{Y_{i}}-\bar{Y}$作为 $a_i$的估计可以理解第二个平方和同理。至于第三个平方和可作为随机误差的影响这一点， 直接看不甚明显。可以从两个角度去理解：在$SS$中去掉$SS_A$ 和$SS_B$后，剩余下的再没有其他系统性因素的影响，故只能作为$SS_e$。另外，由模型$(3.1)$及约束条件$(3.3)$，易知</p><script type="math/tex; mode=display">\begin{align}Y_{ij}-\bar{Y_i}-\bar{Y_j}+\bar{Y} &= (\mu + a_i + b_j + e_{ij}) - (\mu + a_i + \bar{e_{i}}) - (\mu + b_j + \bar{e_{j}} ) + (\mu + \bar{e})  \\&=e_{ij}-\bar{e_i}-\bar{e_j}+\bar{e} \qquad(3.13)\end{align}</script><p>这里面已经毫无$\mu,a_i,b_j$的影响，而只含随机误差。</p><p>得到分解式$(3.12)$后，我们就可以像单囚素情况那样，写出下面的方差分析表：<br>$SS_A , SS_B$ 自由度分别为其水平数减去1，这一点与单因素情况相同．总和自由度为全部观察值数目$kl$减去1．剩下的就是误差平方和自由度：</p><script type="math/tex; mode=display">(kl - 1) - (k - 1) - (l - 1) = (k - 1) (l - 1)</script><p><strong>表3.1 双因素方差分析表</strong></p><div class="table-container"><table><thead><tr><th>项目</th><th>$SS$</th><th>自由度</th><th>$MS$</th><th>$F$比</th><th>显著性</th></tr></thead><tbody><tr><td>$A$</td><td>$SS_A$</td><td>$k-1$</td><td>$MS_A$</td><td>$MS_A / MS_e$</td><td><em>, *</em>, 或无</td></tr><tr><td>$B$</td><td>$SS_B$</td><td>$l-1$</td><td>$MS_B$</td><td>$MS_B / MS_e$</td><td></td></tr><tr><td>误差</td><td>$SS_e$</td><td>$(k - 1) (l - 1)$</td><td>$MS_e$</td><td></td><td></td></tr><tr><td>总和</td><td>$SS$</td><td>$kl-1$</td><td></td><td></td></tr></tbody></table></div><p>还有一点要注意：在采纳模型$(3.1)$时，我们事实上引进了 一 种假定，即两因素$A,B$对指标的效应是可以叠加的．换一种方式说：因素$A$的各水平的优劣比较，与因素$B$处在哪个水平无关，反之亦然．更一般的情况是：$A,B$两因子有“交互作用 ＂ 。这时在模型(5.13)中，还要加上表示交互作用的项$c_{ij}$．这时不仅统计分析复杂化了，尤其是分析结果的解释也复杂化了．本文档暂不讨论这种情况。在一个特定的问题中，交互作用是否需要考虑，在很大程度上取决于问题的实际背景和经验．有时，通过试验数据的分析也可以看出一些问题。例如，若误差方差$\sigma^2$的估计$MS_e $反常地大，则有可能是由于交互作用所致．因为可以证明：若交互作用确实存在而未加考虑，则它的影响进入随机误差而增大了$MS_e$。</p><h3 id="3-2-代码实例"><a href="#3-2-代码实例" class="headerlink" title="3.2 代码实例"></a>3.2 代码实例</h3><p>在双因素方差分析中，受试者被分配到两因子的交叉类别组中。以基础安装中的ToothGrowth数据集为例，随机分配60只豚鼠，分别采用两种喂食方法（橙汁或维生素C），各喂食方法中抗坏血酸含量有三种水平（0.5mg、1mg或2mg），每种处理方式组合都被分配10只豚鼠，牙齿长度为因变量。</p><pre><code class="hljs angelscript">&gt; attach(ToothGrowth)&gt; table(supp, dose)    dosesupp <span class="hljs-number">0.5</span>  <span class="hljs-number">1</span>  <span class="hljs-number">2</span>  OJ  <span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>  VC  <span class="hljs-number">10</span> <span class="hljs-number">10</span> <span class="hljs-number">10</span>&gt; &gt; aggregate(len, by=list(supp, dose), FUN=mean)  Group<span class="hljs-number">.1</span> Group<span class="hljs-number">.2</span>     x<span class="hljs-number">1</span>      OJ     <span class="hljs-number">0.5</span> <span class="hljs-number">13.23</span><span class="hljs-number">2</span>      VC     <span class="hljs-number">0.5</span>  <span class="hljs-number">7.98</span><span class="hljs-number">3</span>      OJ     <span class="hljs-number">1.0</span> <span class="hljs-number">22.70</span><span class="hljs-number">4</span>      VC     <span class="hljs-number">1.0</span> <span class="hljs-number">16.77</span><span class="hljs-number">5</span>      OJ     <span class="hljs-number">2.0</span> <span class="hljs-number">26.06</span><span class="hljs-number">6</span>      VC     <span class="hljs-number">2.0</span> <span class="hljs-number">26.14</span>&gt; &gt; aggregate(len, by=list(supp, dose), FUN=sd)  Group<span class="hljs-number">.1</span> Group<span class="hljs-number">.2</span>        x<span class="hljs-number">1</span>      OJ     <span class="hljs-number">0.5</span> <span class="hljs-number">4.459709</span><span class="hljs-number">2</span>      VC     <span class="hljs-number">0.5</span> <span class="hljs-number">2.746634</span><span class="hljs-number">3</span>      OJ     <span class="hljs-number">1.0</span> <span class="hljs-number">3.910953</span><span class="hljs-number">4</span>      VC     <span class="hljs-number">1.0</span> <span class="hljs-number">2.515309</span><span class="hljs-number">5</span>      OJ     <span class="hljs-number">2.0</span> <span class="hljs-number">2.655058</span><span class="hljs-number">6</span>      VC     <span class="hljs-number">2.0</span> <span class="hljs-number">4.797731</span>&gt; &gt; dose &lt;- factor(dose)#dose变量被转换为因子变量，这样aov()函数就会将它当做一个分组变量，而不是一个数值型协变量&gt; # condider <span class="hljs-built_in">int</span>eractive factor&gt; fit &lt;- aov(len ~ supp*dose)&gt; summary(fit)            Df Sum Sq Mean Sq F value   Pr(&gt;F)    supp         <span class="hljs-number">1</span>  <span class="hljs-number">205.4</span>   <span class="hljs-number">205.4</span>  <span class="hljs-number">15.572</span> <span class="hljs-number">0.000231</span> ***dose         <span class="hljs-number">2</span> <span class="hljs-number">2426.4</span>  <span class="hljs-number">1213.2</span>  <span class="hljs-number">92.000</span>  &lt; <span class="hljs-number">2e-16</span> ***supp:dose    <span class="hljs-number">2</span>  <span class="hljs-number">108.3</span>    <span class="hljs-number">54.2</span>   <span class="hljs-number">4.107</span> <span class="hljs-number">0.021860</span> *  Residuals   <span class="hljs-number">54</span>  <span class="hljs-number">712.1</span>    <span class="hljs-number">13.2</span>                     ---Signif. codes:  <span class="hljs-number">0</span> ‘***’ <span class="hljs-number">0.001</span> ‘**’ <span class="hljs-number">0.01</span> ‘*’ <span class="hljs-number">0.05</span> ‘.’ <span class="hljs-number">0.1</span> ‘ ’ <span class="hljs-number">1</span></code></pre><p>计算结果表明，主效应和交互效应都是显著的。</p><p>有多种方式对结果进行可视化处理。此处可用interaction.plot()函数来展示双因素方<br>差分析的交互效应。</p><pre><code class="hljs routeros"><span class="hljs-comment"># interactive effect</span>interaction.plot(dose, supp, len, <span class="hljs-attribute">type</span>=<span class="hljs-string">"b"</span>,                 <span class="hljs-attribute">col</span>=c("red","blue"), <span class="hljs-attribute">pch</span>=c(16, 18),                 main = <span class="hljs-string">"Interaction between Dose and Supplement Type"</span>)</code></pre><p><img src="/article/2ad44fb9/3-1.png" srcset="/img/loading.gif" alt="3-1"></p><p><strong>图3-1 各种剂量喂食下豚鼠牙齿长度的均值（interaction.plot()函数绘制）</strong></p><p>还可以用gplots包中的plotmeans()函数来展示交互效应。<img src="/article/2ad44fb9/3-2.png" srcset="/img/loading.gif" alt="3-2"><strong>图3-2 喂食方法和剂量对牙齿生长的交互作用。用plotmeans()函数绘制的95%的置</strong><br><strong>信区间的牙齿长度均值</strong></p><p>图形展示了均值、误差棒（95%的置信区间）和样本大小。<br>最后，你还能用HH包中的interaction2wt()函数来可视化结果，图形对任意顺序的因子设计的主效应和交互效应都会进行展示（图3-3）。</p><pre><code class="hljs lisp">library(<span class="hljs-name">HH</span>)interaction2wt(<span class="hljs-name">len</span>~supp*dose)&gt; &gt; detach(<span class="hljs-name">ToothGrowth</span>)</code></pre><p><img src="/article/2ad44fb9/3-3.png" srcset="/img/loading.gif" alt="3-3"></p><p><strong>图3-3 ToothGrowth数据集的主效应和交互效应。图形由interaction2wt()函数创建</strong></p><p>以上三幅图形都表明随着橙汁和维生素C中的抗坏血酸剂量的增加，牙齿长度变长。对于0.5mg和1mg剂量，橙汁比维生素C更能促进牙齿生长；对于2mg剂量的抗坏血酸，两种喂食方法下牙齿长度增长相同。</p><h4 id="参考书目："><a href="#参考书目：" class="headerlink" title="参考书目："></a>参考书目：</h4><ol><li>陈希孺，概率论与数理统计</li><li>Robert I. Kabacoff, R in Action.</li></ol>]]></content>
    
    
    <categories>
      
      <category>概率统计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第九章-时序数据</title>
    <link href="/article/32af8df.html"/>
    <url>/article/32af8df.html</url>
    
    <content type="html"><![CDATA[<p><img src="/article/32af8df/时序数据.png" srcset="/img/loading.gif" alt="时序数据"></p><a id="more"></a>明天要考试了--明后天再补全了~回答有些过于简陋了问题1(a) <pre><code class="hljs prolog">df = pd.read_csv(<span class="hljs-string">'data/time_series_one.csv'</span>, parse_dates=[<span class="hljs-string">'日期'</span>])df[<span class="hljs-string">'日期'</span>].dt.dayofweek[df[<span class="hljs-string">'销售额'</span>].idxmax()]</code></pre><p>输出：</p><p>6<br>（b）</p><pre><code class="hljs routeros">holiday = pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20170501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20170503'</span>).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20171001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20171007'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20180215'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20180221'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20180501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20180503'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20181001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20181007'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20190204'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20190224'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20190501'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20190503'</span>)).append(          pd.date_range(<span class="hljs-attribute">start</span>=<span class="hljs-string">'20191001'</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">'20191007'</span>))result = df[~df[<span class="hljs-string">'日期'</span>].isin(holiday)].set_index(<span class="hljs-string">'日期'</span>).resample(<span class="hljs-string">'MS'</span>).sum()result.head()</code></pre><p>(c)</p><pre><code class="hljs sas">result = df[df[<span class="hljs-string">'日期'</span>].dt.dayofweek.<span class="hljs-meta">isin(</span>[5,6])].<span class="hljs-meta">set</span><span class="hljs-meta">_index(</span><span class="hljs-string">'日期'</span>).resample(<span class="hljs-string">'QS'</span>)<span class="hljs-meta">.sum(</span>)result.head()</code></pre>]]></content>
    
    
    <categories>
      
      <category>pandas下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3常见分布与假设检验</title>
    <link href="/article/7c20a36d.html"/>
    <url>/article/7c20a36d.html</url>
    
    <content type="html"><![CDATA[<h2 id="1-一般随机变量"><a href="#1-一般随机变量" class="headerlink" title="1 一般随机变量"></a>1 一般随机变量</h2><h3 id="1-1-随机变量的两种类型"><a href="#1-1-随机变量的两种类型" class="headerlink" title="1.1 随机变量的两种类型"></a>1.1 随机变量的两种类型</h3><p>根据随机变量可能取值的个数分为离散型（取值有限）和连续型（取值无限）两类。</p><h3 id="1-2-离散型随机变量"><a href="#1-2-离散型随机变量" class="headerlink" title="1.2 离散型随机变量"></a>1.2 离散型随机变量</h3><p>对于离散型随机变量，使用概率质量函数（probability mass function），简称PMF，来描述其分布律。</p><p>假定离散型随机变量X，共有n个取值，$X_1$, $X_2$, ……, $X_n$, 那么</p><script type="math/tex; mode=display">P(X=X_n) \geq 0</script><script type="math/tex; mode=display">\Sigma_{1}^{n} P(X=X_n) =1</script><p>用到PMF的例子：二项分布，泊松分布<a id="more"></a></p><h3 id="1-3-连续型随机变量"><a href="#1-3-连续型随机变量" class="headerlink" title="1.3 连续型随机变量"></a>1.3 连续型随机变量</h3><p>对于连续型随机变量，使用概率密度函数（probability density function），简称PDF，来描述其分布情况。</p><p>连续型随机变量的特点在于取任何固定值的概率都为0，因此讨论其在特定值上的概率是没有意义的，应当讨论其在某一个区间范围内的概率，这就用到了概率密度函数的概念。</p><p>假定连续型随机变量X，f(x)为概率密度函数， 对于任意实数范围如[a,b]，有</p><script type="math/tex; mode=display">P \lbrace a\leq X \leq b\rbrace = \int ^b_a f(x) {\rm d}x</script><p>用到PDF的例子：均匀分布，正态分布，指数分布</p><p>对于连续型随机变量，通常还会用到累积分布函数 (cumulative distribution function)，简称CDF，来描述其性质，在数学上CDF是PDF的积分形式。</p><p>分布函数F(x)在点x处的函数值表示X落在区间(−∞,x]内的概率，所以分布函数就是定义域为R的一个普通函数，因此我们可以把概率问题转化为函数问题，从而可以利用普通的函数知识来研究概率问题，增大了概率的研究范围。</p><h2 id="2-常见分布"><a href="#2-常见分布" class="headerlink" title="2 常见分布"></a>2 常见分布</h2><p>本节通过一些实际例子来认识各种不同的分布及其应用场景</p><h3 id="2-1-离散型分布"><a href="#2-1-离散型分布" class="headerlink" title="2.1 离散型分布"></a>2.1 离散型分布</h3><h4 id="2-1-1-二项分布（Binomial-distribution）"><a href="#2-1-1-二项分布（Binomial-distribution）" class="headerlink" title="2.1.1 二项分布（Binomial distribution）"></a>2.1.1 二项分布（Binomial distribution）</h4><p>二项分布可以认为是一种只有两种结果（成功/失败)的单次试验重复多次后成功次数的分布概率。</p><p>二项分布需要满足以下条件：</p><ul><li>试验次数是固定的</li><li>每次试验都是独立的</li><li>对于每次试验成功的概率都是一样的</li></ul><p>一些二项分布的例子：</p><ul><li>销售电话成功的次数</li><li>一批产品中有缺陷的产品数量</li><li>掷硬币正面朝上的次数</li><li>在一袋糖果中取糖果吃，拿到红色包装的次数</li></ul><p>在n次试验中，单次试验成功率为p，失败率q=1-p，则出现成功次数的概率为</p><script type="math/tex; mode=display">P(X=x) = C_n^x p^x q^{n-x}</script><h4 id="2-1-2-泊松分布（Poisson-distribution）"><a href="#2-1-2-泊松分布（Poisson-distribution）" class="headerlink" title="2.1.2 泊松分布（Poisson distribution）"></a>2.1.2 泊松分布（Poisson distribution）</h4><p>泊松分布是用来描述泊松试验的一种分布，满足以下两个特征的试验可以认为是泊松试验：</p><ul><li>所考察的事件在任意两个长度相等的区间里发生一次的机会均等</li><li>所考察的事件在任何一个区间里发生与否和在其他区间里发生与否没有相互影响，即是独立的</li></ul><p>泊松分布需要满足一些条件：</p><ul><li>试验次数n趋向于无穷大</li><li>单次事件发生的概率p趋向于0</li><li>np是一个有限的数值</li></ul><p>泊松分布的一些例子：</p><ul><li>一定时间段内，某航空公司接到的订票电话数</li><li>一定时间内，到车站等候公交汽车的人数</li><li>一匹布上发现的瑕疵点的个数</li><li>一定页数的书刊上出现的错别字个数</li></ul><p>一个服从泊松分布的随机变量X，在具有比率参数（rate parameter）λ （λ=np）的一段固定时间间隔内，事件发生次数为i的概率为</p><script type="math/tex; mode=display">P\lbrace X= i \rbrace = e^{-λ} \frac{λ^i}{i!}</script><h4 id="2-1-3-二项分布，泊松分布，正态分布的关系"><a href="#2-1-3-二项分布，泊松分布，正态分布的关系" class="headerlink" title="2.1.3 二项分布，泊松分布，正态分布的关系"></a>2.1.3 二项分布，泊松分布，正态分布的关系</h4><p>这三个分布之间具有非常微妙的关联。</p><p>当n很大，p很小时，如<em>n</em> ≥ 100 and <em>np</em> ≤ 10时，二项分布可以近似为泊松分布。</p><p>当λ很大时，如λ≥1000时，泊松分布可以近似为正态分布。</p><p>当n很大时，np和n(1-p)都足够大时，如n ≥ 100 , np  ≥10，n(1-p) ≥10时，二项分布可以近似为正态分布。</p><h4 id="2-1-4-其他离散型随机分布"><a href="#2-1-4-其他离散型随机分布" class="headerlink" title="2.1.4 其他离散型随机分布"></a>2.1.4 其他离散型随机分布</h4><p>除了二项分布和泊松分布以外，还有其他一些不太常用的离散型分布。</p><h5 id="几何分布（Geometric-distribution）"><a href="#几何分布（Geometric-distribution）" class="headerlink" title="几何分布（Geometric distribution）"></a>几何分布（Geometric distribution）</h5><p>考虑独立重复试验，几何分布描述的是经过k次试验才首次获得成功的概率，假定每次成功率为p，</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = {(1-p)}^{n-1} p</script><h5 id="负二项分布（Negative-binomial-distribution）"><a href="#负二项分布（Negative-binomial-distribution）" class="headerlink" title="负二项分布（Negative binomial distribution）"></a>负二项分布（Negative binomial distribution）</h5><p>考虑独立重复试验，负二项分布描述的是试验一直进行到成功r次的概率，假定每次成功率为p，</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = C_{n-1}^{r-1} p^r {(1-p)}^{n-r}</script><h5 id="超几何分布（Hypergeometric-Distribution）"><a href="#超几何分布（Hypergeometric-Distribution）" class="headerlink" title="超几何分布（Hypergeometric Distribution）"></a>超几何分布（Hypergeometric Distribution）</h5><p>超几何分布描述的是在一个总数为N的总体中进行有放回地抽样，其中在总体中k个元素属于一组，剩余N-k个元素属于另一组，假定从总体中抽取n次，其中包含x个第一组的概率为</p><script type="math/tex; mode=display">P\lbrace X= n \rbrace = \frac {C_{k}^{x} C_{N-k}^{n-x}} {C_{N}^{n}}</script><h3 id="2-2-连续型分布"><a href="#2-2-连续型分布" class="headerlink" title="2.2 连续型分布"></a>2.2 连续型分布</h3><h4 id="2-2-1-均匀分布-（Uniform-distribution）"><a href="#2-2-1-均匀分布-（Uniform-distribution）" class="headerlink" title="2.2.1 均匀分布 （Uniform distribution）"></a>2.2.1 均匀分布 （Uniform distribution）</h4><p>均匀分布指的是一类在定义域内概率密度函数处处相等的统计分布。</p><p>若X是服从区间[a,b]上的均匀分布，则记作X~U[a,b]。</p><p>均匀分布X的概率密度函数为</p><script type="math/tex; mode=display">f(x)=\begin{cases}\frac {1} {b-a} ,  &  a \leq x  \leq b \\0, & others\end{cases}</script><p>分布函数为</p><script type="math/tex; mode=display">F(x)=\begin{cases}0 ,  &  x< a \\(x-a)(b-a), & a \leq x  \leq b \\1, & x>b\end{cases}</script><p>均匀分布的一些例子：</p><ul><li>一个理想的随机数生成器</li><li>一个理想的圆盘以一定力度旋转后静止时的角度</li></ul><h4 id="2-2-2-正态分布-（Normal-distribution）"><a href="#2-2-2-正态分布-（Normal-distribution）" class="headerlink" title="2.2.2 正态分布 （Normal distribution）"></a>2.2.2 正态分布 （Normal distribution）</h4><p>正态分布，也叫做高斯分布，是最为常见的统计分布之一，是一种对称的分布，概率密度呈现钟摆的形状，其概率密度函数为</p><script type="math/tex; mode=display">f(x)=\frac{1}{\sqrt{2π}\sigma}e^{\frac{-(x-u)^2}{2\sigma^2}}</script><p>记为X ~ N(μ, $σ^2$) , 其中μ为正态分布的均值，σ为正态分布的标准差</p><p>有了一般正态分布后，可以通过公式变换将其转变为标准正态分布 Z ~ N(0,1)，</p><script type="math/tex; mode=display">Z=\frac {X-μ} {σ}</script><p>正态分布的一些例子：</p><ul><li>成人的身高</li><li>不同方向的气体分子的运动速度</li><li>测量物体质量时的误差</li></ul><p>正态分布在现实生活有着非常多的例子，这一点可以从中心极限定理来解释，中心极限定理说的是一组独立同分布的随机样本的平均值近似为正态分布，无论随机变量的总体符合何种分布。</p><h4 id="2-2-3-指数分布-（Exponential-distribution）"><a href="#2-2-3-指数分布-（Exponential-distribution）" class="headerlink" title="2.2.3 指数分布 （Exponential distribution）"></a>2.2.3 指数分布 （Exponential distribution）</h4><p>指数分布通常被广泛用在描述一个特定事件发生所需要的时间，在指数分布随机变量的分布中，有着很少的大数值和非常多的小数值。</p><p>指数分布的概率密度函数为</p><script type="math/tex; mode=display">f(x)=\begin{cases}λe^{-λx} ,  &   x  \geq 0 \\0, & x < 0\end{cases}</script><p>记为 X~E（λ),   其中λ被称为率参数（rate parameter），表示每单位时间发生该事件的次数。</p><p>分布函数为</p><script type="math/tex; mode=display">F(a) = P\{X \leq a\} = 1-e^{-λa},  a\geq 0</script><p>指数分布的一些例子：</p><ul><li>顾客到达一家店铺的时间间隔</li><li>从现在开始到发生地震的时间间隔</li><li>在产线上收到一个问题产品的时间间隔</li></ul><p>关于指数分布还有一个有趣的性质的是指数分布是无记忆性的，假定在等候事件发生的过程中已经过了一些时间，此时距离下一次事件发生的时间间隔的分布情况和最开始是完全一样的，就好像中间等候的那一段时间完全没有发生一样，也不会对结果有任何影响，用数学语言来表述是</p><script type="math/tex; mode=display">P\{X>s+t | X> t\} =P\{X>s\}</script><h4 id="2-2-4-其他连续分布"><a href="#2-2-4-其他连续分布" class="headerlink" title="2.2.4 其他连续分布"></a>2.2.4 其他连续分布</h4><p><strong>$\Gamma$分布</strong></p><p>常用来描述某个事件总共要发生n次的等待时间的分布</p><p><strong>威布尔分布 （Weibull distribution）</strong></p><p>常用来描述在工程领域中某类具有“最弱链”对象的寿命</p><h3 id="2-3-常见分布的均值和方差汇总"><a href="#2-3-常见分布的均值和方差汇总" class="headerlink" title="2.3 常见分布的均值和方差汇总"></a>2.3 常见分布的均值和方差汇总</h3><p>离散型分布</p><p><img src="/article/7c20a36d/discreteoverall.PNG" srcset="/img/loading.gif" alt="discreteoverall"></p><p>连续型分布</p><p><img src="/article/7c20a36d/continuousoverall.PNG" srcset="/img/loading.gif" alt="continuousoverall"></p><p>图片来自于 [Statistical Inference by Casella and Berger]</p><h3 id="2-4-Python-代码实战"><a href="#2-4-Python-代码实战" class="headerlink" title="2.4 Python 代码实战"></a>2.4 Python 代码实战</h3><h4 id="2-4-1-生成一组符合特定分布的随机数"><a href="#2-4-1-生成一组符合特定分布的随机数" class="headerlink" title="2.4.1 生成一组符合特定分布的随机数"></a>2.4.1 生成一组符合特定分布的随机数</h4><p>在Numpy库中，提供了一组random类可以生成特定分布的随机数</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy<span class="hljs-comment"># 生成大小为1000的符合b(10,0.5)二项分布的样本集</span>s = numpy.random.binomial(n=<span class="hljs-number">10</span>,p=<span class="hljs-number">0.5</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合P(1)的泊松分布的样本集</span>s = numpy.random.poisson(lam=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合U(0,1)均匀分布的样本集，注意在此方法中边界值为左闭右开区间</span>s = numpy.random.uniform(low=<span class="hljs-number">0</span>,high=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合N(0,1)正态分布的样本集，可以用normal函数自定义均值，标准差，也可以直接使用standard_normal函数</span>s = numpy.random.normal(loc=<span class="hljs-number">0</span>,scale=<span class="hljs-number">1</span>,size=<span class="hljs-number">1000</span>)s = numpy.random.standard_normal(size=<span class="hljs-number">1000</span>)<span class="hljs-comment"># 生成大小为1000的符合E(1/2)指数分布的样本集，注意该方法中的参数为指数分布参数λ的倒数</span>s = numpy.random.exponential(scale=<span class="hljs-number">2</span>,size=<span class="hljs-number">1000</span>)</code></pre><p>除了Numpy，Scipy也提供了一组生成特定分布随机数的方法</p><pre><code class="hljs python"><span class="hljs-comment"># 以均匀分布为例，rvs可用来生成一组随机变量的值</span><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> statsstats.uniform.rvs(size=<span class="hljs-number">10</span>)</code></pre><h4 id="2-4-2-计算统计分布的PMF和PDF"><a href="#2-4-2-计算统计分布的PMF和PDF" class="headerlink" title="2.4.2 计算统计分布的PMF和PDF"></a>2.4.2 计算统计分布的PMF和PDF</h4><p>Scipy库提供了一组用于计算离散型随机变量PMF和连续型随机变量PDF的方法。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-comment"># 计算二项分布B(10,0.5)的PMF</span>x=range(<span class="hljs-number">11</span>)p=stats.binom.pmf(x, n=<span class="hljs-number">10</span>, p=<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 计算泊松分布P(1)的PMF</span>x=range(<span class="hljs-number">11</span>)p=stats.poisson.pmf(x, mu=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算均匀分布U(0,1)的PDF</span>x = numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)p= stats.uniform.pdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算正态分布N(0,1)的PDF</span>x = numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)p= stats.norm.pdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)<span class="hljs-comment"># 计算指数分布E(1)的PDF</span>x = numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">1000</span>)p= stats.expon.pdf(x,loc=<span class="hljs-number">0</span>,scale=<span class="hljs-number">1</span>)</code></pre><h4 id="2-4-3-计算统计分布的CDF"><a href="#2-4-3-计算统计分布的CDF" class="headerlink" title="2.4.3 计算统计分布的CDF"></a>2.4.3 计算统计分布的CDF</h4><p>类似计算概率质量/密度函数的方法，只需将上节中的pmf或pdf替换为cdf，即可得到分布函数的值</p><pre><code class="hljs python"><span class="hljs-comment"># 以正态分布为例，计算正态分布N(0,1)的CDF</span>x = numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)p = stats.norm.cdf(x,loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)</code></pre><h4 id="2-4-4-统计分布可视化"><a href="#2-4-4-统计分布可视化" class="headerlink" title="2.4.4 统计分布可视化"></a>2.4.4 统计分布可视化</h4><h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a><strong>二项分布</strong></h5><p>比较n=10，p=0.5的二项分布的真实概率质量和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx = range(<span class="hljs-number">11</span>)  <span class="hljs-comment"># 二项分布成功的次数（X轴）</span>t = stats.binom.rvs(<span class="hljs-number">10</span>,<span class="hljs-number">0.5</span>,size=<span class="hljs-number">10000</span>) <span class="hljs-comment"># B(10,0.5)随机抽样10000次</span>p = stats.binom.pmf(x, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>) <span class="hljs-comment"># B(10,0.5)真实概率质量</span>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.scatterplot(x,p,color=<span class="hljs-string">'purple'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Binomial distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/binomial.png" srcset="/img/loading.gif" alt="binomial"></p><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a><strong>泊松分布</strong></h5><p>比较λ=2的泊松分布的真实概率质量和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=range(<span class="hljs-number">11</span>)t= stats.poisson.rvs(<span class="hljs-number">2</span>,size=<span class="hljs-number">10000</span>)p=stats.poisson.pmf(x, <span class="hljs-number">2</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.scatterplot(x,p,color=<span class="hljs-string">'purple'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Poisson distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/poisson.png" srcset="/img/loading.gif" alt="poisson"></p><p>比较不同参数λ对应的概率质量函数，可以验证随着参数增大，泊松分布开始逐渐变得对称，分布也越来越均匀，趋近于正态分布</p><pre><code class="hljs python">x=range(<span class="hljs-number">50</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  lam <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">20</span>] :        p=stats.poisson.pmf(x, lam)        sns.lineplot(x,p,label=<span class="hljs-string">'lamda= '</span>+ str(lam))plt.title(<span class="hljs-string">'Poisson distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/poisson2.png" srcset="/img/loading.gif" alt="poisson2"></p><h5 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a><strong>均匀分布</strong></h5><p>比较U(0,1)的均匀分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">100</span>)t= stats.uniform.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.uniform.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">10</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Uniforml distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/uniform.png" srcset="/img/loading.gif" alt="uniform"></p><h5 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a><strong>正态分布</strong></h5><p>比较N(0,1)的正态分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">100</span>)t= stats.norm.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.norm.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">100</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Normal distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1.05</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/normal.png" srcset="/img/loading.gif" alt="normal"></p><p>比较不同均值和标准差组合的正态分布的概率密度函数</p><pre><code class="hljs python">x=numpy.linspace(<span class="hljs-number">-6</span>,<span class="hljs-number">6</span>,<span class="hljs-number">100</span>)p=stats.norm.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  mean, std <span class="hljs-keyword">in</span> [(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>),(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>)]:         p=stats.norm.pdf(x, mean, std)        sns.lineplot(x,p,label=<span class="hljs-string">'Mean: '</span>+ str(mean) + <span class="hljs-string">', std: '</span>+ str(std))plt.title(<span class="hljs-string">'Normal distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/uniform2.png" srcset="/img/loading.gif" alt="uniform2"></p><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a><strong>指数分布</strong></h5><p>比较E(1)的指数分布的真实概率密度和10000次随机抽样的结果</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> snsx=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>)t= stats.expon.rvs(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,size=<span class="hljs-number">10000</span>)p=stats.expon.pdf(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)sns.distplot(t,bins=<span class="hljs-number">100</span>,hist_kws=&#123;<span class="hljs-string">'density'</span>:<span class="hljs-literal">True</span>&#125;, kde=<span class="hljs-literal">False</span>,label = <span class="hljs-string">'Distplot from 10000 samples'</span>)sns.lineplot(x,p,color=<span class="hljs-string">'purple'</span>,label=<span class="hljs-string">'True mass density'</span>)plt.title(<span class="hljs-string">'Exponential distribution'</span>)plt.legend(bbox_to_anchor=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))</code></pre><p><img src="/article/7c20a36d/expo.png" srcset="/img/loading.gif" alt="expo"></p><p>比较不同参数的指数分布的概率密度函数</p><pre><code class="hljs python">x=numpy.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">100</span>)fig, ax = plt.subplots()<span class="hljs-keyword">for</span>  scale <span class="hljs-keyword">in</span> [<span class="hljs-number">0.2</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>] :        p=stats.expon.pdf(x, scale=scale)        sns.lineplot(x,p,label=<span class="hljs-string">'lamda= '</span>+ str(<span class="hljs-number">1</span>/scale))plt.title(<span class="hljs-string">'Exponential distribution'</span>)plt.legend()</code></pre><p><img src="/article/7c20a36d/expo2.png" srcset="/img/loading.gif" alt="expo2"></p><h2 id="3-假设检验"><a href="#3-假设检验" class="headerlink" title="3 假设检验"></a>3 假设检验</h2><h3 id="3-1-基本概念"><a href="#3-1-基本概念" class="headerlink" title="3.1 基本概念"></a>3.1 基本概念</h3><p>假设检验问题是统计推断中的一类重要问题，在总体的分布函数完全未知或只知其形式，不知其参数的情况，为了推断总体的某些未知特性，提出某些关于总体的假设，这类问题被称为假设检验。</p><h3 id="3-2-基本步骤"><a href="#3-2-基本步骤" class="headerlink" title="3.2 基本步骤"></a>3.2 基本步骤</h3><p>一个假设检验问题可以分为5步，无论细节如果变化，都一定会遵循这5个步骤。</p><ol><li>陈述研究假设，包含原假设（null hypothesis）和备择假设（alternate hypothesis）</li><li>为验证假设收集数据</li><li>构造合适的统计测试量并测试</li><li>决定是接受还是拒绝原假设</li><li>展示结论</li></ol><p>步骤1：</p><p>通常来说，我们会把原假设的描述写成变量之间不存在某种差异，或不存在某种关联，备择假设则为存在某种差异或关联。</p><p>例如，原假设：男人和女人的平均身高没有差别， 备择假设男人和女人的平均身高存在显著差别。</p><p>步骤2:</p><p>为了统计检验的结果真实可靠，需要根据实际的假设命题从总体中抽取样本，要求抽样的数据要具有代表性，例如在上述男女平均身高的命题中，抽取的样本要能覆盖到各类社会阶级，各个国家等所有可能影响到身高的因素。</p><p>步骤3：</p><p>统计检验量有很多种类，但是所有的统计检验都是基于组内方差和组间方差的比较，如果组间方差足够大，使得不同组之间几乎没有重叠，那么统计量会反映出一个非常小的P值，意味着不同组之间的差异不可能是由偶然性导致的。</p><p>步骤4：</p><p>基于统计量的结果做出接受或拒绝原假设的判断，通常我们会以P=0.05作为临界值（单侧检验）。</p><p>步骤5：</p><p>展示结论。</p><h3 id="3-3-统计量的选择"><a href="#3-3-统计量的选择" class="headerlink" title="3.3 统计量的选择"></a>3.3 统计量的选择</h3><p>选择合适的统计量是进行假设检验的关键步骤，最常用的统计检验包括回归检验(regression test)，比较检验(comparison test)和关联检验(correlation test)三类。</p><p><strong>回归检验</strong></p><p>回归检验适用于预测变量是数值型的情况，根据预测变量的数量和结果变量的类型又分为以下几种。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">预测变量</th><th style="text-align:center">结果变量</th></tr></thead><tbody><tr><td style="text-align:center">简单线性回归</td><td style="text-align:center">单个，连续数值</td><td style="text-align:center">连续数值</td></tr><tr><td style="text-align:center">多重线性回归</td><td style="text-align:center">多个，连续数值</td><td style="text-align:center">连续数值</td></tr><tr><td style="text-align:center">Logistic回归</td><td style="text-align:center">连续数值</td><td style="text-align:center">二元类别</td></tr></tbody></table></div><p><strong>比较检验</strong></p><p>比较检验适用于预测变量是类别型，结果变量是数值型的情况，根据预测变量的分组数量和结果变量的数量又可以分为以下几种。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">预测变量</th><th style="text-align:center">结果变量</th></tr></thead><tbody><tr><td style="text-align:center">Paired t-test</td><td style="text-align:center">两组，类别</td><td style="text-align:center">组来自同一总体，数值</td></tr><tr><td style="text-align:center">Independent t-test</td><td style="text-align:center">两组，类别</td><td style="text-align:center">组来自不同总体，数值</td></tr><tr><td style="text-align:center">ANOVA</td><td style="text-align:center">两组及以上，类别</td><td style="text-align:center">单个，数值</td></tr><tr><td style="text-align:center">MANOVA</td><td style="text-align:center">两组及以上，类别</td><td style="text-align:center">两个及以上，数值</td></tr></tbody></table></div><p><strong>关联检验</strong></p><p>关联检验常用的只有卡方检验一种，适用于预测变量和结果变量均为类别型的情况。</p><p><strong>非参数检验</strong></p><p>此外，由于一般来说上述参数检验都需满足一些前提条件，样本之间独立，不同组的组内方差近似和数据满足正态性，所以当这些条件不满足的时候，我们可以尝试用非参数检验来代替参数检验。</p><div class="table-container"><table><thead><tr><th style="text-align:center">非参数检验</th><th style="text-align:center">用于替代的参数检验</th></tr></thead><tbody><tr><td style="text-align:center">Spearman</td><td style="text-align:center">回归和关联检验</td></tr><tr><td style="text-align:center">Sign test</td><td style="text-align:center">T-test</td></tr><tr><td style="text-align:center">Kruskal–Wallis</td><td style="text-align:center">ANOVA</td></tr><tr><td style="text-align:center">ANOSIM</td><td style="text-align:center">MANOVA</td></tr><tr><td style="text-align:center">Wilcoxon Rank-Sum test</td><td style="text-align:center">Independent t-test</td></tr><tr><td style="text-align:center">Wilcoxon Signed-rank test</td><td style="text-align:center">Paired t-test</td></tr><tr><td style="text-align:center"></td></tr></tbody></table></div><h3 id="3-4-两类错误"><a href="#3-4-两类错误" class="headerlink" title="3.4 两类错误"></a>3.4 两类错误</h3><p>事实上当我们进行假设检验的过程中是存在犯错误的可能的，并且理论上来说错误是无法完全避免的。根据定义，错误分为两类，一类错误（type I error）和二类错误（type II error）。</p><ul><li><p>一类错误：拒绝真的原假设</p></li><li><p>二类错误：接受错误的原假设</p></li></ul><p>一类错误可以通过α值来控制，在假设检验中选择的 α（显著性水平）对一类错误有着直接影响。α可以认为是我们犯一类错误的最大可能性。以95%的置信水平为例，a=0.05，这意味着我们拒绝一个真的原假设的可能性是5%。从长期来看，每做20次假设检验会有一次犯一类错误的事件发生。</p><p>二类错误通常是由小样本或高样本方差导致的，二类错误的概率可以用β来表示，和一类错误不同的是，此类错误是不能通过设置一个错误率来直接控制的。对于二类错误，可以从功效的角度来估计，首先进行功效分析（power analysis）计算出功效值1-β，进而得到二类错误的估计值β。</p><p>一般来说这两类错误是无法同时降低的，在降低犯一类错误的前提下会增加犯二类错误的可能性，在实际案例中如何平衡这两类错误取决于我们更能接受一类错误还是二类错误。</p><h3 id="3-5-Python代码实战"><a href="#3-5-Python代码实战" class="headerlink" title="3.5 Python代码实战"></a>3.5 Python代码实战</h3><p>本节通过一些例子来讲解如何使用python进行假设检验。</p><h4 id="3-5-1-正态检验"><a href="#3-5-1-正态检验" class="headerlink" title="3.5.1 正态检验"></a>3.5.1 正态检验</h4><p>Shapiro-Wilk Test是一种经典的正态检验方法。</p><p>H0: 样本总体服从正态分布</p><p>H1: 样本总体不服从正态分布 </p><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> shapirodata_nonnormal = np.random.exponential(size=<span class="hljs-number">100</span>)data_normal = np.random.normal(size=<span class="hljs-number">100</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normal_judge</span><span class="hljs-params">(data)</span>:</span>stat, p = shapiro(data)<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:<span class="hljs-keyword">return</span> <span class="hljs-string">'stat=&#123;:.3f&#125;, p = &#123;:.3f&#125;, probably gaussian'</span>.format(stat,p)<span class="hljs-keyword">else</span>:<span class="hljs-keyword">return</span> <span class="hljs-string">'stat=&#123;:.3f&#125;, p = &#123;:.3f&#125;, probably not gaussian'</span>.format(stat,p)<span class="hljs-comment"># output</span>normal_judge(data_nonnormal)<span class="hljs-comment"># 'stat=0.850, p = 0.000, probably not gaussian'</span>normal_judge(data_normal)<span class="hljs-comment"># 'stat=0.987, p = 0.415, probably gaussian'</span></code></pre><h4 id="3-5-2-卡方检验"><a href="#3-5-2-卡方检验" class="headerlink" title="3.5.2 卡方检验"></a>3.5.2 卡方检验</h4><p>目的：检验两组类别变量是相关的还是独立的</p><p>H0: 两个样本是独立的</p><p>H1: 两组样本不是独立的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> chi2_contingencytable = [[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],[<span class="hljs-number">6</span>,  <span class="hljs-number">9</span>,  <span class="hljs-number">17</span>]]stat, p, dof, expected = chi2_contingency(table)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably independent'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably dependent'</span>) <span class="hljs-comment"># output</span><span class="hljs-comment">#stat=0.272, p=0.873</span><span class="hljs-comment">#Probably independent</span></code></pre><h4 id="3-5-3-T-test"><a href="#3-5-3-T-test" class="headerlink" title="3.5.3 T-test"></a>3.5.3 T-test</h4><p>目的：检验两个独立样本集的均值是否具有显著差异</p><p>H0: 均值是相等的</p><p>H1: 均值是不等的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> ttest_ind<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata1 = np.random.normal(size=<span class="hljs-number">10</span>)data2 = np.random.normal(size=<span class="hljs-number">10</span>)stat, p = ttest_ind(data1, data2)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>)    <span class="hljs-comment"># output</span><span class="hljs-comment"># stat=-1.382, p=0.184</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h4 id="3-5-4-ANOVA"><a href="#3-5-4-ANOVA" class="headerlink" title="3.5.4 ANOVA"></a>3.5.4 ANOVA</h4><p>目的：与t-test类似，ANOVA可以检验两组及以上独立样本集的均值是否具有显著差异</p><p>H0: 均值是相等的</p><p>H1: 均值是不等的</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> f_oneway<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdata1 = np.random.normal(size=<span class="hljs-number">10</span>)data2 = np.random.normal(size=<span class="hljs-number">10</span>)data3 = np.random.normal(size=<span class="hljs-number">10</span>)stat, p = f_oneway(data1, data2, data3)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>) <span class="hljs-comment"># output</span><span class="hljs-comment"># stat=0.189, p=0.829</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h4 id="3-5-5-Mann-Whitney-U-Test"><a href="#3-5-5-Mann-Whitney-U-Test" class="headerlink" title="3.5.5 Mann-Whitney U Test"></a>3.5.5 Mann-Whitney U Test</h4><p>目的：检验两个样本集的分布是否相同</p><p>H0: 两个样本集的分布相同</p><p>H1: 两个样本集的分布不同</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> mannwhitneyudata1 = [<span class="hljs-number">0.873</span>, <span class="hljs-number">2.817</span>, <span class="hljs-number">0.121</span>, <span class="hljs-number">-0.945</span>, <span class="hljs-number">-0.055</span>, <span class="hljs-number">-1.436</span>, <span class="hljs-number">0.360</span>, <span class="hljs-number">-1.478</span>, <span class="hljs-number">-1.637</span>, <span class="hljs-number">-1.869</span>]data2 = [<span class="hljs-number">1.142</span>, <span class="hljs-number">-0.432</span>, <span class="hljs-number">-0.938</span>, <span class="hljs-number">-0.729</span>, <span class="hljs-number">-0.846</span>, <span class="hljs-number">-0.157</span>, <span class="hljs-number">0.500</span>, <span class="hljs-number">1.183</span>, <span class="hljs-number">-1.075</span>, <span class="hljs-number">-0.169</span>]stat, p = mannwhitneyu(data1, data2)print(<span class="hljs-string">'stat=%.3f, p=%.3f'</span> % (stat, p))<span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.05</span>:print(<span class="hljs-string">'Probably the same distribution'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'Probably different distributions'</span>)<span class="hljs-comment"># output</span><span class="hljs-comment"># stat=40.000, p=0.236</span><span class="hljs-comment"># Probably the same distribution</span></code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Ross S . 概率论基础教程[M]. 人民邮电出版社, 2007.</li><li>盛骤, 谢式千, 潘承毅, 等. 概率论与数理统计 (第四版)[J]. 2008.</li><li><a href="https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/" target="_blank" rel="noopener">https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/</a></li><li><a href="https://www.scipy.org/" target="_blank" rel="noopener">https://www.scipy.org/</a></li><li><a href="https://www.thoughtco.com/difference-between-type-i-and-type-ii-errors-3126414" target="_blank" rel="noopener">https://www.thoughtco.com/difference-between-type-i-and-type-ii-errors-3126414</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>概率统计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第八章-分类数据</title>
    <link href="/article/28c12c7e.html"/>
    <url>/article/28c12c7e.html</url>
    
    <content type="html"><![CDATA[<p><img src="/article/28c12c7e/分类数据.png" srcset="/img/loading.gif" alt></p><a id="more"></a><h1 id="问题与练习"><a href="#问题与练习" class="headerlink" title="问题与练习"></a>问题与练习</h1><p>【问题一】如何使用union_categoricals方法，它的作用是什么？</p><ul><li>如果要组合不一定具有相同类别的类别，union_categoricals函数将组合类似列表的类别。新类别将是合并的类别的并集。如下所示：</li></ul><pre><code class="hljs python"><span class="hljs-keyword">from</span> pandas.api.types <span class="hljs-keyword">import</span> union_categoricalsa = pd.Categorical([<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>])b = pd.Categorical([<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>])union_categoricals([a,b])</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-1.png" srcset="/img/loading.gif" alt="image-20200627224024855"></p><ul><li>默认情况下，生成的类别将按照在数据中显示的顺序排列。如果要对类别进行排序，可使用sort_categories=True参数。</li><li>union_categoricals也适用于组合相同类别和顺序信息的两个分类。</li><li>union_categoricals可以在合并分类时重新编码类别的整数代码。</li></ul><p>【问题二】利用concat方法将两个序列纵向拼接，它的结果一定是分类变量吗？什么情况下不是？</p><ul><li>pd.concat对象只能是pd.Series或pd.DataFrame，所以结果是object。</li><li>使用union_categoricals拼接两个分类变量可得到分类变量。</li></ul><p>【问题三】当使用groupby方法或者value_counts方法时，分类变量的统计结果和普通变量有什么区别？</p><ul><li>分类变量的groupby方法/value_counts方法，统计对象是类别。</li><li>普通变量groupby方法/value_counts方法，统计对象是唯一值(不包含NA)。</li></ul><p>【问题四】下面的代码说明了Series创建分类变量的什么”缺陷”？如何避免？（提示使用Series的copy参数）</p><pre><code class="hljs python">cat = pd.Categorical([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>], categories=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">10</span>])s = pd.Series(cat, name=<span class="hljs-string">"cat"</span>)cat</code></pre><p>输出：</p><pre><code class="hljs tex">[1, 2, 3, 10]Categories (5, int64): [1, 2, 3, 4, 10]</code></pre><hr><pre><code class="hljs python">s.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>] = <span class="hljs-number">10</span>cat</code></pre><p>输出：</p><pre><code class="hljs tex">[10, 10, 3, 10]Categories (5, int64): [1, 2, 3, 4, 10]</code></pre><p>分类变量在Series改动时也被改动了。<br><strong>使用copy参数避免分类变量被修改。</strong></p><pre><code class="hljs python">cat = pd.Categorical([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">10</span>],categories=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">10</span>])s = pd.Series(cat, name=<span class="hljs-string">'cat'</span>, copy=<span class="hljs-literal">True</span>)print(cat)s.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>] = <span class="hljs-number">10</span>print(cat)</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-2.png" srcset="/img/loading.gif" alt="image-20200627230102057"></p><p>【练习一】现继续使用第四章中的地震数据集，请解决以下问题：<br>（a）现在将深度分为七个等级：[0.5,10,15,20,30,50,np.inf]，请以深度Ⅰ,Ⅱ,Ⅲ,Ⅳ,Ⅴ,Ⅵ,Ⅶ为索引并按照由浅到深的顺序进行排序。</p><p>使用cut方法对列表中的深度划分，并将该列作为索引值。然后按索引排序即可。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/Earthquake.csv'</span>)df_result = df.copy()df_result[<span class="hljs-string">'深度'</span>] = pd.cut(df[<span class="hljs-string">'深度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">15</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">50</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>,<span class="hljs-string">'Ⅴ'</span>,<span class="hljs-string">'Ⅵ'</span>,<span class="hljs-string">'Ⅶ'</span>])df_result = df_result.set_index(<span class="hljs-string">'深度'</span>).sort_index()df_result.head()</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-3.png" srcset="/img/loading.gif" alt="image-20200627231333013"></p><p>（b）在(a)的基础上，将烈度分为4个等级：[0,3,4,5,np.inf]，依次对南部地区的深度和烈度等级建立多级索引排序。</p><p>跟(a)很相似，cut方法对深度，烈度进行切分，把index设为[‘深度’，‘烈度’]，然后进行索引排序即可。</p><pre><code class="hljs python">df[<span class="hljs-string">'烈度'</span>] = pd.cut(df[<span class="hljs-string">'烈度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>])df[<span class="hljs-string">'深度'</span>] = pd.cut(df[<span class="hljs-string">'深度'</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">10</span>,<span class="hljs-number">15</span>,<span class="hljs-number">20</span>,<span class="hljs-number">30</span>,<span class="hljs-number">50</span>,np.inf], right=<span class="hljs-literal">False</span>, labels=[<span class="hljs-string">'Ⅰ'</span>,<span class="hljs-string">'Ⅱ'</span>,<span class="hljs-string">'Ⅲ'</span>,<span class="hljs-string">'Ⅳ'</span>,<span class="hljs-string">'Ⅴ'</span>,<span class="hljs-string">'Ⅵ'</span>,<span class="hljs-string">'Ⅶ'</span>])df_ds = df.set_index([<span class="hljs-string">'深度'</span>,<span class="hljs-string">'烈度'</span>])df_ds.sort_index()</code></pre><p>【练习二】 对于分类变量而言，调用第4章中的变形函数会出现一个BUG（目前的版本下还未修复）：例如对于crosstab函数，按照<a href="https://pandas.pydata.org/pandas-docs/version/1.0.0/user_guide/reshaping.html#cross-tabulations" target="_blank" rel="noopener">官方文档的说法</a>，即使没有出现的变量也会在变形后的汇总结果中出现，但事实上并不是这样，比如下面的例子就缺少了原本应该出现的行’c’和列’f’。基于这一问题，请尝试设计my_crosstab函数，在功能上能够返回正确的结果。<br>因为Categories中肯定包含出现的变量。所以将第一个参数作为index，第二个参数作为columns，建立一个DataFrame，然后把出现的变量组合起来，对应位置填入1即可。</p><pre><code class="hljs python">foo = pd.Categorical([<span class="hljs-string">'b'</span>,<span class="hljs-string">'a'</span>], categories=[<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>])bar = pd.Categorical([<span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>], categories=[<span class="hljs-string">'d'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'f'</span>])<span class="hljs-keyword">import</span> numpy<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_crosstab</span><span class="hljs-params">(a, b)</span>:</span>    s1 = pd.Series(list(foo.categories), name=<span class="hljs-string">'row'</span>)    s2 = list(bar.categories)    df = pd.DataFrame(np.zeros((len(s1), len(s2)),int),index=s1, columns=s2)    index_1 = list(foo)    index_2 = list(bar)    <span class="hljs-keyword">for</span> loc <span class="hljs-keyword">in</span> zip(index_1, index_2):        df.loc[loc] = <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> dfmy_crosstab(foo, bar)</code></pre><p>输出：</p><p><img src="/article/28c12c7e/image-4.png" srcset="/img/loading.gif" alt="image-20200627232221247"></p>]]></content>
    
    
    <categories>
      
      <category>pandas下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第7章 文本数据</title>
    <link href="/article/88685a55.html"/>
    <url>/article/88685a55.html</url>
    
    <content type="html"><![CDATA[<p><img src="/article/88685a55/文本数据.png" srcset="/img/loading.gif" alt></p><h1 id="六、问题与练习"><a href="#六、问题与练习" class="headerlink" title="六、问题与练习"></a>六、问题与练习</h1><h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1.问题"></a>1.问题</h2><p>【问题一】 str对象方法和df/Series对象方法有什么区别?</p><ul><li><p>str对象方法有很多针对字符串的方法</p></li><li><p>在replace方法上，二者有较大差异：</p><p>1）str.replace针对object类型或string类型，默认操作：正则表达式</p><p>​      replace针对任意类型的序列或数据框，正则表达式替换：regex=True。使用字典可以支持      多列替换。</p><p>2）str.replace类型赋值参数不能是pd.NA。</p><p>3）对于string类型Series，使用replace函数不能使用正则表达式替换。</p></li></ul><p>【问题二】 给出一列string类型，如何判断单元格是否是数值型数据？</p><ul><li>正则表达式进行匹配。</li><li>str.isnumeric有一定的局限性。</li></ul><p>【问题三】str.split方法的作用是什么？在什么场合下使用？</p><ul><li>使用分隔符对每个字符串进行分割并返回列表。</li><li>可以使用str方法进行元素选择。</li></ul><p>【问题四】在本章的第二到第四节分别介绍了字符串类型的5类操作，请思考这些操作的应用场景？</p><ul><li>拆分：</li><li>拼接</li><li>替换</li><li>字符匹配</li><li>字符提取</li></ul><h2 id="2-练习"><a href="#2-练习" class="headerlink" title="2.练习"></a>2.练习</h2><p>【练习一】现有一份关于字符串的数据集，请解决以下问题：</p><p>（a）先对字符串编码存储人员信息（在编号后添加ID列），使用如下格式：”xxx(名字)：x国人，性别x，生于x年x月x日“</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/String_data_one.csv'</span>,index_col=<span class="hljs-string">'人员编号'</span>, dtype=<span class="hljs-string">'string'</span>)df[<span class="hljs-string">'ID'</span>] = df[<span class="hljs-string">'姓名'</span>].str.cat([<span class="hljs-string">': '</span>+df[<span class="hljs-string">'国籍'</span>]+<span class="hljs-string">'国人,'</span>,<span class="hljs-string">'性别'</span>+df[<span class="hljs-string">'性别'</span>]+<span class="hljs-string">','</span>,<span class="hljs-string">'生于'</span>+df[<span class="hljs-string">'出生年'</span>]+<span class="hljs-string">'年'</span>,df[<span class="hljs-string">'出生月'</span>]+<span class="hljs-string">'月'</span>, df[<span class="hljs-string">'出生日'</span>] +<span class="hljs-string">'日'</span>],na_rep=<span class="hljs-string">'*'</span>)df[<span class="hljs-string">'ID'</span>].head()</code></pre><p>输出：</p><p><img src="/article/88685a55/image-1.png" srcset="/img/loading.gif" alt="image-20200627003713531"></p><p>（b）将（a）中的人员生日信息部分修改为用中文表示（如一九七四年十月二十三日），其余返回格式不变。</p><p>（c）将（b）中的ID列结果拆分为原列表相应的5列，并使用equals检验是否一致。</p><p>【练习二】现有一份半虚拟的数据集，第一列包含了新型冠状病毒的一些新闻标题，请解决以下问题：</p><p>（a）选出所有关于北京市和上海市新闻标题的所在行。</p><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">'data/String_data_two.csv'</span>)df[<span class="hljs-string">'col1'</span>].str.extract(<span class="hljs-string">r'(?P&lt;name_1&gt;北京|上海)'</span>).dropna().index</code></pre><p>输出：</p><p><img src="/article/88685a55/image-2.png" srcset="/img/loading.gif" alt="image-20200626233841173"></p><p>（b）求col2的均值。</p><p>（c）求col3的均值。</p>]]></content>
    
    
    <categories>
      
      <category>pandas下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Task02-LBP特征描述算子-人脸检测</title>
    <link href="/article/50afbda.html"/>
    <url>/article/50afbda.html</url>
    
    <content type="html"><![CDATA[<p>学习任务：</p><p><strong>本文主要包含以下两部分</strong>：</p><ul><li><strong>理论部分</strong>：掌握LBP特征描述算子原理</li><li><strong>练习部分</strong>：使用OpenCV的LBP检测器完成人脸检测任务</li></ul><p>LBP是指局部二值模式(Local Binary Pattern)，是一种用来描述图像局部特征的算子，具有灰<br>度不变性和旋转不变性等显著优点。<a id="more"></a></p><h1 id="2-1-算法理论"><a href="#2-1-算法理论" class="headerlink" title="2.1 算法理论"></a>2.1 算法理论</h1><h2 id="LBP概念"><a href="#LBP概念" class="headerlink" title="LBP概念"></a>LBP概念</h2><p>LBP指的是局部二值模式，英文全称：Local Binary Pattern，是一种用来描述图像局部特征的算子。<br>LBP特征比较出名的应用是人脸识别和目标检测中。<br>计算机视觉开源库OpenCV：</p><ul><li>使用LBP特征实现人脸识别的接口。</li><li>使用LBP特征实现目标检测分类器的接口。</li></ul><p>本文将会对上述第一点做简单实现。</p><h2 id="基本LBP-纹理特征"><a href="#基本LBP-纹理特征" class="headerlink" title="基本LBP:纹理特征"></a>基本LBP:纹理特征</h2><p>基本LBP特征描述</p><ol><li>基本的LBP算子定义在像素3*3的领域内；</li><li>以领域中文像素为阈值，相邻8个像素的灰度值与中心像素的灰度值比较，若周围像素大于中心像素，标记为1，否则标记为0；</li><li>3*3领域内的8个点经过第二步后产生8个二进制数，依次排列为一个二进制序列。</li><li>8位二进制序列共有$2^8$即256种LBP值。中心像素的LBP值反映了该像素周围区域的纹理信息。中心像素的灰度值决定了局部区域的整体亮度。</li></ol><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200605193831589.png#pic_center" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.1 LBP计算示意图</div></center><p>​        中心像素坐标$(x_0,y_0)$局部领域中像素个数为$P(P&gt;1)$，$g_c$表示中心像素的灰度值，$g_p$ 表示领域像素的灰度值。</p><p>​        基本LBP表示为：$LBP(x_0,y_0)=\sum_{p=0}^{P-1} s(g_{p}-g_{c}) 2^{p}$<br>​        s是示性函数：$s(x)=\left\{\begin{array}{l}1, x \geq 0 \\<br>0, x&lt;0<br>\end{array}\right.$</p><p>&emsp;&emsp; <font color="blue">基本LBP具有灰度不变性。</font>对光照变化是鲁棒的。</p><h2 id="圆形LBP算子"><a href="#圆形LBP算子" class="headerlink" title="圆形LBP算子"></a>圆形LBP算子</h2><p>&emsp;基本LBP算子的缺陷：只覆盖了固定半径范围的小区域，不能适应不同尺度和频率纹理的变化。<br>&emsp;圆形LBP(Circular LBP or Extended LBP)：<strong>将局部领域扩展到任意领域，用圆形领域代替正方形领域。</strong><br>&emsp;改进后的LBP算子允许半径为R的圆形领域有任意多个像素点。<br>    使用可变半径的圆对近邻像素进行编码，可得到如下近邻：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/20190527221543821.png" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 五种编码对应的LBP</div></center><p>​    半径为R的圆形区域内含有P个采样点的LBP算子，表示为$LBP^{R}_P$；</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200605202407325.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 圆形LBP示意图</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/20190527221655965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.2 LBP扩展与多尺度表达</div></center><p>​        对于给定中心点$(x_c,y_c)$,其邻域像素位置为$(x_p,y_p)$，$p∈P$，其采样点$(x_p,y_p)$用如下公式计算：</p><script type="math/tex; mode=display">{x}_{p}=x_{c}+\operatorname{Rcos}\left(\frac{2 \pi p}{P}\right) \\y_{p}=y_{c}+\operatorname{Rsin}\left(\frac{2 \pi p}{P}\right)</script><p>​    &emsp;R是采样半径，p是第p个采样点，P是采样数目。如果近邻点不在整数位置上，就需要进行插值运算，然后使用计算出的插值点。OpenCV使用的是双线性插值。</p><h2 id="旋转不变性LBP"><a href="#旋转不变性LBP" class="headerlink" title="旋转不变性LBP"></a>旋转不变性LBP</h2><p>实现：<strong>不断旋转圆形邻域得到一系列初始定义的LPB值，取最小值作为中心像素点的LBP特征。</strong></p><script type="math/tex; mode=display">L B P_{P R}^{ri}=\min \left(R O R\left(L B P_{P, R}, i\right) | i=0,1, \ldots, P-1\right)\tag{2-7}</script><p>&emsp;其中$L B P_{P R}^{ri}$表示具有旋转不变性的LBP特征。$ROR(x, i)$为旋转函数，表示将P-bit​数右循环i位。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200606135438907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 求取旋转不变的LBP特征示意图</div></center><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/v55RgEh.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 36种不同的旋转不变性LBP特征(P=8)</div></center><h2 id="Uniform-Pattern-LBP特征"><a href="#Uniform-Pattern-LBP特征" class="headerlink" title="Uniform Pattern LBP特征"></a>Uniform Pattern LBP特征</h2><p>Uniform Pattern，也被称为等价模式或均匀模式。<br>        将LBP算子用于纹理分类或人脸识别时，常用于LBP模式的统计直方图来表达图像的信息，较多的模式种类将使得数据量过大，且直方图过于稀疏。因此，需要对原始的LBP进行降维，使得数据量减少的情况下更好地表达图像信息。<br>        为了解决二进制模式过多的问题，提高统计性，Ojala提出了采用一种“<strong>等价模式</strong>”(Uniform Pattern)来对LBP算子的模式种类<strong>进行降维</strong>。<br>        <strong>在实际图像中，绝大多数LBP模式最多只包含两次从1到0或从0到1的跳变。</strong></p><ul><li>将<strong>等价模式</strong>定义为：当某个LBP所对应的循环二进制数从0到1或从1到0最多有两次跳变时，该LBP所对应的二进制就称为一个等价模式类。<strong>如00000000(0次跳变)，00000111(只含一次从0到1的跳变)，10001111(先由1跳到0，再由0跳到1，共两次跳变)</strong>,都是等价模式类。</li><li>除等价模式类以外的模式都归为另一类，称为<strong>混合模式类</strong>。<strong>例如10010111(共四次跳变)</strong>。</li></ul><p>&emsp;&emsp;<strong>检查某种模式是否是等价模式</strong>：将当前位置和移动一位后的二进制模式按位相减。并绝对值求和。若U$\left(G_{p}\right)$ 小于等于2，则为等价模式。</p><script type="math/tex; mode=display">U\left(G_{p}\right)=\left|s\left(g_{p_{-1}}-g_{c}\right)-s\left(g_{0}-g_{c}\right)\right|+\sum_{p=1}^{P_{-1}}\left|s\left(g_{p}-g_{c}\right)-s\left(g_{P-1}-g_{c}\right)\right|</script><p>&emsp;&emsp;二进制模式数量由原来的$2^P$种减少为了$P(P-1)+2+1$种。其中等价模式类为$P(P-1)+2$种，在LBP特征图的灰度值为$1-[P(P-1)+2]$，混合模式类为1种，灰度值为0。因此等价模式LBP特征图像整体偏暗。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/article/50afbda/image-20200628174535833.png" srcset="/img/loading.gif">    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.3 LBP等价模式类(P=8)</div></center><h2 id="LBPH特征"><a href="#LBPH特征" class="headerlink" title="LBPH特征"></a>LBPH特征</h2><p>LBPH特征：全称为Local Binary Patterns Histograms，即LBP特征的统计直方图。LBPH将LBP特征与图像的空间信息结合在一起。</p><p>一幅图具体的计算LBPH过程如下：</p><p>1.计算图像的LBP特征图像；</p><p>2.将LBP特征图像进行分块，OpenCV中将LBP特征图像分成8行8列64块区域；</p><p>3.计算每块区域特征图像的直方图cell_LBPH，并将直方图进行归一化，直方图大小为 1*numPatterns；</p><p>4.将上面计算的每块区域特征图像的直方图按分块的空间顺序依次排列成一列，形成LBP特征分量，其大小为 1<em>(numPatterns</em>64)；</p><p>5.用机器学习的方式对LBP特征向量进行训练，用来检测和识别目标。</p><h1 id="2-2-基于OpenCV的LBP人脸检测"><a href="#2-2-基于OpenCV的LBP人脸检测" class="headerlink" title="2.2 基于OpenCV的LBP人脸检测"></a>2.2 基于OpenCV的LBP人脸检测</h1><h2 id="多级级联对人脸图像进行检测："><a href="#多级级联对人脸图像进行检测：" class="headerlink" title="多级级联对人脸图像进行检测："></a>多级级联对人脸图像进行检测：</h2><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="https://img-blog.csdnimg.cn/20200606145525679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk0MDUxMg==,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图 2.3.5 人脸检测流程图</div></center><h2 id="基于OpenCV的实现"><a href="#基于OpenCV的实现" class="headerlink" title="基于OpenCV的实现"></a>基于OpenCV的实现</h2><ul><li>使用OpenCV预训练模型</li><li>将haarcascade_frontalface_default.xml下载至本地以方便调用，下载链接：<a href="https://github.com/opencv/opencv/blob/master/data/lbpcascades/lbpcascade_frontalface_improved.xml" target="_blank" rel="noopener">https://github.com/opencv/opencv/blob/master/data/lbpcascades/lbpcascade_frontalface_improved.xml</a>   </li></ul><p>函数：<br>detectMultiScale(self, image, scaleFactor=None, minNeighbors=None, flags=None, minSize=None, maxSize=None)<br>参数:<br>1）scaleFactor: 图像缩放比例<br>2）minNeighbors: 每个候选矩形保留的邻居个数，值越大-&gt;精度越大<br>3）minSize：检测到的最小矩形大小<br>4）maxSize: 检测到的最大矩形大小</p><p>待测人脸图像(网上随意摘取一张)：</p><p><img src="/article/50afbda/image2.jpg" srcset="/img/loading.gif" alt="image2"></p><pre><code class="hljs python"><span class="hljs-comment"># 利用LBP进行人脸检测</span><span class="hljs-keyword">import</span> cv2img = cv2.imread(<span class="hljs-string">'image2.jpg'</span>)<span class="hljs-comment"># cv2.imshow('test', img)</span>face_detect = cv2.CascadeClassifier(<span class="hljs-string">'lbpcascade_frontalface_improved.xml'</span>)<span class="hljs-comment"># ======================检测人脸==============================</span><span class="hljs-comment"># 灰度处理</span>gray = cv2.cvtColor(img, code=cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 检测人脸 按照1.1倍放到 周围最小像素为5</span>face_zone = face_detect.detectMultiScale(gray, scaleFactor=<span class="hljs-number">1.1</span>, minNeighbors=<span class="hljs-number">2</span>)print(face_zone)<span class="hljs-comment"># 绘制矩形检测人脸</span><span class="hljs-keyword">for</span> (x, y, w, h) <span class="hljs-keyword">in</span> face_zone:    <span class="hljs-comment"># 绘制矩形人脸区域</span>    print(x,y,w,h)    cv2.rectangle(img, pt1=(x, y), pt2=(x+w, y+h), color=[<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], thickness=<span class="hljs-number">2</span>)<span class="hljs-comment"># 显示图片</span>cv2.imshow(<span class="hljs-string">"output"</span>, img)<span class="hljs-comment"># 等待显示，设置任意键退出程序</span>cv2.waitKey(<span class="hljs-number">0</span>)cv2.destroyAllWindows()</code></pre><p>检测结果：</p><p><img src="/article/50afbda/output.jpg" srcset="/img/loading.gif" alt="output"></p><p>简单示意例子：<a href="https://sharky93.github.io/docs/gallery/auto_examples/plot_local_binary_pattern.html" target="_blank" rel="noopener">https://sharky93.github.io/docs/gallery/auto_examples/plot_local_binary_pattern.html</a></p><h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p><a href="https://blog.csdn.net/qq_34246778/article/details/90613779?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">LBP(Local Binary Patterns特征)</a></p><p><a href="https://blog.csdn.net/YhL_Leo/article/details/52120195" target="_blank" rel="noopener">局部二值模式（Local Binary Patterns）纹理灰度与旋转不变性</a></p><p><a href="https://github.com/datawhalechina/team-learning/blob/master/03 计算机视觉/计算机视觉基础：图像处理（下）/Task02 LBP特征描述算子.md" target="_blank" rel="noopener">Task02 LBP特征描述算子</a></p>]]></content>
    
    
    <categories>
      
      <category>图像处理下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Task01 Harris特征点检测器-兴趣点检测</title>
    <link href="/article/8b3bd9dd.html"/>
    <url>/article/8b3bd9dd.html</url>
    
    <content type="html"><![CDATA[<h1 id="Harris特征点检测器-兴趣点检测"><a href="#Harris特征点检测器-兴趣点检测" class="headerlink" title="Harris特征点检测器-兴趣点检测"></a>Harris特征点检测器-兴趣点检测</h1><p><strong>特则点</strong>又称为兴趣点或角点，通常具有旋转不变性和光照不变性和视角不变性等优点，是图像的重要特征之一，常被应用到目标匹配、目标跟踪、三维重建等应用中。</p><p><strong>点特征</strong>主要是指图像中的明显点，如突出的角点、边缘端点、权值点等等。</p><p><strong>兴趣点提取(检测)算子</strong>是用于点特征提取的算子<a id="more"></a>，常用的有</p><ul><li>Harris角点检测：用于检测角点</li><li>FAST特征检测：用于检测角点</li><li>SIFT特征检测：用于检测斑点</li><li>SURF特征检测：用于检测角点</li><li>BRIEF特征检测：用于检测斑点</li><li>ORB：该算法代表带方向的FAST算法和具有旋转不变性的BRIEF算法</li></ul><p><strong>特征匹配</strong>：</p><ul><li>暴力(Brute-Force)匹配法</li><li>基于FLANN匹配法</li></ul><h2 id="一、Harris特征点检测算法的思想和数学原理"><a href="#一、Harris特征点检测算法的思想和数学原理" class="headerlink" title="一、Harris特征点检测算法的思想和数学原理"></a>一、Harris特征点检测算法的思想和数学原理</h2><h2 id="1-1-基础知识"><a href="#1-1-基础知识" class="headerlink" title="1.1 基础知识"></a>1.1 基础知识</h2><ul><li><p><strong>角点</strong></p><p>左图表示一个平坦区域，在各个方向移动，窗口内像素值均没有太大变化；</p><p>中图表示一个边缘特征(Edges)，若沿着水平方向移动(梯度方向)，像素值会发生跳变；若沿着边缘移动(平行于边缘)，像素值不变发生变化；</p><p>右图表示一个角(Corners)，它朝哪个方向移动，像素值都会发生很大变化。即为<font color="red">角点</font>。</p></li></ul><p><img src="https://camo.githubusercontent.com/460ac04994bec25117ee2724563058da99e5bf1e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303630393230343234393231392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304d4459304e7a67784f513d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><ul><li><strong>图像梯度</strong>:图像局部内，图像梯度越大表示该局部内像素值变化越大。图形梯度在数学上可用微分或者积分表示,使用差分来近似导数：$G_x(x,y)=H(x+1,y)-H(x-1,y)$,</li><li><strong>补充</strong>：对图像求梯度通常是考虑图像的每个像素的某个领域内的灰度变化，因此通常对原始图像中像素某个领域设置梯度算子，然后采用小区域模板进行卷积计算，常用的有Prewitt算子、Sobel算子、Robinson算子、Laplace算子等。</li></ul><h2 id="1-2-Harris角点检测算法原理"><a href="#1-2-Harris角点检测算法原理" class="headerlink" title="1.2 Harris角点检测算法原理"></a>1.2 Harris角点检测算法原理</h2><h3 id="1-2-1-计算窗口内部的像素值变化量-E-x-y"><a href="#1-2-1-计算窗口内部的像素值变化量-E-x-y" class="headerlink" title="1.2.1 计算窗口内部的像素值变化量$E(x,y)$"></a>1.2.1 计算窗口内部的像素值变化量$E(x,y)$</h3><p><strong>建立数学模型，确定哪些窗口会引起较大的灰度值变化</strong></p><p>窗口$W$对应的像素坐标位置$(x,y)$，窗口的大小决定了有多少位置。</p><p>像素位置坐标$(x,y)$对应的像素灰度值为$I(x,y)$，窗口分别向$x$和$y$方向上移动$(u,v)$，到达$(x+u,y+v)$上，对应的像素灰度值$I(x+u,y+v)$。</p><p>窗口移动引起的灰度值的变化量为$I(x+u,y+v)-I(x,y)$。</p><p>$(x,y)$位置的窗口函数为$w(x,y)$。即为窗口内各像素的权重。</p><p><img src="/article/8b3bd9dd/image1-2.png" srcset="/img/loading.gif" alt="image-20200623143300133"></p><p><strong>窗口移动(u,v)引起的灰度值的加权变化量</strong>：</p><script type="math/tex; mode=display">E(u,v) = \sum_{x,y}{w(x,y){[I(x+u,y+v)-I(x,y)]}^{2}}</script><p>根据二维泰勒公式展开：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}E(u,v) &\approx \sum_{x,y \in W}{w(x,y){[I(x,y)+uI_x+vI_y-I(x,y)]}^{2}} \\  &= \sum_{x,y\in W}{w(x,y){(uI_x+vI_y)}^{2}} \\  &= \sum_{x,y\in W}{w(x,y){(u^2I_x^2+vI_y^2+2uvI_xI_y)}^{2}} \\  &= \sum_{x,y\in W}{w(x,y)\begin{bmatrix}u&v\end{bmatrix}\begin{bmatrix}I_x^2 & I_xI_y\\                                       I_xI_y & I_y^2\end{bmatrix} \begin{bmatrix}u\\v\end{bmatrix}} \\  &= \begin{bmatrix}u&v\end{bmatrix}(\sum_{x,y\in W}{w(x,y) \begin{bmatrix}I_x^2 & I_xI_y\\                                     I_xI_y & I_y^2\end{bmatrix} })\begin{bmatrix}u\\v\end{bmatrix}                                      \\  &= \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\v\end{bmatrix}\end{aligned}\end{equation}</script><p>其中$I_x,I_y$分别为窗口内像素点$(x,y)$在$x$方向上和$y$方向上的梯度值。矩阵$M$为：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}M &= \sum_{(x,y)\in W}{w(x,y) \begin{bmatrix}I_x^2 & I_xI_y\\                                       I_xI_y & I_y^2\end{bmatrix}}\\  &=R^{-1}\begin{bmatrix}\lambda_1 & 0\\0 & \lambda_2\end{bmatrix}R\end{aligned}\end{equation}</script><p>最后使用实对称矩阵对角化处理后得到我们想要的结果。</p><h3 id="1-2-2-计算对应的角点响应函数-R"><a href="#1-2-2-计算对应的角点响应函数-R" class="headerlink" title="1.2.2 计算对应的角点响应函数$R$"></a>1.2.2 计算对应的角点响应函数$R$</h3><p>通过矩阵的梯度变化可以得到协方差矩阵M，协方差矩阵M决定了灰度值的加权变化量。因此通过计算<strong>角点响应函数R</strong>得到每个窗口对应的得分:</p><script type="math/tex; mode=display">R = det(M)-k(trace(M))^2</script><p>其中$det(M)=\lambda_1\lambda_2$为矩阵的行列式，$trace(M)=\lambda_1+\lambda_2$为矩阵的迹。</p><p>$k$是一个经验常数，需要经验确定它的合适大小，通常在(0.04,0.06)之间取值。</p><h3 id="1-2-3-角点判定"><a href="#1-2-3-角点判定" class="headerlink" title="1.2.3 角点判定"></a>1.2.3 角点判定</h3><p>根据R值判断窗口是平面、边缘还是角点：</p><ul><li>平面：$|R|$值非常小，$\lambda_1$和$\lambda_2$都较小，窗口区域的像素点的梯度变化小。</li><li>边缘：$|R|$值为负数，$\lambda_1 \gg \lambda_2$或$\lambda_2 \gg \lambda_1$，像素点的某个方向的梯度幅值变化比较明显，另一个方向上的梯度变化比较弱。</li><li>角点：$|R|$值很大，$(I_x,I_y)$对应的$\lambda_1$和$\lambda_2$都很大。像素点的梯度分布比较散，梯度变化程度比较大。</li></ul><p>如下图所示：</p><p><img src="https://camo.githubusercontent.com/c6eef9fa98b4f1e099bc722b21536c73c6b9bf9f/68747470733a2f2f696d67636f6e766572742e6373646e696d672e636e2f6148523063484d364c7939706257466e5a584d794d4445314c6d4e75596d78765a334d75593239744c324a73623263764e4455784e6a59774c7a49774d5459774e4338304e5445324e6a41744d6a41784e6a41304d6a45784d5441314e4455354f5445744e4451304e6a6b314e5445344c6e42755a773f782d6f73732d70726f636573733d696d6167652f666f726d61742c706e67237069635f63656e746572" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>Harris角点检测：设定一个阈值，高于阈值的像素对应角点。</p><ul><li>补充：角点的非极大值原理—在一个窗口内，如果有很多角点则用值最大的那个角点，其他的角点都删除。</li></ul><h2 id="1-3-Shi-Tomasi角点检测器"><a href="#1-3-Shi-Tomasi角点检测器" class="headerlink" title="1.3 Shi-Tomasi角点检测器"></a>1.3 Shi-Tomasi角点检测器</h2><p><strong>Shi-Tomasi 角点检测</strong>：《Good_Features_to_Track》论文提出的Harris改进版。</p><p>Harris角点检测中每个窗口的分数公式：$R=\lambda_1\lambda_2 - k(\lambda_1+\lambda_2)^2$</p><p>缺陷：Harris角点检测算法的稳定性和k值有关，但k是经验值，不好设定最佳值。</p><p>改进：角点稳定性与矩阵$M$的较小特征值有关，Shi-Tomasi 直接采用较小的特征值作为分数。(如此一来就不用调整k值啦)</p><p>Shi-Tomasi角点检测中每个窗口的分数公式：$R=min(\lambda_1,\lambda_2)$</p><p>判定角点的方式不变：分数大于设定的阈值，即为角点。</p><h2 id="二、OpenCV的Harris算子进行兴趣点检测"><a href="#二、OpenCV的Harris算子进行兴趣点检测" class="headerlink" title="二、OpenCV的Harris算子进行兴趣点检测"></a>二、OpenCV的Harris算子进行兴趣点检测</h2><h2 id="2-1-Harris角点检测"><a href="#2-1-Harris角点检测" class="headerlink" title="2.1 Harris角点检测"></a>2.1 Harris角点检测</h2><p><strong>opencv</strong>提供了实现<strong>Harris</strong>角点检测函数：<a href="https://link.zhihu.com/?target=https%3A//docs.opencv.org/master/dd/d1a/group__imgproc__feature.html%23gac1fc3598018010880e370e2f709b4345">cv2.cornerHarris</a>，下面调用该接口进行<strong>Harris</strong>特征点检测。</p><p>函数：<strong>cv2.cornerHarris(src, blockSize, ksize, k[, dst[, borderType]])​</strong></p><p>函数功能：对于每一个像素 $(x,y)$，在 ($blockSize$ x $blockSize$) 邻域内，计算梯度图的协方差矩阵$M(x,y)$，通过计算角点响应函数得到结果图。该结果图的局部最大值即图像中的角点。</p><p>函数参数：</p><ul><li><strong>src</strong>:待检测的灰度图像(float32类型)</li><li><strong>blockSize</strong>:用于角点检测的领域大小，即窗口尺寸</li><li><strong>ksize</strong>:用于计算梯度图的Sobel算子的尺寸</li><li><strong>k</strong>:用于计算角点响应函数的参数k，取值范围在0.04~0.06之间</li></ul><p>待检测的图片：</p><p><img src="/article/8b3bd9dd/image1.jpg" srcset="/img/loading.gif" alt="image1"></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 检测参数</span>block_size = <span class="hljs-number">3</span>sobel_size = <span class="hljs-number">3</span>k = <span class="hljs-number">0.06</span>image = cv.imread(<span class="hljs-string">'image1.jpg'</span>)print(image.shape)height = image.shape[<span class="hljs-number">0</span>]width = image.shape[<span class="hljs-number">1</span>]channels = image.shape[<span class="hljs-number">2</span>]print(<span class="hljs-string">"width: %s height: %s channel: %s"</span>%(width, height, channels))<span class="hljs-comment"># 将图像转换为灰度图</span>gray_img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)<span class="hljs-comment"># 将数据格式转换为32位浮点数</span>gray_img = np.float32(gray_img)<span class="hljs-comment"># 使用合适值作为输入参数检测角点，得到的结果图用来标出角点</span>corners_img = cv.cornerHarris(gray_img, block_size, sobel_size, k)</code></pre><p>输出：</p><pre><code class="hljs tex">(225, 225, 3)width: 225 height: 225 channel: 3</code></pre><pre><code class="hljs python">kernel = cv.getStructuringElement(cv.MORPH_RECT,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))dst = cv.dilate(corners_img, kernel)<span class="hljs-comment"># dst = cv.dilate(corners_img,None)</span><span class="hljs-comment"># image[dst&gt;0.05*dst.max()] = [255,0,0]</span><span class="hljs-comment"># num = dst &gt; 0.05 * dst.max()</span><span class="hljs-comment"># count = np.sum(num)</span><span class="hljs-comment"># print(count)</span>count = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> range(height):    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(width):        pix = dst[r,c]        <span class="hljs-comment"># 阈值，蓝色标记角点</span>        <span class="hljs-keyword">if</span> pix &gt; <span class="hljs-number">0.05</span> * dst.max():            cv.circle(image,(c,r),<span class="hljs-number">2</span>,(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">-1</span>)            count += <span class="hljs-number">1</span>print(count)cv.imwrite(<span class="hljs-string">'harris_img1.jpg'</span>,image)</code></pre><p>输出：</p><pre><code class="hljs tex">741</code></pre><p>处理后图片如下：</p><p><img src="/article/8b3bd9dd/harris_img1.jpg" srcset="/img/loading.gif" alt="harris_img1"></p><pre><code class="hljs python"><span class="hljs-comment"># 使用点画图，点小一点，好看一丢丢把。。。</span>image[dst&gt;<span class="hljs-number">0.05</span>*dst.max()] = [<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]</code></pre><p><img src="/article/8b3bd9dd/harris_img2.jpg" srcset="/img/loading.gif" alt="harris_img1"></p><h2 id="2-2-Shi-Tomasi角点检测"><a href="#2-2-Shi-Tomasi角点检测" class="headerlink" title="2.2 Shi-Tomasi角点检测"></a>2.2 Shi-Tomasi角点检测</h2><p>opencv提供了实现Shi-Tomasi 角点检测函数：<strong>cv2.goodFeaturesToTrack()</strong>，下面调用该接口进行Harris特征点检测。</p><p>函数：<strong>goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]])</strong></p><p>函数功能：流程大体与Harris相似。不同之处在于窗口分数计算公式不同。在检测到的角点中，所有低于质量水平的角点都会被忽略，合格角点按角点质量进行降序排列，保留质量最高的一个角点，将它附近（最小距离之内）的角点都删掉（类似于非极大值抑制），按这样的方式最后得到 N 个最佳角点。</p><p>函数参数：</p><ul><li><strong>image</strong>：输入灰度图像，float32类型</li><li><strong>maxCorners</strong>：返回角点的最大数目，值为0表表示不设置最大值限制，返回所有检测到的角点。</li><li><strong>qualityLevel</strong>：质量系数（小于1.0的正数，一般在0.01-0.1之间），表示可接受角点的最低质量水平。该系数乘以最好的角点分数（也就是上面较小的那个特征值），作为可接受的最小分数；例如，如果最好的角点分数值为1500且质量系数为0.01，那么所有质量分数小于15的角都将被忽略。</li><li><strong>minDistance</strong>：角之间最小欧式距离，忽略小于此距离的点。</li><li><strong>corners</strong>：输出角点坐标</li><li><strong>mask</strong>：可选的感兴趣区域，指定想要检测角点的区域。</li><li><strong>blockSize</strong>：默认为3，角点检测的邻域大小（窗口尺寸）</li><li><strong>useHarrisDetector</strong>：用于指定角点检测的方法，如果是true则使用Harris角点检测，false则使用Shi Tomasi算法。默认为False。</li><li><strong>k</strong>：默认为0.04，Harris角点检测时使用。</li></ul><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> cv2maxCorners = <span class="hljs-number">600</span>qualityLevel = <span class="hljs-number">0.01</span>minDistance = <span class="hljs-number">10</span>img = cv2.imread(<span class="hljs-string">'image1.jpg'</span>)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)corners = cv2.goodFeaturesToTrack(gray, maxCorners, qualityLevel, minDistance)corners = np.int0(corners)count = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> corners:    x,y = i.ravel()    cv2.circle(img,(x,y), <span class="hljs-number">2</span>, (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">-1</span>)    <span class="hljs-comment"># img[y,x] = [255,0,0]</span>    count += <span class="hljs-number">1</span>cv.imwrite(<span class="hljs-string">'Shi-Tomasi.jpg'</span>,img)img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)print(count)</code></pre><p><img src="Task01-Harris特征点检测器-兴趣点检测//Tomasi.jpg" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Harris和Shi-Tomasi都是基于梯度计算的角点检测方式。</p><p>Harris角点检测的性质：</p><ul><li><p>阈值界定角点数量</p></li><li><p>Harris角点算子对亮度和对比度的变化不敏感</p><p>因为在进行Harris角点检测时，使用了微分算子对图像进行微分运算，而微分运算对图像密度的拉升或收缩和对亮度的抬高或下降不敏感。换言之，对亮度和对比度的仿射变换并不改变Harris响应的极值点出现的位置，但是，由于阈值的选择，可能会影响角点检测的数量。</p><p><img src="/article/8b3bd9dd/image1-3.png" srcset="/img/loading.gif" alt="image-20200623145505861"></p><p>左图表示亮度变化，右图表示对比度变化。</p></li><li><p>Harris角点检测算子具有旋转不变性</p><p>Harris角点检测算子使用的是角点附近的区域灰度二阶矩矩阵。而二阶矩矩阵可以表示成一个椭圆，椭圆的长短轴正是二阶矩矩阵特征值平方根的倒数。当特征椭圆转动时，特征值并不发生变化，所以判断角点响应值RR也不发生变化，由此说明Harris角点检测算子具有旋转不变性。</p></li><li><p>Harris角点检测算子不具有尺度不变性</p><p><img src="/article/8b3bd9dd/image1-1.png" srcset="/img/loading.gif" alt="image-20200623003833148"></p><p>如上图所示，当图像被缩小时，在检测窗口尺寸不变的前提下，在窗口内所包含图像的内容是完全不同的。左侧的图像可能被检测为边缘或曲线，而右侧的图像则可能被检测为一个角点。</p></li></ul><p>基于梯度的角点检测器的缺点：计算复杂度高、图像中的噪声阻碍梯度计算。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/datawhalechina/team-learning/blob/master/03%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%8B%EF%BC%89/Task01%20Harris%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B.md" target="_blank" rel="noopener">Task01 Harris特征点检测器-兴趣点检测</a></p><p><a href="https://zhuanlan.zhihu.com/p/83064609" target="_blank" rel="noopener">角点检测：Harris 与 Shi-Tomasi</a></p><p><a href="https://www.cnblogs.com/zyly/p/9508131.html" target="_blank" rel="noopener">Harris角点检测原理(赋源码)</a></p>]]></content>
    
    
    <categories>
      
      <category>图像处理下</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Introduction_and_Word_Vectors</title>
    <link href="/article/fd8e5887.html"/>
    <url>/article/fd8e5887.html</url>
    
    <content type="html"><![CDATA[<h1 id="Task1-Introduction-and-Word-Vectors"><a href="#Task1-Introduction-and-Word-Vectors" class="headerlink" title="Task1: Introduction and Word Vectors"></a>Task1: Introduction and Word Vectors</h1><p>理论部分</p><ul><li>介绍NLP研究的对象</li><li>如何表示单词的含义</li><li>Word2Vec方法的基本原理</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2数理统计与描述性统计</title>
    <link href="/article/83114b11.html"/>
    <url>/article/83114b11.html</url>
    
    <content type="html"><![CDATA[<h1 id="一、数理统计概念"><a href="#一、数理统计概念" class="headerlink" title="一、数理统计概念"></a>一、数理统计概念</h1><h2 id="1-基本概念释义"><a href="#1-基本概念释义" class="headerlink" title="1.基本概念释义"></a>1.基本概念释义</h2><font color="blue">总体</font>：研究对象的全体，通常用一个随机变量表示。<font color="blue">个体</font>：组成总体的每个基本单元。<a id="more"></a>从总体$X$中随机抽取一部分个体$X_1,X_2,...,X_n$，称$X_1,X_2,...,X_n$为取自$X$的容量为$n$的样本。实际上，数理统计学中的总体是指与总体相联系的某个(某几个)数量指标$X$取值的全体。<font color="red">样本具有两重性，在一次具体抽样后是一组确定的数值。一般叙述中由于采取随机抽样，样本是一组随机变量(结果未知)。</font><p>一般地，用$X_1,X_2,…,X_n$，表示随机样本，取到的值记为$x_1,x_2,…,x_n$称为样本观测值。$n$为样本容量。</p><font color="blue">样本分布</font>：样本作为随机变量的概率分布。显然，样本分布取决于总体的性质和样本的性质。## 2.统计量与抽样数理统计的任务是采集和处理带有随机影响的数据，或者说收集样本并对之进行加工，在此基础上对研究的问题进行分析并作出一定的结论，这一过程称为<font color="blue">统计推断</font>。在统计推断过程中，对样本进行加工整理，实际上就是根据样本计算出一些量，把研究问题相关的信息集中起来。这种根据样本计算出的量就是<font color="blue">统计量</font>。因此，统计量是样本的某种函数。定义：设 $X_1,X_2,...,X_n $ 是总体 $X$ 的一个简单随机样本， $T(X_1, X_2,...,X_n)$ 为一个 $n$ 元连续函数，且 $T$ 中不包含任何关于总体的未知参数，则称 $T(X_1, X_2,...,X_n)$ 是一个统计量，称统计量的分布为抽样分布。常用的统计量<font color="blue">样本均值</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，称$\overline X =  \frac{1} {n} {\sum_{i=1}^{n}X_i}$为样本均值。通常用样本均值来估计总体分布的均值和对有关总体分布均值的假设作检验。</p><font color="blue">样本方差</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，$\overline X$ 为样本均值，称$S^2 =  \frac{1} {n-1} {\sum_{i=1}^{n}(X_i-\overline X)^2}$为样本方差。</p><p>通常用样本方差来估计总体分布的方差和对有关总体分布均值或方差的假设作检验。</p><font color="blue">k阶样本原点矩</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，称$A_k =  \frac{1} {n} {\sum_{i=1}^{n}X_i^k}$</p><p>为样本的 $k$ 阶原点矩（可以看到 $k=1$ 时，相当于样本均值），通常用样本的无阶原点矩来估计总体分布的 $k$ 阶原点矩。（可以看到 $k=1$ 时，相当于样本均值）</p><p>通常用样本的无阶原点矩来估计总体分布的 $k$ 阶原点矩。</p><font color="blue">k阶样本中心矩</font><p>设 $X_1,X_2,…,X_n $ 是总体 $X$ 的一个简单随机样本，$\overline X$ 为样本均值，称$M_k =  \frac{1} {n} {\sum_{i=1}^{n}(X_i-\overline X)^k}$为样本的 $k$ 阶中心矩。</p><p>通常用样本的 $k$ 阶中心矩来估计总体分布的 $k$ 阶中心矩。</p><font color="blue">顺序统计量</font><p>设 $X_1,X_2,…,X_n $ 是抽自总体 $X$ 的样本，$x_1,x_2,…，x_n$  为样本观测值。将 $x_1,x_2,…，x_n$  按照从小到大的顺序排列为$x_{(1)}&lt;=x_{(2)}&lt;=…&lt;=x_{(n)}$。</p><p>当样本 $X_1,X_2,…,X_n $ 取值 $x_1,x_2,…，x_n$  时，定义 $X_{(k)}$ 取值 $X_{(k)}(k=1,2，…,n)$，称  $X_{(1)},X_{(2)},…,X_{(n)} $ 为 $X_1,X_2,…,X_n $ 的顺序统计量。</p><p>显然，$X_{(1)} =min {X_i}$ 是样本观察中最小的一个，称为最小顺序统计量。$X_{(n)} =max {X_i}$ 是样本观测值中取值最大的一个，成为最大顺序统计量。称$X_{（r）}$ 为第 $r$ 个顺序统计量。</p><h1 id="二、描述性统计"><a href="#二、描述性统计" class="headerlink" title="二、描述性统计"></a>二、描述性统计</h1><h2 id="1-数据集中趋势的度量"><a href="#1-数据集中趋势的度量" class="headerlink" title="1.数据集中趋势的度量"></a>1.数据集中趋势的度量</h2><font color="blue">平均数</font><p>是表示一组数据集中趋势的量数，是指在一组数据中所有数据之和再除以这组数据的个数。</p><script type="math/tex; mode=display">\overline X =  \frac{1} {n} {\sum_{i=1}^{n}X_i}</script><font color="blue">中位数</font><p>是指在一组数据，按顺序排列后，居于中间位置的数。中位数描述数据中心位置的数字特征。</p><p>对于对称分布的数据，均值与中位数比较接近；对于偏态分布的数据，均值与中位数不同。中位数不受异常值的影响，具有稳健性。</p><script type="math/tex; mode=display">m_p=\left\{\begin{array}{lcl}x_{[np]+1},       &      & {当np不是整数}\\\frac{1}{2}(x_{(np)}+x_{(np+1)})     &      & {当np是整数时}\\\end{array} \right.</script><font color="blue">频数</font><p>同一观测值在一组数据中出现的次数（掷骰子中，一共掷了20次，出现数字5的次数）。</p><font color="blue">众数</font><p>就是一组数据中，出现次数最多的那个数（几个数）。</p><font color="blue">均值 vs 中位数 vs 众数</font><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">优点</th><th style="text-align:center">缺点</th></tr></thead><tbody><tr><td style="text-align:center">均值</td><td style="text-align:center">充分利用所有数据，适用性强</td><td style="text-align:center">容易受极端值影响</td></tr><tr><td style="text-align:center">中位数</td><td style="text-align:center">不受极端值影响</td><td style="text-align:center">缺乏敏感性</td></tr><tr><td style="text-align:center">众数</td><td style="text-align:center">不受极端值影响；当数据具有明显的集中趋势时，代表性好</td><td style="text-align:center">缺乏唯一性</td></tr></tbody></table></div><font color="blue">百分位数</font><p>百分位数是中位数的推广，将数据按从小到大排列后，对于$0 \leq p &lt; 1$，它的p分位点定义为</p><script type="math/tex; mode=display">m_p=\left\{\begin{array}{lcl}x_{[np]+1},       &      & {当np不是整数}\\\frac{1}{2}(x_{(np)}+x_{(np+1)})     &      & {当np是整数时}\\\end{array} \right.</script><p>其中，<strong>[np]</strong>表示<strong>np</strong>的整数部分。所以，0.5分位数（第50百分位数）就是中位数。</p><p><img src="/article/83114b11/百分位数图.png" srcset="/img/loading.gif" alt></p><h3 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h3><p><img src="/article/83114b11/image1" srcset="/img/loading.gif" alt="image-20200624144415651"></p><h2 id="2-数据离散趋势的度量"><a href="#2-数据离散趋势的度量" class="headerlink" title="2.数据离散趋势的度量"></a>2.数据离散趋势的度量</h2><p>表示数据分散（离散，差异）程度的特征量有方差，标准差，极差以及变异系数等。</p><font color="blue">方差</font><p>用来计算每一个变量（观察值）与总体均数之间的差异。实际工作中，总体均数难以得到时，应用样本统计量代替总体参数，经校正后，样本方差计算公式:</p><script type="math/tex; mode=display">s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i-\overline{x})}^2</script><font color="blue">标准差</font><p>样本方差的开平方成为样本标准差。</p><script type="math/tex; mode=display">s=\sqrt{s^2}=\sqrt{\frac{1}{n-1}\sum_{1}^{n-1}{(x_i-\overline{x})}^2}</script><font color="blue">极差</font>$$R=X_{(n)}-x_{(1)}=\max(x)-\min(x)$$数据越分散，极差越大。<font color="blue">变异系数</font><ul><li>是刻画数据相对分散性的一种度量。变异系数只在平均值不为零时有定义，而且一般适用于平均值大于零的情况。变异系数也被称为<strong>标准离差率</strong>或<strong>单位风险</strong>。</li><li>当需要比较两组数据离散程度大小的时候，如果两组数据的测量尺度相差太大，或者数据量纲的不同，变异系数可以消除测量尺度和量纲的影响。</li></ul><script type="math/tex; mode=display">CV=100\times\frac{s}{x}(\%)</script><font color="blue">四分位差</font><p>样本上、下四分位数之差称为四分位差(或半极差)。</p><script type="math/tex; mode=display">R_1 = Q_3 - Q_1</script><p>它也是度量样本分散性的重要数字特征，特别对于具有异常值的数据，它作为分散性具有稳健性<font color="red">  <strong>（见百分位数示意图）</strong></font>。</p><h3 id="python实现-1"><a href="#python实现-1" class="headerlink" title="python实现"></a>python实现</h3><p><img src="/article/83114b11/image2.PNG" srcset="/img/loading.gif" alt="image2" style="zoom:80%;"></p><h2 id="3-分布特征"><a href="#3-分布特征" class="headerlink" title="3.分布特征"></a>3.分布特征</h2><p> <strong>引言：</strong>描述一个随机变量，不仅要说明它能够取那些值，而且还要关心它取这些值的概率（可能性）。</p><font color="blue">离散变量和连续变量</font><ul><li><font color="blue">离散型随机变量</font>：其数值只能用<strong>自然数或整数</strong>单位表示。例如，<font color="red">  班级人数，电脑台数等，</font>只能按计量单位数计数，这种变量的数值一般用计数方法取得。</li><li><font color="blue">连续变量</font>：在一定区间内可以任意取值的变量。例如，<font color="red">人体测量的身高，体重等。</font></li></ul><font color="blue">概率函数</font>：用函数的形式来表达概率。- **连续型随机变量**的概率函数就叫做**概率密度函数**。- **离散型随机变量**的概率函数就叫做**概率质量函数**。<font color="blue">分布函数</font><p>设$X$是一个随机变量，对任意的实数$x$，令$F(x)=P(X&lt;=x),x \in (-\infty,+\infty)$，则称$F(x)$是随机变量$X$的分布函数(概率累积函数)。</p><p><img src="/article/83114b11/分布函数与密度函数的关系.png" srcset="/img/loading.gif" alt></p><font color="red">密度函数与分布函数关系</font> <font color="blue">正态分布</font>：也称高斯分布，是一个非常常见的连续概率分布，概率密度函数为$$f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\{- \frac{(x-u)^2}{2\sigma^2} \}}, \qquad -\infty<x<+\infty $$ 则称$x$服从$n(u,\sigma^2)$分布。 ![uniform2](2数理统计与描述性统计 uniform2.png) <font color="red">正态分布的概率密度函数曲线 <p><img src="/article/83114b11/标准正态分布.png" srcset="/img/loading.gif" alt="标准正态分布"></p><p>​              <font color="red">标准正态分布和对应区间上积分（面积）的百分比</font> </p><p>这个概念可以推广到一般正态分布。$[u-3\sigma,u+3\sigma]$的概率密度曲线之下的面积占总面积的99.7%，是著名的$3\sigma$原则。</p><h2 id="4-偏度与峰度"><a href="#4-偏度与峰度" class="headerlink" title="4. 偏度与峰度"></a>4. 偏度与峰度</h2><p><font color="blue">偏度（skewness）</font>：也称为偏态，是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。直观看来就是密度函数曲线尾部的相对长度。<strong>偏度刻画的是分布函数（数据）的对称性。</strong></p><p>关于均值对称的数据其偏度系数为0，右侧更分散的数据偏度系数为正，左侧更分散的数据偏度系数为负.</p><ul><li><p><strong>正态分布的偏度为0，两侧尾部长度对称。</strong></p></li><li><p><strong>左偏:</strong>   </p><p>1）若以$b_s$表示偏度。$b_s&lt;0$称分布具有<strong>负偏离</strong>，也称左偏态；</p><p>2）此时数据位于均值左边的比位于右边的少，直观表现为左边的尾部相对于与右边的尾部要长；</p><p>3）<strong>因为有少数变量值很小，使曲线左侧尾部拖得很长；</strong></p></li><li><p><strong>右偏：</strong></p><p>1）$b_s&gt;0$称分布具有<strong>正偏离</strong>，也称右偏态；</p><p>2）此时数据位于均值右边的比位于左边的少，直观表现为右边的尾部相对于与左边的尾部要长；</p><p>3）<strong>因为有少数变量值很大，使曲线右侧尾部拖得很长；</strong></p></li></ul><p><font color="blue">峰度(peakedness;kurtosis)</font>：说明的是分布曲线在平均值处峰值高低的特征数。直观看来，峰度反映了峰部的尖度。样本的峰度是和正态分布相比较而言统计量，如果峰度大于正态分布峰度，峰的形状比较尖，比正态分布峰要陡峭。反之亦然。<strong>峰度刻画的是分布函数的集中和分散程度。</strong></p><p><img src="/article/83114b11/偏态与峰度.png" srcset="/img/loading.gif" alt></p><h3 id="公式与python实现"><a href="#公式与python实现" class="headerlink" title="公式与python实现"></a>公式与python实现</h3><p><font color="blue">样本偏度系数</font>：</p><script type="math/tex; mode=display">g_1 = \frac{n}{(n-1)(n-2)s^3}\sum_{i=1}^{n}{(x_i-\overline{x})}^3=\frac{n^2 u_3}{(n-1)(n-2)s^3}</script><p><font color="blue">样本峰度系数</font>：</p><script type="math/tex; mode=display">g_2 = \frac{n(n+1)}{(n-1)(n-2)(n-3)s^4}\sum_{i=1}^{n}{(x_i-\overline{x})}^4-3\frac{(n-1)^2}{(n-2)(n-3)}</script><p><img src="/article/83114b11/image3.PNG" srcset="/img/loading.gif" style="zoom:80%;"></p></x<+\infty>]]></content>
    
    
    <categories>
      
      <category>概率统计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
